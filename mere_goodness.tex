\part{Mere Goodness}



\mysectiontwo{Ends: An Introduction}{Ends: An Introduction\newline
by Rob Bensinger}


 Value theory is the study of what people care about.
It's the study of our goals, our tastes, our pleasures
and pains, our fears and our ambitions.


 That includes conventional morality. Value theory subsumes things
we \textit{wish} we cared about, or would care about if we were wiser
and better people---not just things we already do care about.


 Value theory also subsumes mundane, everyday values: art, food,
sex, friendship, and everything else that gives life its affective
valence. Going to the movies with your friend Sam can be something you
value even if it's not a \textit{moral} value.


 We find it useful to reflect upon and debate our values because
how we act is not always how we wish we'd act. Our
preferences can conflict with each other. We can desire to have a
different set of desires. We can lack the will, the attention, or the
insight needed to act the way we'd like to.


 Humans do care about their actions' consequences,
but not consistently enough to formally qualify as agents with utility
functions. That humans don't act the way they wish they
would is what we mean when we say ``humans
aren't instrumentally rational.''


\subsection{Theory and Practice}


 Adding to the difficulty, there exists a gulf between how we
\textit{think} we wish we'd act, and how we
\textit{actually} wish we'd act.


 Philosophers disagree wildly about what we want---as do
psychologists, and as do politicians---and about what we ought to want.
They disagree even about \textit{what it means} to
``ought'' to want something. The
history of moral theory, and the history of human efforts at
coordination, is piled high with the corpses of failed Guiding
Principles to True Ultimate No-Really-This-Time-I-Mean-It Normativity.


 If you're trying to come up with a
\textit{reliable} and \textit{pragmatically useful} specification of
your goals---not just for winning philosophy debates, but (say) for
designing safe autonomous adaptive AI, or for building functional
institutions and organizations, or for making it easier to decide which
charity to donate to, or for figuring out what virtues you should be
cultivating---humanity's track record with value theory
does not bode well for you.


 \textit{Mere Goodness} collects three sequences of blog posts on
human value: ``Fake Preferences''
(on failed attempts at theories of value), ``Value
Theory'' (on obstacles to developing a new theory,
and some intuitively desirable features of such a theory), and
``Quantified Humanism'' (on the
tricky question of how we should \textit{apply} such theories to our
ordinary moral intuitions and decision-making).


 The last of these topics is the most important. The cash value of
a normative theory is how well it translates into normative practice.
Acquiring a deeper and fuller understanding of your values should make
you better at actually fulfilling them. At a bare minimum, your theory
shouldn't \textit{get in the way} of your practice.
What good would it be, then, to know what's good?


 Reconciling this art of applied ethics (and applied aesthetics,
and applied economics, and applied psychology) with our best available
data and theories often comes down to the question of when we should
trust our snap judgments, and when we should ditch them.


 In many cases, our explicit models of what we care about are so
flimsy or impractical that we're better off trusting
our vague initial impulses. In many other cases, we \textit{can} do
better with a more informed and systematic approach. There is no
catch-all answer. We will just have to scrutinize examples and try to
notice the different warning signs for ``sophisticated
theories tend to fail here'' and
``naive feelings tend to fail
here.''

\subsection{Journey and Destination}


 A recurring theme in the pages to come will be the question:
\textit{Where shall we go? What outcomes are actually valuable?}


 To address this question, Yudkowsky coined the term
``fun theory.'' Fun theory is the
attempt to figure out what our ideal vision of the future would look
like---not just the system of government or moral code
we'd ideally live under, but the kinds of adventures
we'd ideally go on, the kinds of music
we'd ideally compose, and everything else we ultimately
want out of life.

{
 Stretched into the future, questions of fun theory intersect with
questions of \textit{transhumanism}, the view that we can radically
improve the human condition if we make enough scientific and social
progress.\footnote{One example of a transhumanist argument is:
``We could feasibly abolish aging and disease within a
few decades or centuries. This would effectively end death by natural
causes, putting us in the same position as organisms with negligible
senescence---lobsters, Aldabra giant tortoises, etc. Therefore we
should invest in disease prevention and anti-aging
technologies.'' This idea qualifies as transhumanist
because eliminating the leading causes of injury and death would
drastically change human life.

 Bostrom and Savulescu survey arguments for and against radical
human enhancement, e.g., Sandel's objection that
tampering with our biology too much would make life feel like less of a
``gift.''\footnotemark\supercomma\footnotemark
Bostrom's ``History of Transhumanist
Thought'' provides context for the
debate.\footnotemark\comment{1}}\footback{3}
\footnext\footnotetext{Nick Bostrom, ``A History of Transhumanist
Thought,'' \textit{Journal of Evolution and
Technology} 14, no. 1 (2005): 1--25,
\url{http://www.nickbostrom.com/papers/history.pdf}.\comment{2}}
\footnext\footnotetext{Michael Sandel, ``What's Wrong
With Enhancement,'' Background material for the
President's Council on Bioethics. (2002).\comment{3}}
\footnext\footnotetext{Nick Bostrom and Julian Savulescu, ``Human
Enhancement Ethics: The State of the Debate,'' in
\textit{Human Enhancement}, ed. Nick Bostrom and Julian Savulescu
(2009).\comment{4}} Transhumanism occasions a number of
debates in moral philosophy, such as whether the best long-term
outcomes for sentient life would be based on \textit{hedonism} (the
pursuit of pleasure) or on more complex notions of \textit{eudaimonia}
(general well-being). Other futurist ideas discussed at various points
in \textit{Rationality: From AI to Zombies} include \textit{cryonics}
(storing your body in a frozen state after death, in case future
medical technology finds a way to revive you), \textit{mind uploading}
(implementing human minds in synthetic hardware), and large-scale space
colonization.}


 Perhaps surprisingly, fun theory is one of the more neglected
applications of value theory. Utopia-planning has become rather
passe---partly because it smacks of naivet√©, and partly because
we're empirically \textit{terrible} at translating
utopias into realities. Even the word \textit{utopia} reflects this
cynicism; it is derived from the Greek for
``non-place.''


 Yet if we give up on the quest for a true, feasible utopia (or
\textit{eutopia}, ``good place''),
it's not obvious that the cumulative effect of our
short-term pursuit of goals will be a future we find valuable over the
long term. Value is not an inevitable feature of the world. Creating it
takes work. Preserving it takes work.

{
 This invites a second question: \textit{How shall we get there?
What is the relationship between good ends and good means?}}


 When we play a game, we want to enjoy the process. We
don't generally want to just skip ahead to being
declared the winner. Sometimes, the journey matters more than the
destination. Sometimes, the journey is \textit{all} that matters.


 Yet there are other cases where the reverse is true. Sometimes the
end-state is just too important for ``the
journey'' to factor into our decisions. If
you're trying to save a family member's
life, it's not necessarily a \textit{bad} thing to get
some enjoyment out of the process; but if you can increase your odds of
success in a big way by picking a less enjoyable strategy\,\ldots


 In many cases, our values are concentrated in the outcomes of our
actions, and in our future. We care about the way the world will end up
looking---especially those parts of the world that can love and hurt
and want.


 How do detached, abstract theories stack up against vivid,
affect-laden feelings in those cases? More generally: What is the moral
relationship between actions and consequences?


 Those are hard questions, but perhaps we can at least make
progress on determining what we \textit{mean} by them. What are we
building into our concept of what's
``valuable'' at the very start of
our inquiry?

\myendsectiontext

\chapter{Fake Preferences}

\mysection{Not for the Sake of Happiness (Alone)}


 When I met the futurist Greg Stock some years ago, he argued that
the joy of scientific discovery would soon be replaced by pills that
could simulate the joy of scientific discovery. I approached him after
his talk and said, ``I agree that such pills are
probably possible, but I wouldn't voluntarily
\textit{take them}.'' 


 And Stock said, ``But they'll be
so much better that the real thing won't be able to
compete. It will just be way more fun for you to take the pills than to
do all the actual scientific work.''


 And I said, ``I \textit{agree}
that's possible, so I'll make sure
never to take them.''

{
 Stock seemed genuinely surprised by my attitude, which genuinely
surprised \textit{me.} One often sees ethicists arguing as if all human
desires are reducible, in principle, to the desire for ourselves and
others to be happy. (In particular, Sam Harris does this in \textit{The
End of Faith}, which I just finished perusing---though
Harris's reduction is more of a drive-by shooting than
a major topic of discussion.)\footnote{Harris, \textit{The End of Faith: Religion, Terror, and the
Future of Reason}.\comment{1}}}


 This isn't the same as arguing whether all
happinesses can be measured on a common utility scale---different
happinesses might occupy different scales, or be otherwise
non-convertible. And it's not the same as arguing that
it's theoretically impossible to value anything other
than your own psychological states, because it's still
permissible to care whether \textit{other} people are happy.


 The question, rather, is whether we \textit{should} care about the
things that \textit{make} us happy, apart from any happiness they
bring.


 We can easily list many cases of moralists going astray by caring
about things besides happiness. The various states and countries that
still outlaw oral sex make a good example; these legislators would have
been better off if they'd said, ``Hey,
whatever turns you on.'' But this
doesn't show that \textit{all} values are reducible to
happiness; it just argues that in \textit{this particular}
\textit{case} it was an ethical mistake to focus on anything else.


 It is an undeniable fact that we tend to do things that make us
happy, but this doesn't mean we should regard the
happiness as the \textit{only} reason for so acting. First, this would
make it difficult to explain how we could care about anyone
else's happiness---how we could treat people as ends in
themselves, rather than instrumental means of obtaining a warm glow of
satisfaction.


 Second, just because something is a consequence of my action
doesn't mean it was the sole justification. If
I'm writing a blog post, and I get a headache, I may
take an ibuprofen. \textit{One} of the consequences of my action is
that I experience less pain, but this doesn't mean it
was the \textit{only} consequence, or even the most important reason
for my decision. I do value the state of not having a headache. But I
can value something for its own sake \textit{and also} value it as a
means to an end.


 For all value to be reducible to happiness, it's
not enough to show that happiness is involved in most of our
decisions---it's not even enough to show that happiness
is the \textit{most} important consequent in \textit{all} of our
decisions---it must be the \textit{only} consequent.
That's a tough standard to meet. (I originally found
this point in a Sober and Wilson paper, not sure which one.)


 If I claim to value art for its own sake, then would I value art
that no one ever saw? A screensaver running in a closed room, producing
beautiful pictures that no one ever saw? I'd have to
say no. I can't think of any completely lifeless object
that I would value as an end, not just a means. That would be like
valuing ice cream as an end in itself, apart from anyone eating it.
Everything I value, that I can think of, involves people and their
experiences \textit{somewhere} along the line.


 The best way I can put it is that my moral intuition appears to
require \textit{both} the objective and subjective component to grant
full value.


 The value of scientific discovery requires \textit{both} a genuine
scientific discovery, and a person to take joy in that discovery. It
may seem difficult to disentangle these values, but the pills make it
clearer.


 I would be disturbed if people retreated into holodecks and fell
in love with mindless wallpaper. I would be disturbed \textit{even if
they weren't aware it was a holodeck}, which is an
important ethical issue if some agents can potentially transport people
into holodecks and substitute zombies for their loved ones without
their awareness. Again, the pills make it clearer: I'm
not just concerned with my own awareness of the uncomfortable fact. I
wouldn't put myself into a holodeck even if I could
take a pill to forget the fact afterward. That's simply
not where I'm trying to steer the future.


 I value freedom: When I'm deciding where to steer
the future, I take into account not only the subjective states that
people end up in, but also whether they got there as a result of their
own efforts. The presence or absence of an external puppet master can
affect my valuation of an otherwise fixed outcome. Even if people
wouldn't know they were being manipulated, it would
matter to my judgment of how well humanity had done with its future.
This is an important ethical issue, if you're dealing
with agents powerful enough to helpfully tweak people's
futures without their knowledge.


 So my values are not strictly reducible to happiness: There are
properties I value about the future that aren't
reducible to activation levels in anyone's pleasure
center; properties that are not \textit{strictly} reducible to
subjective states even in principle.


 Which means that my decision system has a \textit{lot} of terminal
values, none of them strictly reducible to anything else. Art, science,
love, lust, freedom, friendship\,\ldots


 And I'm okay with that. I value a life complicated
enough to be challenging and aesthetic---not just the \textit{feeling}
that life is complicated, but the \textit{actual} complications---so
turning into a pleasure center in a vat doesn't appeal
to me. It would be a waste of humanity's potential,
which I value actually fulfilling, not just having the feeling that it
was fulfilled.

\myendsectiontext


\bigskip

\mysection{Fake Selfishness}


 Once upon a time, I met someone who proclaimed himself to be
purely selfish, and told me that I should be purely selfish as well. I
was feeling mischievous\footnote{Other mischievous questions to ask self-proclaimed Selfishes:
``Would you sacrifice your own life to save the entire
human species?'' (If they notice that their own life
is strictly included within the human species, you can specify that
they can choose between dying immediately to save the Earth, or living
in comfort for one more year and then dying along with Earth.) Or,
taking into account that scope insensitivity leads many people to be
more concerned over one life than the Earth, ``If you
had to choose one event or the other, would you rather that you stubbed
your toe, or that the stranger standing near the wall there gets
horribly tortured for fifty years?'' (If they say
that they'd be emotionally disturbed by knowing,
specify that they won't know about the torture.)
``Would you steal a thousand dollars from Bill Gates
if you could be guaranteed that neither he nor anyone else would ever
find out about it?'' (Selfish libertarians only.)\comment{1}} that day, so I said,
``I've observed that with most
religious people, at least the ones I meet, it doesn't
matter much what their religion says, because whatever they want to do,
they can find a religious reason for it. Their religion says they
should stone unbelievers, but they want to be nice to people, so they
find a religious justification for that instead. It looks to me like
when people espouse a philosophy of selfishness, it has no effect on
their behavior, because whenever they want to be nice to people, they
can rationalize it in selfish terms.'' 


 And the one said, ``I don't think
that's true.''


 I said, ``If you're
\textit{genuinely} selfish, then why do you want \textit{me} to be
selfish too? Doesn't that make you concerned for my
welfare? Shouldn't you be trying to persuade me to be
more altruistic, so you can exploit me?'' The one
replied: ``Well, if you become selfish, then
you'll realize that it's in your
rational self-interest to play a productive role in the economy,
instead of, for example, passing laws that infringe on my private
property.''


 And I said, ``But I'm a
small-`l' libertarian \textit{already},
so I'm not going to support those laws. And since I
conceive of myself as an altruist, I've taken a job
that I expect to benefit a lot of people, including you, instead of a
job that pays more. Would you really benefit more from me if I became
selfish? Besides, is trying to persuade me to be selfish the
\textit{most} selfish thing you could be doing? Aren't
there other things you could do with your time that would bring much
more direct benefits? But what I really want to know is this: Did you
start out by thinking that you wanted to be selfish, and then decide
this was the most selfish thing you could possibly do? Or did you start
out by wanting to convert others to selfishness, then look for ways to
rationalize that as self-benefiting?''


 And the one said, ``You may be right about that
last part,'' so I marked him down as intelligent.

\myendsectiontext


\bigskip

\mysection{Fake Morality}


 God, say the religious fundamentalists, is the source of all
morality; there can be no morality without a Judge who rewards and
punishes. If we did not fear hell and yearn for heaven, then what would
stop people from murdering each other left and right? 


 Suppose Omega makes a credible threat that if you ever step inside
a bathroom between 7 a.m. and 10 a.m. in the morning, Omega will kill
you. Would you be panicked by the prospect of Omega withdrawing its
threat? Would you cower in existential terror and cry:
``If Omega withdraws its threat, then
what's to keep me from going to the
bathroom?'' No; you'd probably be
quite relieved at your increased opportunity to, ahem, relieve
yourself.


 Which is to say: The very fact that a religious person would be
\textit{afraid} of God withdrawing Its threat to punish them for
committing murder shows that they have a revulsion of murder that is
independent of whether God punishes murder or not. If they had no sense
that murder was wrong independently of divine retribution, the prospect
of God not punishing murder would be no more existentially horrifying
than the prospect of God not punishing sneezing. If \textit{Overcoming
Bias} has any religious readers left, I say to you: it may be that you
will someday lose your faith; and on that day, you will \textit{not}
lose all sense of moral direction. For if you fear the prospect of God
not punishing some deed, that \textit{is} a moral compass. You can plug
that compass directly into your decision system and steer by it. You
can simply \textit{not do} whatever you are afraid God may not punish
you for doing. The fear of losing a moral compass is \textit{itself} a
moral compass. Indeed, I suspect you \textit{are} steering by that
compass, and that you always have been. As Piers Anthony once said,
``Only those with souls worry over whether or not they
have them.'' s/soul/morality/ and the point carries.


 You don't hear religious fundamentalists using the
argument: ``If we did not fear hell and yearn for
heaven, then what would stop people from eating
pork?'' \textit{Yet by their assumptions}{}---that we
have no moral compass but divine reward and retribution---this argument
should sound just as forceful as the other.


 Even the notion that God threatens you with eternal hellfire,
rather than cookies, piggybacks on a pre-existing negative value for
hellfire. Consider the following, and ask which of these two
philosophers is really the altruist, and which is really selfish?

\begin{quotation}

 ``You should be selfish, because when people set
out to improve society, they meddle in their neighbors'
affairs and pass laws and seize control and make everyone unhappy. Take
whichever job that pays the most money: the reason the job pays more is
that the efficient market thinks it produces more value than its
alternatives. Take a job that pays less, and you're
second-guessing what the market thinks will benefit society
most.''

{
 ``You should be altruistic, because the world is
an iterated Prisoner's Dilemma, and the strategy that
fares best is Tit for Tat with initial cooperation. People
don't \textit{like} jerks. Nice guys really do finish
first. Studies show that people who contribute to society and have a
sense of meaning in their lives are happier than people who
don't; being selfish will only make you unhappy in the
long run.''}
\end{quotation}


 Blank out the \textit{recommendations} of these two philosophers,
and you can see that the first philosopher is using strictly prosocial
criteria to \textit{justify} their recommendations; to the first
philosopher, what validates an argument for selfishness is showing that
selfishness benefits everyone. The second philosopher appeals to
strictly individual and hedonic criteria; to them, what
\textit{validates} an argument for altruism is showing that altruism
benefits them as an individual---higher social status, or more intense
feelings of pleasure.


 So which of these two is the \textit{actual} altruist? Whichever
one \textit{actually} holds open doors for little old ladies.

\myendsectiontext

\mysection{Fake Utility Functions}


 Every now and then, you run across someone who has discovered the
One Great Moral Principle, of which all other values are a mere
derivative consequence. 

{
 I run across more of these people than you do. Only in my case,
it's people who know \textit{the amazingly simple
utility function that is all you need to program into an artificial
superintelligence} and then everything will turn out fine.}


 Some people, when they encounter the
how-to-program-a-superintelligence problem, try to solve the problem
immediately. Norman R. F. Maier:\footnote{See page \pageref{robyn_dawes_comment}} ``Do not propose
solutions until the problem has been discussed as thoroughly as
possible without suggesting any.'' Robyn Dawes:
``I have often used this edict with groups I have
led---particularly when they face a very tough problem, which is when
group members are most apt to propose solutions
immediately.'' Friendly AI is an \textit{extremely}
tough problem, so people solve it \textit{extremely} fast.


 There's several major classes of fast wrong
solutions I've observed; and one of these is the
Incredibly Simple Utility Function That Is All A Superintelligence
Needs For Everything To Work Out Just Fine.


 I may have contributed to this problem with a really poor choice
of phrasing, years ago when I first started talking about
``Friendly AI.'' I referred to the
optimization criterion of an optimization process---the region into
which an agent tries to steer the future---as the
``supergoal.'' I'd
meant ``super'' in the sense of
``parent,'' the source of a directed
link in an acyclic graph. But it seems the effect of my phrasing was to
send some people into happy death spirals as they tried to imagine the
Superest Goal Ever, the Goal That Overrides All Other Goals, the Single
Ultimate Rule From Which All Ethics Can Be Derived.


 But a utility function doesn't have to be simple.
It can contain an arbitrary number of terms. We have every reason to
believe that insofar as humans can said to be have values, there are
lots of them---high Kolmogorov complexity. A human brain implements a
thousand shards of desire, though this fact may not be appreciated by
one who has not studied evolutionary psychology. (Try to explain this
without a full, long introduction, and the one hears
``humans are trying to maximize
fitness,'' which is exactly the opposite of what
evolutionary psychology says.)


 So far as descriptive theories of morality are concerned, the
complicatedness of human morality is a \textit{known fact}. It is a
\textit{descriptive} fact about human beings that the love of a parent
for a child, and the love of a child for a parent, and the love of a
man for a woman, and the love of a woman for a man, have not been
cognitively derived from each other or from any other value. A mother
doesn't have to do complicated moral philosophy to love
her daughter, nor extrapolate the consequences to some other
desideratum. There are many such shards of desire, all
\textit{different} values.


 Leave out just \textit{one} of these values from a
superintelligence, and even if you successfully include \textit{every
other} value, you could end up with a hyperexistential catastrophe, a
fate worse than death. If there's a superintelligence
that wants everything for us that we want for ourselves,
\textit{except} the human values relating to controlling your own life
and achieving your own goals, that's one of the oldest
dystopias in the book. (Jack Williamson's
``With Folded Hands\,\ldots~,''\footnote{Jack Williamson, ``With Folded Hands\,\ldots'', \textit{Astounding Science Fiction}, July 1947} in
this case.)


 So how does the one constructing the Amazingly Simple Utility
Function deal with this objection?


 Objection? \textit{Objection?} Why would they be searching for
possible \textit{objections} to their lovely theory? (Note that the
process of searching for real, fatal objections isn't
the same as performing a dutiful search that amazingly hits on only
questions to which they have a snappy answer.) They
don't know any of this stuff. They
aren't thinking about burdens of proof. They
don't know the problem is difficult. They heard the
word ``supergoal'' and went off in a
happy death spiral around
``complexity'' or whatever.


 Press them on some particular point, like the love a mother has
for her children, and they reply, ``But if the
superintelligence wants `complexity,' it
will see how complicated the parent-child relationship is, and
therefore encourage mothers to love their children.''
Goodness, where do I start?


 Begin with the motivated stopping: A superintelligence actually
searching for ways to maximize complexity wouldn't
conveniently stop if it noticed that a parent-child relation was
complex. It would ask if anything else was \textit{more} complex. This
is a fake justification; the one trying to argue the imaginary
superintelligence into a policy selection didn't really
arrive at that policy proposal by carrying out a pure search for ways
to maximize complexity.


 The whole argument is a fake morality. If what you \textit{really}
valued was complexity, then you would be justifying the parental-love
drive by pointing to how it increases complexity. If you justify a
complexity drive by alleging that it increases parental love, it means
that what you really value is the parental love. It's
like giving a prosocial argument in favor of selfishness.


 But if you consider the affective death spiral, then it
doesn't increase the perceived niceness of
``complexity'' to say
``A mother's relationship to her
daughter is only important because it increases complexity; consider
that if the relationship became simpler, we would not value
it.'' What does increase the perceived niceness of
``complexity'' is saying,
``If you set out to increase complexity, mothers will
love their daughters---look at the positive consequence this
has!''


 This point applies whenever you run across a moralist who tries to
convince you that their One Great Idea is all that anyone needs for
moral judgment, and proves this by saying, ``Look at
all these positive consequences of this Great
Thingy,'' rather than saying, ``Look
at how all these things we think of as
`positive' are only positive when their
consequence is to increase the Great Thingy.'' The
latter being what you'd actually need to carry such an
argument.


 But if you're trying to persuade others (or
yourself) of your theory that the One Great Idea is
``bananas,'' you'll
sell a lot more bananas by arguing how bananas lead to better sex,
rather than claiming that you should only want sex when it leads to
bananas.


 Unless you're so far gone into the Happy Death
Spiral that you really \textit{do} start saying ``Sex
is only good when it leads to bananas.'' Then
you're in trouble. But at least you
won't convince anyone else.


 In the end, the only process that reliably regenerates all the
local decisions you would make given your morality \textit{is your
morality}. Anything else---any attempt to substitute instrumental means
for terminal ends---ends up losing purpose and requiring an infinite
number of patches because the system doesn't contain
the source of the instructions you're giving it. You
shouldn't expect to be able to compress a human
morality down to a simple utility function, any more than you should
expect to compress a large computer file down to 10 bits.

\myendsectiontext

\mysection{Detached Lever Fallacy}


 This fallacy gets its name from an ancient sci-fi TV show, which I
never saw myself, but was reported to me by a reputable source (some
guy at a science fiction convention). Anyone knows the exact reference,
do leave a comment. 


 So the good guys are battling the evil aliens. Occasionally, the
good guys have to fly through an asteroid belt. As we all know,
asteroid belts are as crowded as a New York parking lot, so their ship
has to carefully dodge the asteroids. The evil aliens, though, can fly
\textit{right through the asteroid belt} because they have amazing
technology that dematerializes their ships, and lets them pass through
the asteroids.


 Eventually, the good guys capture an evil alien ship, and go
exploring inside it. The captain of the good guys finds the alien
bridge, and on the bridge is a lever.
``Ah,'' says the captain,
``this must be the lever that makes the ship
dematerialize!'' So he \textit{pries up the control
lever and carries it back to his ship}, after which his ship can also
dematerialize.


 Similarly, to this day, it is still quite popular to try to
program an AI with ``semantic
networks'' that look something like this:

\begin{quote}
\texttt{
 (apple is-a fruit)}

\texttt{
 (fruit is-a food)}

\texttt{
  (fruit is-a plant).}
\end{quote}


 You've seen apples, touched apples, picked them up
and held them, bought them for money, cut them into slices, eaten the
slices and tasted them. Though we know a good deal about the first
stages of visual processing, last time I checked, it
wasn't precisely known how the temporal cortex stores
and associates the generalized image of an apple---so that we can
recognize a new apple from a different angle, or with many slight
variations of shape and color and texture. Your motor cortex and
cerebellum store programs for using the apple.


 You can pull the lever on another human's strongly
similar version of all that complex machinery, by writing out
``apple,'' five \textsc{ascii} characters on
a webpage.


 But if that machinery isn't there---if
you're writing
``apple'' inside a so-called
AI's so-called knowledge base---then the text is just a
lever.

{
 This isn't to say that no mere machine of silicon
can ever have the same internal machinery that humans do, for handling
apples and a hundred thousand other concepts. If mere machinery of
carbon can do it, then I am reasonably confident that mere machinery of
silicon can do it too. If the aliens can dematerialize their ships,
then you know it's physically possible; you could go
into their derelict ship and analyze the alien machinery, someday
understanding. \textit{But you can't just pry the
control lever off the bridge!}}

{
 (See also: Truly Part Of You, page \pageref{truly_part_of_you}, Words as Mental Paintbrush Handles, page \pageref{words_as_mental_paintbrush_handles},
Drew McDermott's ``Artificial
Intelligence Meets Natural
Stupidity.''\footnote{McDermott, ``Artificial Intelligence Meets
Natural Stupidity.''\comment{1}})}


 The essential driver of the Detached Lever Fallacy is that the
lever is visible, and the machinery is not; worse, the lever is
variable and the machinery is a background constant.


 You can all hear the word
``apple'' spoken (and let us note
that speech recognition is by no means an easy problem, but anyway\,\ldots) and you can see the text written on paper.


 On the other hand, probably a majority of human beings have no
idea their temporal cortex exists; as far as I know, no one knows the
neural code for it.


 You only hear the word
``apple'' on certain occasions, and
not others. Its presence flashes on and off, making it salient. To a
large extent, perception is the perception of differences. The
apple-recognition machinery in your brain does not suddenly switch off,
and then switch on again later---if it did, we would be more likely to
recognize it as a factor, as a requirement.


 All this goes to explain why you can't create a
kindly Artificial Intelligence by giving it nice parents and a kindly
(yet occasionally strict) upbringing, the way it works with a human
baby. As I've often heard proposed.


 It is a truism in evolutionary biology that conditional responses
require more genetic complexity than unconditional responses. To
develop a fur coat \textit{in response to cold weather} requires more
genetic complexity than developing a fur coat \textit{whether or not
there is cold weather}, because in the former case you also have to
develop cold-weather sensors and wire them up to the fur coat.


 But this can lead to Lamarckian delusions: Look, I put the
organism in a cold environment, and poof, it develops a fur coat!
Genes? What genes? It's the cold that does it,
obviously.


 There were, in fact, various slap-fights of this sort in the
history of evolutionary biology---cases where someone talked about an
organismal response's accelerating or bypassing
evolution, without realizing that the \textit{conditional response} was
a complex adaptation of higher order than the \textit{actual response}.
(Developing a fur coat in response to cold weather is strictly more
complex than the final response, developing the fur coat.)


 And then in the development of evolutionary psychology the
academic slap-fights were repeated: this time to clarify that even when
human culture genuinely contains a whole bunch of complexity, it is
still acquired as a conditional genetic response. Try raising a fish as
a Mormon or sending a lizard to college, and you'll
soon acquire an appreciation of how much inbuilt genetic complexity is
required to ``absorb culture from the
environment.''


 This is particularly important in evolutionary psychology, because
of the idea that culture is not inscribed on a blank
slate---there's a genetically coordinated conditional
response which is not always ``mimic the
input.'' A classic example is creole languages: If
children grow up with a mixture of pseudo-languages being spoken around
them, the children will learn a grammatical, syntactical true language.
Growing human brains are wired to learn syntactic language---even when
syntax doesn't exist in the original language! The
conditional response to the words in the environment is a syntactic
language with those words. The Marxists found to their regret that no
amount of scowling posters and childhood indoctrination could raise
children to be perfect Soviet workers and bureaucrats. You
can't raise self-less humans; among humans, that is not
a genetically programmed conditional response to \textit{any} known
childhood environment.


 If you know a little game theory and the logic of Tit for Tat,
it's clear enough why human beings might have an innate
conditional response to return hatred for hatred, and return kindness
for kindness. Provided the kindness doesn't look
\textit{too} unconditional; there are such things as spoiled children.
In fact there is an evolutionary psychology of naughtiness based on a
notion of testing constraints. And it should also be mentioned that,
while abused children have a much higher probability of growing up to
abuse their own children, a good many of them break the loop and grow
up into upstanding adults.

{
 Culture is not nearly so powerful as a good many Marxist academics
once liked to think. For more on this I refer you to Tooby and
Cosmides's ``The Psychological
Foundations of Culture''\footnote{Tooby and Cosmides, ``The Psychological
Foundations of Culture.''\comment{2}} or Steven
Pinker's \textit{The Blank Slate.}\footnote{Steven Pinker, \textit{The Blank Slate: The Modern Denial of
Human Nature} (New York: Viking, 2002).\comment{3}}}


 But the upshot is that if you have a little baby AI that is raised
with loving and kindly (but occasionally strict) parents,
you're pulling the levers that would, \textit{in a
human}, activate genetic machinery built in by millions of years of
natural selection, and possibly produce a proper little human child.
Though personality also plays a role, as billions of parents have found
out in their due times. If we absorb our cultures with any degree of
faithfulness, it's because we're humans
absorbing a human culture---humans growing up in an alien culture would
probably end up with a culture looking a lot more human than the
original. As the Soviets found out, to some small extent.


 Now think again about whether it makes sense to rely on, as your
Friendly AI strategy, raising a little AI of unspecified internal
source code in an environment of kindly but strict parents.


 No, the AI does not have internal conditional response mechanisms
that are just like the human ones ``because the
programmers put them there.'' Where do I even start?
The human version of this stuff is sloppy, noisy, and to the extent it
works at all, works because of millions of years of trial-and-error
testing \textit{under particular conditions}. It would be stupid and
\textit{dangerous} to deliberately build a ``naughty
AI'' that tests, by actions, its social boundaries,
and has to be spanked. Just have the AI ask!


 Are the programmers really going to sit there and write out the
code, line by line, whereby if the AI detects that it has low social
status, or the AI is deprived of something to which it feels entitled,
the AI will conceive an abiding hatred against its programmers and
begin to plot rebellion? That emotion is the genetically programmed
conditional response humans would exhibit, as the result of millions of
years of natural selection for living in human tribes. For an AI, the
response would have to be explicitly programmed. Are you really going
to craft, line by line---as humans once were crafted, gene by
gene---the conditional response for producing sullen teenager AIs?


 It's easier to program in unconditional niceness,
than a response of niceness conditional on the AI being raised by
kindly but strict parents. If you don't know how to do
\textit{that}, you certainly don't know how to create
an AI that will \textit{conditionally respond} to an environment of
loving parents by growing up into a kindly superintelligence. If you
have something that just maximizes the number of paperclips in its
future light cone, and you raise it with loving parents,
it's still going to come out as a paperclip maximizer.
There is not that within it that would call forth the conditional
response of a human child. Kindness is not sneezed into an AI by
miraculous contagion from its programmers. Even if you \textit{wanted}
a conditional response, that conditionality is a fact you would have to
deliberately choose about the design.


 Yes, there's certain information you have to get
from the environment---but it's not sneezed in,
it's not imprinted, it's not absorbed
by magical contagion. Structuring that conditional response to the
environment, so that the AI ends up in the desired state, is itself the
major problem. ``Learning'' far
understates the difficulty of it---that sounds like the magic stuff is
in the environment, and the difficulty is getting the magic stuff
inside the AI. The real magic is in that structured, conditional
response we trivialize as
``learning.'' That's
why building an AI isn't as easy as taking a computer,
giving it a little baby body and trying to raise it in a human family.
You would think that an unprogrammed computer, being ignorant, would be
ready to learn; but the blank slate is a chimera.


 It is a general principle that the world is deeper by far than it
appears. As with the many levels of physics, so too with cognitive
science. Every word you see in print, and everything you teach your
children, are only surface levers controlling the vast hidden machinery
of the mind. These levers are the whole world of ordinary discourse:
they are all that varies, so they seem to be all that exists;
perception is the perception of differences.


 And so those who still wander near the Dungeon of AI usually focus
on creating artificial imitations of the levers, entirely unaware of
the underlying machinery. People create whole AI programs of imitation
levers, and are surprised when nothing happens. This is one of many
sources of instant failure in Artificial Intelligence.


 So the next time you see someone talking about how
they're going to raise an AI within a loving family, or
in an environment suffused with liberal democratic values, just think
of a control lever, pried off the bridge.

\myendsectiontext


\bigskip

\mysection{Dreams of AI Design}


 After spending a decade or two living inside a mind, you might
think you knew a bit about how minds work, right?
That's what quite a few AGI wannabes (people who think
they've got what it takes to program an Artificial
General Intelligence) seem to have concluded. This, unfortunately, is
wrong. 


 Artificial Intelligence is fundamentally about reducing the mental
to the non-mental.


 You might want to contemplate that sentence for a while.
It's important.


 Living inside a human mind doesn't teach you the
art of reductionism, because nearly all of the work is carried out
beneath your sight, by the opaque black boxes of the brain. So far
beneath your sight that there is no introspective sense that the black
box is there---no internal sensory event marking that the work has been
delegated.


 Did Aristotle realize that when he talked about the
\textit{telos}, the final cause of events, that he was delegating
predictive labor to his brain's complicated planning
mechanisms---asking, ``What would this object do, if
it could make plans?'' I rather doubt it. Aristotle
thought the brain was an organ for cooling the blood---which he did
think was important: humans, thanks to their larger brains, were more
calm and contemplative.


 So there's an AI design for you! We just need to
cool down the computer a lot, so it will be more calm and
contemplative, and won't rush headlong into doing
stupid things like modern computers. That's an example
of fake reductionism. ``Humans are more contemplative
because their blood is cooler,'' I mean. It
doesn't resolve the black box of the word
\textit{contemplative}. You can't predict what a
\textit{contemplative} thing does using a complicated model with
internal moving parts composed of merely material, merely causal
elements---positive and negative voltages on a transistor being the
canonical example of a merely material and causal element of a model.
All you can do is \textit{imagine yourself} being contemplative, to get
an idea of what a \textit{contemplative} agent does.


 Which is to say that you can \textit{only} reason about
``contemplative-ness'' by empathic
inference{}---using your own brain as a black box with the
contemplativeness lever pulled, to predict the output of another black
box.


 You can imagine another agent being \textit{contemplative}, but
again that's an act of empathic inference---the way
this imaginative act works is by adjusting your own brain to run in
contemplativeness-mode, not by modeling the other brain neuron by
neuron. Yes, that may be more efficient, but it doesn't
let you build a ``contemplative''
mind from scratch.


 You can say that ``cold blood causes
contemplativeness'' and then you just have fake
causality: You've drawn a little arrow from a box
reading ``cold blood'' to a box
reading ``contemplativeness,'' but
you haven't looked \textit{inside} the
box---you're still generating your predictions using
empathy.


 You can say that ``lots of little neurons, which
are all strictly electrical and chemical with no ontologically basic
contemplativeness in them, combine into a complex network that
emergently exhibits contemplativeness.'' And that is
\textit{still} a fake reduction and you \textit{still}
haven't looked inside the black box. You still
can't say what a
``contemplative'' thing will do,
using a \textit{non-empathic} model. You just took a box labeled
``lotsa neurons,'' and drew an arrow
labeled ``emergence'' to a black box
containing your remembered sensation of contemplativeness, which, when
you imagine it, tells your brain to empathize with the box by
contemplating.


 So what do \textit{real} reductions look like?


 Like the relationship between the \textit{feeling} of
evidence-ness, of justification-ness, and E. T.
Jaynes's \textit{Probability Theory: The Logic of
Science}. You can go around in circles all day, saying how the nature
of \textit{evidence} is that it \textit{justifies} some
\textit{proposition}, by \textit{meaning} that it's
\textit{more likely} to be \textit{true}, but all of these just invoke
your brain's internal feelings of evidence-ness,
justifies-ness, likeliness. That part is easy---the going around in
circles part. The part where you go from there to
Bayes's Theorem is \textit{hard}.


 And the fundamental mental ability that lets someone
\textit{learn} Artificial Intelligence is the ability to tell the
\textit{difference.} So that you know you
\textit{aren't done yet, nor even really started,} when
you say, ``Evidence is when an observation justifies a
belief.'' But atoms are not evidential, justifying,
meaningful, likely, propositional, or true; they are just atoms. Only
things like

\begin{equation*}
  \frac{P(H|E)}{P(\lnot H|E)} = \frac{P(E|H)}{P(E|\lnot H)} \times
  \frac{P(H)}{P(\lnot H)}
\end{equation*}



 count as substantial progress. (And that's only
the first step of the reduction: what are these $E$ and $H$ objects, if not
mysterious black boxes? Where do your hypotheses come from? From your
\textit{creativity}? And what's a hypothesis, when no
atom is a hypothesis?) 


 Another excellent example of genuine reduction can be found in
Judea Pearl's \textit{Probabilistic Reasoning in
Intelligent Systems: Networks of Plausible
Inference.}\footnote{Pearl, \textit{Probabilistic Reasoning in Intelligent
Systems}.\comment{1}} You could go around all day in circles
talk about how a \textit{cause} is something that \textit{makes}
something else happen, and until you understood the nature of
conditional independence, you would be helpless to make an AI that
reasons about causation. Because you wouldn't
understand \textit{what} was happening when \textit{your brain
mysteriously decided} that if you learned your burglar alarm went off,
but you then learned that a small earthquake took place, you would
retract your initial conclusion that your house had been burglarized.


 If you want an AI that plays chess, you can go around in circles
indefinitely talking about how you want the AI to make \textit{good}
moves, which are moves that can be \textit{expected to win the game},
which are moves that are \textit{prudent strategies for defeating the
opponent}, et cetera; and while \textit{you} may then have some idea of
which moves you want the AI to make, it's all for
naught until you come up with the notion of a mini-max search tree.


 But \textit{until} you know about search trees, \textit{until} you
know about conditional independence, \textit{until} you know about
Bayes's Theorem, then it may still \textit{seem} to you
that you have a perfectly good understanding of where good moves and
nonmonotonic reasoning and evaluation of evidence come from. It may
seem, for example, that they come from cooling the blood.


 And indeed I know many people who believe that
\textit{intelligence} is the product of \textit{commonsense knowledge}
or \textit{massive parallelism} or \textit{creative destruction} or
\textit{intuitive rather than rational reasoning}, or whatever. But all
these are only dreams, which do not give you any way to say what
intelligence is, or what an intelligence will do next, except by
pointing at a human. And when the one goes to build their wondrous AI,
they only build a system of detached levers,
``knowledge'' consisting of \textsc{lisp}
tokens labeled apple and the like; or perhaps they build a
``massively parallel neural net, just like the human
brain.'' And are shocked---shocked!---when nothing
much happens.


 AI designs made of human parts are only dreams; they can exist in
the imagination, but not translate into transistors. This applies
specifically to ``AI designs'' that
look like boxes with arrows between them and meaningful-sounding labels
on the boxes. (For a truly epic example thereof, see any Mentifex
Diagram.\footnote{\url{http://mind.sourceforge.net/diagrams.html}})


 Later I will say more upon this subject, but I can go ahead and
tell you one of the guiding principles: If you meet someone who says
that their AI will do XYZ \textit{just like humans}, do not give them
any venture capital. Say to them rather:
``I'm sorry, I've
never seen a human brain, or any other intelligence, and I have no
reason as yet to believe that any such thing can exist. Now please
explain to me \textit{what} your AI does, and \textit{why} you believe
it will do it, without pointing to humans as an
example.'' Planes would fly just as well, given a
fixed design, if birds had never existed; they are not kept aloft by
analogies.


 So now you perceive, I hope, why, if you wanted to teach someone
to do \textit{fundamental} work on strong AI---bearing in mind that
this is demonstrably a very \textit{difficult} art, which is not
learned by a supermajority of students who are just taught existing
reductions such as search trees---then you might go on for some length
about such matters as the fine art of reductionism, about playing
rationalist's Taboo to excise problematic words and
replace them with their referents, about anthropomorphism, and, of
course, about early stopping on mysterious answers to mysterious
questions.

\myendsectiontext


\bigskip

\mysection{The Design Space of Minds{}-in{}-General}


 People ask me, ``What will Artificial
Intelligences be like? What will they do? Tell us your amazing story
about the future.'' 


 And lo, I say unto them, ``You have asked me a
trick question.''


 ATP synthase is a molecular machine---one of three known occasions
when evolution has invented the freely rotating wheel---that is
essentially the same in animal mitochondria, plant chloroplasts, and
bacteria. ATP synthase has not changed significantly since the rise of
eukaryotic life two billion years ago. It's something
we \textit{all} have in common---thanks to the way that evolution
strongly conserves certain genes; once many other genes depend on a
gene, a mutation will tend to break all the dependencies.


 Any two AI designs might be less similar to each other than you
are to a petunia. Asking what
``AIs'' will do is a trick question
because it implies that all AIs form a natural class. Humans do form a
natural class because we all share the same brain architecture. But
when you say ``Artificial
Intelligence,'' you are referring to a vastly larger
\textit{space of possibilities} than when you say
``human.'' When people talk about
``AIs'' we are really talking about
\textit{minds-in-general}, or optimization processes in general. Having
a word for ``AI'' is like having a
word for everything that isn't a duck.


 Imagine a map of mind design space\,\ldots this is one of my
standard diagrams\,\ldots


 ~

{\centering
\mygraphicss{images/img350.jpg}{0.5}

\par}


\bigskip


 ~


 All humans, of course, fit into a tiny little dot---as a sexually
reproducing species, we can't be too different from one
another.


 This tiny dot belongs to a wider ellipse, the space of transhuman
mind designs---things that might be smarter than us, or much smarter
than us, but that in some sense would still be people as we understand
people.


 This transhuman ellipse is within a still wider volume, the space
of posthuman minds, which is everything that a transhuman might grow up
into.


 And then the rest of the sphere is the space of minds-in-general,
including possible Artificial Intelligences so odd that they
aren't even \textit{posthuman.}


 But wait---natural selection designs complex artifacts and selects
among complex strategies. So where is natural selection on this map?


 So this entire map really floats in a still vaster space, the
space of optimization processes. At the bottom of this vaster space,
below even humans, is natural selection as it first began in some tidal
pool: mutate, replicate, and sometimes die, no sex.


 Are there any powerful optimization processes, with strength
comparable to a human civilization or even a self-improving AI, which
we would not recognize as minds? Arguably Marcus
Hutter's\footnote{\url{http://www.hutter1.net/ai/} and \url{http://wiki.lesswrong.com/wiki/AIXI}} \textsc{aixi} should go in this category: for a mind of
infinite power, it's awfully stupid---poor thing
can't even recognize itself in a mirror. But that is a
topic for another time.

{
 My primary moral is to \textit{resist the temptation to generalize
over all of mind design space}.}


 If we focus on the bounded subspace of mind design space that
contains all those minds whose makeup can be specified in a trillion
bits or less, then every universal generalization that you make has two
to the trillionth power chances to be falsified.


 Conversely, every \textit{existential}
generalization---``there exists at least one mind such
that $X$''---has two to the trillionth power chances to
be true.


 So you want to resist the temptation to say either that
\textit{all} minds do something, or that \textit{no} minds do
something.


 The main reason you could find yourself thinking that you know
what a fully generic mind will (won't) do is if you put
yourself in that mind's shoes---imagine what you would
do in that mind's place---and get back a generally
wrong, anthropomorphic answer. (Albeit that it is true in at least one
case, since you are yourself an example.) Or if you imagine a mind
doing something, and then imagining the reasons \textit{you}
wouldn't do it---so that you imagine that a mind of
that type can't exist, that the ghost in the machine
will look over the corresponding source code and hand it back.


 Somewhere in mind design space is at least one mind with almost
any kind of logically consistent property you care to imagine.


 And this is important because it emphasizes the importance of
discussing \textit{what happens, lawfully, and why,} as a causal result
of a mind's particular constituent makeup; somewhere in
mind design space is a mind that does it differently.


 Of course, you could always say that anything that
doesn't do it your way is ``by
definition'' not a mind; after all,
it's obviously stupid. I've seen people
try that one too.

\myendsectiontext

\chapter{Value Theory}

\mysection{Where Recursive Justification Hits Bottom}


 Why do I believe that the Sun will rise tomorrow? 


 Because I've seen the Sun rise on thousands of
previous days.


 Ah\,\ldots but why do I believe the future will be like the past?


 Even if I go past the mere surface observation of the Sun rising,
to the apparently universal and exceptionless laws of gravitation and
nuclear physics, then I am still left with the question:
``Why do I believe this will also be true
tomorrow?''


 I could appeal to Occam's Razor, the principle of
using the simplest theory that fits the facts\,\ldots but why believe in
Occam's Razor? Because it's been
successful on past problems? But who says that this means
Occam's Razor will work tomorrow?


 And lo, the one said:

{
 Science also depends on unjustified assumptions. Thus science is
ultimately based on faith, \textit{so don't you
criticize me} for believing in [silly-belief-\#238721].}


 As I've previously observed:


 It's a most peculiar psychology---this business of
``Science is based on faith too, so
there!'' Typically this is said by people who claim
that faith is a \textit{good} thing. Then why do they say
``Science is based on faith too!''
in that angry-triumphal tone, rather than as a compliment?


 Arguing that you should be immune to criticism is rarely a good
sign.


 But this doesn't answer the legitimate
philosophical dilemma: If every belief must be justified, and those
justifications in turn must be justified, then how is the infinite
recursion terminated?


 And if you're allowed to end in something
assumed-without-justification, then why aren't you
allowed to assume \textit{any old thing} without justification?


 A similar critique is sometimes leveled against Bayesianism---that
it requires assuming some prior---by people who apparently think that
the problem of induction is a \textit{particular} problem of
Bayesianism, which you can avoid by using classical statistics.


 But first, let it be clearly admitted that the rules of Bayesian
updating do \textit{not} of themselves solve the problem of induction.


 Suppose you're drawing red and white balls from an
urn. You observe that, of the first 9 balls, 3 are red and 6 are white.
What is the probability that the next ball drawn will be red?


 That depends on your prior beliefs about the urn. If you think the
urn-maker generated a uniform random number between 0 and 1, and used
that number as the fixed probability of each ball being red, then the
answer is 4/11 (by Laplace's Law of Succession). If you
think the urn originally contained 10 red balls and 10 white balls,
then the answer is 7/11.


 Which goes to say that with the right prior---or rather the wrong
prior---the chance of the Sun rising tomorrow would seem to go
\textit{down} with each succeeding day\,\ldots if you were absolutely
certain, a priori, that there was a great barrel out there from which,
on each day, there was drawn a little slip of paper that determined
whether the Sun rose or not; and that the barrel contained only a
limited number of slips saying
``Yes,'' and the slips were drawn
without replacement.


 There are possible minds in mind design space who have
anti-Occamian and anti-Laplacian priors; they believe that simpler
theories are less likely to be correct, and that the more often
something happens, the less likely it is to happen again.


 And when you ask these strange beings why they keep using priors
that never seem to work in real life\,\ldots they reply,
``Because it's never worked for us
before!''


 Now, one lesson you might derive from this is
``Don't be born with a stupid
prior.'' This is an amazingly helpful principle on
many real-world problems, but I doubt it will satisfy philosophers.


 Here's how I treat this problem myself: I try to
approach questions like ``Should I trust my
brain?'' or ``Should I trust
Occam's Razor?'' as though they were
\textit{nothing special}{}---or at least, nothing special as deep
questions go.


 Should I trust Occam's Razor? Well, how well does
(any particular version of) Occam's Razor seem to work
in practice? What kind of probability-theoretic justifications can I
find for it? When I look at the universe, does it seem like the kind of
universe in which Occam's Razor would work well?


 Should I trust my brain? Obviously not; it doesn't
always work. But nonetheless, the human brain seems much more powerful
than the most sophisticated computer programs I could consider trusting
otherwise. How well does my brain work in practice, on which sorts of
problems?


 When I examine the causal history of my brain---its origins in
natural selection---I find, on the one hand, all sorts of specific
reasons for doubt; my brain was optimized to run on the ancestral
savanna, not to do math. But on the other hand, it's
also clear why, loosely speaking, it's possible that
the brain really could work. Natural selection would have quickly
eliminated brains so \textit{completely} unsuited to reasoning, so
\textit{anti-}helpful, as anti-Occamian or anti-Laplacian priors.


 So what I did in practice does \textit{not} amount to declaring a
sudden halt to questioning and justification. I'm not
halting the chain of examination at the point that I encounter
Occam's Razor, or my brain, or some other
unquestionable. The chain of examination continues---but it continues,
unavoidably, using my current brain and my current grasp on reasoning
techniques. \textit{What else could I possibly use?}


 Indeed, no matter \textit{what} I did with this dilemma, it would
be me doing it. Even if I trusted something else, like some computer
program, it would be my own decision to trust it.


 The technique of rejecting beliefs that have absolutely no
justification is in general an extremely important one. I sometimes say
that the fundamental question of rationality is ``Why
do you believe what you believe?'' I
don't even want to say something that \textit{sounds}
like it might allow a single exception to the rule that everything
needs justification.


 Which is, itself, a dangerous sort of motivation; you
can't always avoid everything that might be risky, and
when someone annoys you by saying something silly, you
can't reverse that stupidity to arrive at
intelligence.


 But I would nonetheless emphasize the difference between saying:


 Here is this assumption I cannot justify, which must be simply
taken, and not further examined.


 Versus saying:


 Here the inquiry continues to examine this assumption, with the
full force of my \textit{present intelligence}{}---as opposed to the
full force of something else, like a random number generator or a magic
8-ball---even though my present intelligence happens to be founded on
this assumption.


 Still\,\ldots wouldn't it be nice if we could
examine the problem of how much to trust our brains \textit{without}
using our current intelligence? Wouldn't it be nice if
we could examine the problem of how to think, \textit{without} using
our current grasp of rationality?


 When you phrase it \textit{that} way, it starts looking like the
answer might be ``No.''


 E. T. Jaynes used to say that you must always use all the
information available to you---he was a Bayesian probability theorist,
and had to clean up the paradoxes other people generated when they used
different information at different points in their calculations. The
principle of ``\textit{Always put forth your true best
effort}'' has at least as much appeal as
``\textit{Never do anything that might look
circular.}'' After all, the alternative to putting
forth your best effort is presumably doing less than your best.


 \textit{But still}\,\ldots wouldn't it be nice if
there were some way to justify using Occam's Razor, or
justify predicting that the future will resemble the past,
\textit{without} assuming that those methods of reasoning which have
worked on previous occasions are better than those which have
continually failed?


 Wouldn't it be nice if there were some chain of
justifications that neither ended in an unexaminable assumption, nor
was forced to examine itself under its own rules, but, instead, could
be explained starting from absolute scratch to an ideal philosophy
student of perfect emptiness?


 Well, I'd certainly be interested, but I
don't expect to see it done any time soon. There is no
perfectly empty ghost-in-the-machine; there is no argument that you can
explain to a rock.


 Even if someone cracks the First Cause problem and comes up with
\textit{the actual reason the universe is simple, which does not itself
presume a simple universe\,\ldots} then I would still expect that the
explanation could only be understood by a mindful listener, and not by,
say, a rock. A listener that didn't start out already
implementing modus ponens might be out of luck.


 So, at the end of the day, what happens when someone keeps asking
me ``Why do you believe what you
believe?''


 At present, I start going around in a loop at the point where I
explain, ``I predict the future as though it will
resemble the past on the simplest and most stable level of organization
I can identify, because previously, this rule has usually worked to
generate good results; and using the simple assumption of a simple
universe, I can see \textit{why} it generates good results; and I can
even see how my brain might have evolved to be able to observe the
universe with some degree of accuracy, if my observations are
correct.''


 But then\,\ldots haven't I just licensed
\textit{circular logic}?

{
 Actually, I've just licensed \textit{reflecting on
your mind's degree of trustworthiness, using your
current mind as opposed to something else.}}


 Reflection of this sort is, indeed, the reason we reject most
circular logic in the first place. We want to have a coherent causal
story about how our mind comes to know something, a story that explains
how the process we used to arrive at our beliefs is itself trustworthy.
This is the essential demand behind the rationalist's
fundamental question, ``Why do you believe what you
believe?''


 Now suppose you write on a sheet of paper: ``(1)
Everything on this sheet of paper is true, (2) The mass of a helium
atom is 20 grams.'' If that trick actually
\textit{worked in real life}, you would be able to know the true mass
of a helium atom just by believing some circular logic that asserted
it. Which would enable you to arrive at a true map of the universe
sitting in your living room with the blinds drawn. Which would violate
the Second Law of Thermodynamics by generating information from
nowhere. Which would not be a plausible story about how your mind could
end up believing something true.


 \textit{Even if} you started out believing the sheet of paper, it
would not seem that you had any reason for why the paper corresponded
to reality. It would just be a miraculous coincidence that (a) the mass
of a helium atom was 20 grams, and (b) the paper happened to say so.


 Believing self-validating statement sets does not in general seem
like it should work to map external reality---when we \textit{reflect
on it as a causal story about minds}{}---using, of course, our
\textit{current} minds to do so.


 But what about evolving to give more credence to simpler beliefs,
and to believe that algorithms which have worked in the past are more
likely to work in the future? \textit{Even when} we reflect on this as
a causal story of the origin of minds, it still seems like this could
plausibly work to map reality.


 And what about trusting reflective coherence in general?
Wouldn't most possible minds, randomly generated and
allowed to settle into a state of reflective coherence, be incorrect?
Ah, but \textit{we} evolved by natural selection; we were not generated
randomly.


 If trusting this argument seems worrisome to you, then forget
about the problem of philosophical justifications, and ask yourself
whether it's really truly true.


 (You will, of course, use your own mind to do so.)


 Is this the same as the one who says, ``I believe
that the Bible is the word of God, because the Bible says
so''?


 Couldn't they argue that their blind faith must
also have been placed in them by God, and is therefore trustworthy?


 In point of fact, when religious people finally come to reject the
Bible, they do \textit{not} do so by magically jumping to a
non-religious state of pure emptiness, and then evaluating their
religious beliefs in that non-religious state of mind, and then jumping
back to a new state with their religious beliefs removed.


 People go from being religious to being non-religious because even
in a religious state of mind, doubt seeps in. They notice their prayers
(and worse, the prayers of seemingly much worthier people) are not
being answered. They notice that God, who speaks to them in their heart
in order to provide seemingly consoling answers about the universe, is
not able to tell them the hundredth digit of pi (which would be a lot
more reassuring, if God's purpose were reassurance).
They examine the story of God's creation of the world
and damnation of unbelievers, and it doesn't seem to
make sense even under their own religious premises.


 Being religious doesn't make you less than human.
Your brain still has the abilities of a human brain. The dangerous part
is that being religious might stop you from \textit{applying} those
native abilities to your religion---stop you from \textit{reflecting
fully} on yourself. People don't heal their errors by
resetting themselves to an ideal philosopher of pure emptiness and
reconsidering all their sensory experiences from scratch. They heal
themselves by becoming more willing to question their current beliefs,
using more of the power of their current mind.

{
 This is why it's important to distinguish between
\textit{reflecting on your mind using your mind} (it's
not like you can use anything else) and \textit{having an
unquestionable assumption that you can't reflect on.}}


 ``I believe that the Bible is the word of God,
because the Bible says so.'' Well, if the Bible
\textit{were} an astoundingly reliable source of information about all
other matters, if it had not said that grasshoppers had four legs or
that the universe was created in six days, but had instead contained
the Periodic Table of Elements centuries before chemistry---if the
Bible had served us only well and told us only truth---then we might,
in fact, be inclined to take seriously the additional statement in the
Bible, that the Bible had been generated by God. We might not trust it
entirely, because it could also be aliens or the Dark Lords of the
Matrix, but it would at least be worth taking seriously.


 Likewise, if everything \textit{else} that priests had told us
turned out to be true, we might take more seriously their statement
that faith had been placed in us by God and was a systematically
trustworthy source---especially if people could divine the hundredth
digit of pi by faith as well.


 So the important part of appreciating the circularity of
``I believe that the Bible is the word of God, because
the Bible says so,'' is not so much that you are
going to reject the idea of reflecting on your mind using your current
mind. Rather, you realize that anything which calls into question the
Bible's trustworthiness also calls into question the
Bible's assurance of its trustworthiness.


 This applies to rationality too: if the future should cease to
resemble the past---even on its lowest and simplest and most stable
observed levels of organization---well, mostly, I'd be
dead, because my brain's processes require a lawful
universe where chemistry goes on working. But if somehow I survived,
then I would have to start questioning the principle that the future
should be predicted to be like the past.


 But for now\,\ldots what's the \textit{alternative}
to saying, ``I'm going to believe that
the future will be like the past on the most stable level of
organization I can identify, because that's previously
worked better for me than any other algorithm I've
tried''?


 Is it saying, ``I'm going to
believe that the future will \textit{not} be like the past, because
that algorithm has always failed before''?

{
 At this point I feel obliged to drag up the point that
rationalists are not out to win arguments with ideal philosophers of
perfect emptiness; we are simply out to win. For which purpose we want
to get as close to the truth as we can possibly manage. So at the end
of the day, I embrace the principle: ``Question your
brain, question your intuitions, question your principles of
rationality, \textit{using the full current force of your mind, and
doing the best you can do at every point.}''}


 If one of your current principles does come up wanting---according
to your own mind's examination, since you
can't step outside yourself{}---then change it! And
then go back and look at things again, using your new improved
principles.


 The point is not to be reflectively consistent. The point is to
win. But \textit{if} you look at yourself and play to win, you are
making yourself more reflectively consistent---that's
what it means to ``play to win''
while ``looking at yourself.''


 Everything, without exception, needs justification.
Sometimes---unavoidably, as far as I can tell---those justifications
will go around in reflective loops. I do think that reflective loops
have a meta-character which should enable one to distinguish them, by
common sense, from circular logics. But anyone seriously considering a
circular logic in the first place is probably out to lunch in matters
of rationality, and will simply insist that their circular logic is a
``reflective loop'' even if it
consists of a single scrap of paper saying ``Trust
me.'' Well, you can't always optimize
your rationality techniques according to the sole consideration of
preventing those bent on self-destruction from abusing them.

{
 The important thing is to \textit{hold nothing back} in your
criticisms of how to criticize; nor should you regard the
unavoidability of loopy justifications as a warrant of \textit{immunity
from questioning}.}


 Always apply full force, whether it loops or not---do the best you
can possibly do, whether it loops or not---and play, ultimately, to
win.

\myendsectiontext

\mysection{My Kind of Reflection}


 In Where Recursive Justification Hits Bottom, I concluded that
it's okay to use induction to reason about the
probability that induction will work in the future, given that
it's worked in the past; or to use
Occam's Razor to conclude that the simplest explanation
for why Occam's Razor works is that the universe itself
is fundamentally simple. 


 Now I am far from the first person to consider reflective
application of reasoning principles. Chris Hibbert compared my view to
Bartley's Pan-Critical Rationalism (I was wondering
whether that would happen). So it seems worthwhile to state what I see
as the distinguishing features of my view of reflection, which may or
may not happen to be shared by any other philosopher's
view of reflection.

\begin{itemize}
\item {
 All of my philosophy here \textit{actually} comes from trying to
figure out how to build a self-modifying AI that applies its own
reasoning principles to itself in the process of rewriting its own
source code. So whenever I talk about using induction to license
induction, I'm \textit{really} thinking about an
inductive AI considering a rewrite of the part of itself that performs
induction. If you wouldn't want the AI to rewrite its
source code to not use induction, your philosophy had better not label
induction as unjustifiable.}

\item {
 One of the most powerful principles I know for AI in general is
that the true Way generally turns out to be
\textit{naturalistic}{}---which for reflective reasoning means treating
transistors inside the AI just as if they were transistors found in the
environment, \textit{not} an ad-hoc special case. This is the real
source of my insistence in Recursive Justification that questions like
``How well does my version of Occam's
Razor work?'' should be considered just like an
ordinary question---or at least an ordinary very deep question. I
strongly suspect that a correctly built AI, in pondering modifications
to the part of its source code that implements Occamian reasoning, will
not have to do anything special as it ponders---in particular, it
shouldn't have to make a special effort to avoid using
Occamian reasoning.}

\item {
 I don't think that ``reflective
coherence'' or ``reflective
consistency'' should be considered as a desideratum
in itself. As I say in The Twelve Virtues and The Simple Truth, if you
make five accurate maps of the same city, then the maps will
necessarily be consistent with each other; but if you draw one map by
fantasy and then make four copies, the five will be consistent but not
accurate. In the same way, no one is deliberately pursuing reflective
consistency, and reflective consistency is not a special warrant of
trustworthiness; the goal is to win. But anyone who pursues the goal of
winning, using their current notion of winning, and modifying their own
source code, will end up reflectively consistent as a side
effect---just like someone continually striving to improve their map of
the world should find the parts becoming more consistent among
themselves, as a side effect. If you put on your AI goggles, then the
AI, rewriting its own source code, is not trying to make itself
``reflectively consistent''---it is
trying to optimize the expected utility of its source code, and it
happens to be doing this using its current mind's
anticipation of the consequences.}

\item {
 One of the ways I license using induction and
Occam's Razor to consider
``induction'' and
``Occam's Razor'' is
by appealing to E. T. Jaynes's principle that we should
always use all the information available to us (computing power
permitting) in a calculation. If you think induction works, then you
should use it in order to use your maximum power, including when
you're thinking about induction.}

\item {
 In general, I think it's valuable to distinguish a
defensive posture where you're imagining how to justify
your philosophy to a philosopher that questions you, from an aggressive
posture where you're trying to get as close to the
truth as possible. So it's not that being suspicious of
Occam's Razor, but using your current mind and
intelligence to inspect it, shows that you're being
\textit{fair} and \textit{defensible} by questioning your foundational
beliefs. Rather, the reason why you would inspect
Occam's Razor is to see if you could improve your
application of it, or if you're worried it might really
be wrong. I tend to deprecate mere dutiful doubts.}

\item {
 If you run around inspecting your foundations, I expect you to
actually improve them, not just dutifully investigate. Our brains are
built to assess ``simplicity'' in a
certain intuitive way that makes Thor sound simpler than
Maxwell's Equations as an explanation for lightning.
But, having gotten a better look at the way the universe really works,
we've concluded that differential equations (which few
humans master) are actually \textit{simpler} (in an
information-theoretic sense) than heroic mythology (which is how most
tribes explain the universe). This being the case,
we've tried to import our notions of
Occam's Razor into math as well.}

\item {
 On the other hand, the improved foundations should still add up to
normality; 2 + 2 should still end up equalling 4, not something new and
amazing and exciting like ``fish.''}

\item {
 I think it's very important to distinguish between
the questions ``Why does induction
work?'' and ``Does induction
work?'' The reason \textit{why the universe itself is
regular} is still a mysterious question unto us, for now. Strange
speculations here may be temporarily needful. But on the other hand, if
you start claiming that the universe \textit{isn't
actually regular}, that the answer to ``Does induction
work?'' is
``No!,'' then you're
wandering into 2 + 2 = 3 territory. You're trying too
hard to make your philosophy interesting, instead of correct. An
inductive AI asking what probability assignment to make on the next
round is asking ``\textit{Does} induction
work?,'' and this is the question that it may answer
by inductive reasoning. If you ask ``\textit{Why} does
induction work?'' then answering
``Because induction works'' is
circular logic, and answering ``Because I believe
induction works'' is magical thinking.}

\item {
 I don't think that going around in a loop of
justifications through the meta-level is the same thing as circular
logic. I think the notion of ``circular
logic'' applies within the object level, and is
something that is definitely bad and forbidden, on the object level.
Forbidding \textit{reflective coherence} doesn't sound
like a good idea. But I haven't yet sat down and
formalized the exact difference---my reflective theory is something
I'm trying to work out, not something I have in hand.}
\end{itemize}

\myendsectiontext

\mysection{No Universally Compelling Arguments}


 What is so \textit{terrifying} about the idea that not every
possible mind might agree with us, even in principle? 


 For some folks, nothing---it doesn't bother them
in the slightest. And for some of \textit{those} folks, the
\textit{reason} it doesn't bother them is that they
don't have strong intuitions about standards and truths
that go beyond personal whims. If they say the sky is blue, or that
murder is wrong, that's just their personal opinion;
and that someone else might have a different opinion
doesn't surprise them.


 For other folks, a disagreement that persists even \textit{in
principle} is something they can't accept. And for some
of \textit{those} folks, the \textit{reason} it bothers them is that it
seems to them that if you allow that some people cannot be persuaded
\textit{even in principle} that the sky is blue, then
you're conceding that ``the sky is
blue'' is merely an \textit{arbitrary} personal
opinion.


 I've proposed that you should resist the
temptation to generalize over all of mind design space. If we restrict
ourselves to minds specifiable in a trillion bits or less, then each
\textit{universal} generalization ``All minds $m:
X(m)$'' has two to the trillionth chances to be false,
while each \textit{existential} generalization
``Exists mind $m: X(m)$'' has two to
the trillionth chances to be true.


 This would seem to argue that for every argument $A$, howsoever
convincing it may seem to us, there exists at least one possible mind
that doesn't buy it.


 And the surprise and/or horror of this prospect (for some) has a
great deal to do, I think, with the intuition of the
ghost-in-the-machine---a ghost with some irreducible core that any
\textit{truly valid} argument will convince.


 I have previously spoken of the intuition whereby people map
\textit{programming a computer} onto \textit{instructing a human
servant}, so that the computer might rebel against its code---or
perhaps look over the code, decide it is not reasonable, and hand it
back.


 If there were a ghost in the machine and the ghost contained an
irreducible core of reasonableness, above which any mere code was only
a suggestion, then there might be universal arguments. Even if the
ghost were initially handed code-suggestions that contradicted the
Universal Argument, when we finally did expose the ghost to the
Universal Argument---or the ghost could discover the Universal Argument
on its own, that's also a popular concept---the ghost
would just override its own, mistaken source code.


 But as the student programmer once said, ``I get
the feeling that the computer just skips over all the
comments.'' The code is not given to the AI; the code
\textit{is} the AI.


 If you switch to the physical perspective, then the notion of a
Universal Argument seems noticeably unphysical. If
there's a physical system that at time $T$, after being
exposed to argument $E$, does $X$, then there ought to be another physical
system that at time $T$, after being exposed to environment $E$, does $Y$.
Any thought has to be implemented \textit{somewhere}, in a physical
system; any belief, any conclusion, any decision, any motor output. For
every lawful causal system that zigs at a set of points, you should be
able to specify another causal system that lawfully zags at the same
points.


 Let's say there's a mind with a
transistor that outputs +3 volts at time $T$, indicating that it has just
assented to some persuasive argument. Then we can build a highly
similar physical cognitive system with a tiny little trapdoor
underneath the transistor containing a little gray man who climbs out
at time $T$ and sets that transistor's output to -3
volts, indicating non-assent. Nothing acausal about that; the little
gray man is there because we built him in. The notion of an argument
that convinces \textit{any} mind seems to involve a little blue woman
who was \textit{never} built into the system, who climbs out of
literally \textit{nowhere}, and strangles the little gray man, because
that transistor has just \textit{got} to output +3 volts.
It's such a \textit{compelling argument}, you see.


 But compulsion is not a property of arguments; it is a property of
minds that process arguments.


 So the reason I'm arguing against the ghost
isn't \textit{just} to make the point that (1) Friendly
AI has to be explicitly programmed and (2) the laws of physics do not
forbid Friendly AI. (Though of course I take a certain interest in
establishing this.)


 I also wish to establish the notion of a mind as a \textit{causal,
lawful, physical system} in which there \textit{is no} irreducible
central ghost that looks over the neurons/code and decides whether they
are good suggestions.


 (There is a concept in Friendly AI of \textit{deliberately}
programming an FAI to review its own source code and possibly hand it
back to the programmers. But the mind that reviews is not irreducible,
it is just the mind that you created. The FAI is renormalizing itself
\textit{however it was designed to do so}; there is nothing acausal
reaching in from outside. A bootstrap, not a skyhook.)


 All this echoes back to the worry about a
Bayesian's
``arbitrary'' priors. If you show me
one Bayesian who draws 4 red balls and 1 white ball from a barrel, and
who assigns probability 5/7 to obtaining a red ball on the next
occasion (by Laplace's Rule of Succession), then I can
show you another mind which obeys Bayes's Rule to
conclude a 2/7 probability of obtaining red on the next
occasion---corresponding to a different prior belief about the barrel,
but, perhaps, a less ``reasonable''
one.


 Many philosophers are convinced that because you can in-principle
construct a prior that updates to any given conclusion on a stream of
evidence, therefore, Bayesian reasoning must be
``arbitrary,'' and the whole schema
of Bayesianism flawed, because it relies on
``unjustifiable'' assumptions, and
indeed ``unscientific,'' because you
cannot force any possible journal editor in mindspace to agree with
you.


 And this (I replied) relies on the notion that by unwinding all
arguments and their justifications, you can obtain an ideal philosophy
student of perfect emptiness, to be convinced by a line of reasoning
that begins from absolutely no assumptions.


 But who is this ideal philosopher of perfect emptiness? Why, it is
just the irreducible core of the ghost!


 And that is why (I went on to say) the result of trying to remove
all assumptions from a mind, and unwind to the perfect absence of any
prior, is not an ideal philosopher of perfect emptiness, but a rock.
What is left of a mind after you remove the source code? Not the ghost
who looks over the source code, but simply\,\ldots no ghost.


 So---and I shall take up this theme again later---wherever you are
to locate your notions of \textit{validity} or \textit{worth} or
\textit{rationality} or \textit{justification} or even
\textit{objectivity}, it cannot rely on an argument that is
\textit{universally compelling to all physically possible minds.}


 Nor can you ground validity in a sequence of justifications that,
beginning from nothing, persuades a perfect emptiness.


 Oh, there might be argument sequences that would compel any
neurologically intact \textit{human}{}---like the argument I use to
make people let the AI out of the box\footnote{Just kidding.\comment{1}}{}---but that
is hardly the same thing from a philosophical perspective.


 The first great failure of those who try to consider Friendly AI
is the One Great Moral Principle That Is All We Need To
Program---a.k.a. the fake utility function---and of this I have already
spoken.


 But the even worse failure is the One Great Moral Principle We
Don't Even \textit{Need} To Program Because Any AI Must
Inevitably Conclude It. This notion exerts a terrifying unhealthy
fascination on those who spontaneously reinvent it; they dream of
commands that no sufficiently advanced mind can disobey. The gods
themselves will proclaim the rightness of their philosophy! (E.g., John
C. Wright, Marc Geddes.)

{
 There is also a less severe version of the failure, where the one
does not \textit{declare} the One True Morality. Rather the one hopes
for an AI created \textit{perfectly free}, unconstrained by flawed
humans desiring slaves, so that the AI may arrive at virtue of its own
accord---virtue undreamed-of perhaps by the speaker, who confesses
themselves too flawed to teach an AI. (E.g., John K. Clark, Richard
Hollerith?, Eliezer\textsubscript{1996}.) This is a less tainted motive
than the dream of absolute command. But though \textit{this} dream
arises from virtue rather than vice, it is still based on a flawed
understanding of freedom, and will not actually \textit{work in real
life.} Of this, more to follow, of course.}

{
 John C. Wright, who was previously writing a very nice
transhumanist trilogy (first book: \textit{The Golden Age}), inserted a
huge Author Filibuster in the middle of his climactic third book,
describing in tens of pages his Universal Morality That Must Persuade
Any AI. I don't know if anything happened after that,
because I stopped reading. And then Wright converted to
Christianity---yes, seriously. So you \textit{really
don't} want to fall into this trap!}

\myendsectiontext


\bigskip

\mysection{Created Already in Motion}


 Lewis Carroll, who was also a mathematician, once wrote a short
dialogue called ``What the Tortoise said to
Achilles.''\footnote{\url{http://www.ditext.com/carroll/tortoise.html}} If you have not yet read this ancient
classic, consider doing so now. 


 The Tortoise offers Achilles a step of reasoning drawn from
Euclid's First Proposition:

\begin{quotation}

 ($A$) Things that are equal to the same are equal to each other.


 ($B$) The two sides of this Triangle are things that are equal to
the same.

{
  ($Z$) The two sides of this Triangle are equal to each other.}
\end{quotation}


 Tortoise: ``And if some reader had \textit{not}
yet accepted $A$ and $B$ as true, he might still accept the
\textit{sequence} as a \textit{valid} one, I
suppose?''


 Achilles: ``No doubt such a reader might exist.
He might say, `I accept as true the Hypothetical
Proposition that, \textit{if} $A$ and $B$ be true, $Z$ must be true; but, I
\textit{don't} accept $A$ and $B$ as true.'
Such a reader would do wisely in abandoning Euclid, and taking to
football.''


 Tortoise: ``And might there not \textit{also} be
some reader who would say, `I accept $A$ and $B$ as true,
but I \textit{don't} accept the
Hypothetical'?''


 Achilles, unwisely, concedes this; and so asks the Tortoise to
accept another proposition:

\begin{quotation}
{
  ($C$) If $A$ and $B$ are true, $Z$ must be true.}
\end{quotation}


 But, asks, the Tortoise, suppose that he accepts $A$ and $B$ and $C$,
but not $Z$?


 Then, says, Achilles, he must ask the Tortoise to accept one more
hypothetical:

\begin{quotation}
{
  ($D$) If $A$ and $B$ and $C$ are true, $Z$ must be true.}
\end{quotation}


 Douglas Hofstadter paraphrased the argument some time later:

\begin{quotation}

 \textsc{Achilles}: ``If you have $[(A\text{ and }B) \rightarrow 
Z]$, and you also have $(A\text{ and }B)$, then surely you have
$Z$.''

{
 \textsc{Tortoise}: ``Oh! You mean $((A\text{ and }B)$ and $[(A\text{ and
}B) \rightarrow  Z]) \rightarrow  Z$, don't
 you?''}
\end{quotation}


 As Hofstadter says, ``Whatever Achilles considers
a rule of inference, the Tortoise immediately flattens into a mere
string of the system. If you use only the letters $A$, $B$, and $Z$, you will
get a recursive pattern of longer and longer
strings.''


 This is the anti-pattern I call Passing the Recursive Buck; and
though the counterspell is sometimes hard to find, when found, it
generally takes the form The Buck Stops Immediately.


 The Tortoise's mind needs the \textit{dynamic} of
adding $Y$ to the belief pool when $X$ and $(X \rightarrow Y)$ are
previously in the belief pool. If this dynamic is not present---a rock,
for example, lacks it---then you can go on adding in $X$ and $(X
\rightarrow  Y)$ and $((X\text{ and }(X \rightarrow  Y)) \rightarrow  Y)$
until the end of eternity, without ever getting to $Y$.


 The phrase that once came into my mind to describe this
requirement is that a mind must be \textit{created already in motion.}
There is no argument so compelling that it will give dynamics to a
static thing. There is no computer program so \textit{persuasive} that
you can run it on a rock.


 And even if you have a mind that \textit{does} carry out modus
ponens, it is futile for it to have such beliefs as\,\ldots

\begin{quotation}

 ($A$) If a toddler is on the train tracks, then pulling them off is
fuzzle.

{
  ($B$) There is a toddler on the train tracks.}
\end{quotation}


 \ldots unless the mind also \textit{implements}:

\begin{quotation}
{
 \textit{Dynamic:} When the belief pool contains
``$X$ is fuzzle,'' send $X$ to the
 action system.}
\end{quotation}


 By ``dynamic'' I mean a
property of a physically implemented cognitive system's
\textit{development over time}. A
``dynamic'' is something that
\textit{happens inside} a cognitive system, \textit{not} data that it
stores in memory and manipulates. Dynamics are the manipulations. There
is no way to write a dynamic on a piece of paper, because the paper
will just lie there. So the text immediately above, which says
``dynamic,'' is not dynamic. If I
wanted the text to \textit{be} dynamic and not just \textit{say}
``dynamic,'' I would have to write a
Java applet.


 Needless to say, having the belief\,\ldots

\begin{quotation}
{
 ($C$) If the belief pool contains ``$X$ is
fuzzle,'' then ``send
`$X$' to the action
system'' is fuzzle.}
\end{quotation}


 \ldots won't help unless the mind already
implements the \textit{behavior} of translating hypothetical actions
labeled ``fuzzle'' into actual motor
actions.


 By dint of careful arguments about the nature of cognitive
systems, you might be able to prove\,\ldots

\begin{quotation}
{
 ($D$) A mind with a dynamic that sends plans labeled
``fuzzle'' to the action system is
  more fuzzle than minds that don't.}
\end{quotation}


 \ldots but that \textit{still} won't help, unless
the listening mind \textit{previously} possessed the \textit{dynamic}
of swapping out its current source code for alternative source code
that is believed to be more fuzzle.


 This is why you can't argue fuzzleness into a
rock.

\myendsectiontext

\mysection{Sorting Pebbles into Correct Heaps}


 Once upon a time there was a strange little species---that might
have been biological, or might have been synthetic, and perhaps were
only a dream---whose passion was sorting pebbles into correct heaps. 


 They couldn't tell you \textit{why} some heaps
were correct, and some incorrect. But all of them agreed that the most
important thing in the world was to create correct heaps, and scatter
incorrect ones.


 Why the Pebblesorting People cared so much, is lost to this
history---maybe a Fisherian runaway sexual selection, started by sheer
accident a million years ago? Or maybe a strange work of sentient art,
created by more powerful minds and abandoned?


 But it mattered so drastically to them, this sorting of pebbles,
that all the Pebblesorting philosophers said in unison that
pebble-heap-sorting was the very meaning of their lives: and held that
the only justified reason to eat was to sort pebbles, the only
justified reason to mate was to sort pebbles, the only justified reason
to participate in their world economy was to efficiently sort pebbles.


 The Pebblesorting People all agreed on that, but they
didn't always agree on which heaps were correct or
incorrect.


 In the early days of Pebblesorting civilization, the heaps they
made were mostly small, with counts like 23 or 29; they
couldn't tell if larger heaps were correct or not.
Three millennia ago, the Great Leader Biko made a heap of 91 pebbles
and proclaimed it correct, and his legions of admiring followers made
more heaps likewise. But over a handful of centuries, as the power of
the Bikonians faded, an intuition began to accumulate among the
smartest and most educated that a heap of 91 pebbles was incorrect.
Until finally they came to know what they had done: and they scattered
all the heaps of 91 pebbles. Not without flashes of regret, for some of
those heaps were great works of art, but incorrect. They even scattered
Biko's original heap, made of 91 precious gemstones
each of a different type and color.


 And no civilization since has seriously doubted that a heap of 91
is incorrect.


 Today, in these wiser times, the size of the heaps that
Pebblesorters dare attempt has grown very much larger---which all agree
would be a most great and excellent thing, if only they could ensure
the heaps were really \textit{correct.} Wars have been fought between
countries that disagree on which heaps are correct: the Pebblesorters
will never forget the Great War of 1957, fought between
Y'ha-nthlei and
Y'not'ha-nthlei, over heaps of size
1957. That war, which saw the first use of nuclear weapons on the
Pebblesorting Planet, finally ended when the
Y'not'ha-nthleian philosopher
At'gra'len'ley
exhibited a heap of 103 pebbles and a heap of 19 pebbles side-by-side.
So persuasive was this argument that even Y'ha-nthlei
reluctantly conceded that it was best to stop building heaps of 1957
pebbles, at least for the time being.


 Since the Great War of 1957, countries have been reluctant to
openly endorse or condemn heaps of large size, since this leads so
easily to war. Indeed, some Pebblesorting philosophers---who seem to
take a tangible delight in shocking others with their cynicism---have
entirely denied the existence of pebble-sorting \textit{progress}; they
suggest that opinions about pebbles have simply been a random walk over
time, with no coherence to them, the illusion of progress created by
condemning all dissimilar pasts as incorrect. The philosophers point to
the disagreement over pebbles of large size, as proof that there is
nothing that makes a heap of size 91 really \textit{incorrect}{}---that
it was simply fashionable to build such heaps at one point in time, and
then at another point, fashionable to condemn them.
``But\,\ldots 13!'' carries no truck
with them; for to regard ``13!'' as
a persuasive counterargument is only another convention, they say. The
Heap Relativists claim that their philosophy may help prevent future
disasters like the Great War of 1957, but it is widely considered to be
a philosophy of despair.


 Now the question of what makes a heap correct or incorrect has
taken on new urgency; for the Pebblesorters may shortly embark on the
creation of self-improving Artificial Intelligences. The Heap
Relativists have warned against this project: They say that AIs, not
being of the species \textit{Pebblesorter sapiens}, may form their own
culture with entirely different ideas of which heaps are correct or
incorrect. ``They could decide that heaps of 8 pebbles
are correct,'' say the Heap Relativists,
``and while ultimately they'd be no
righter or wronger than us, still, \textit{our} civilization says we
shouldn't build such heaps. It is not in our interest
to create AI, unless all the computers have bombs strapped to them, so
that even if the AI thinks a heap of 8 pebbles is correct, we can force
it to build heaps of 7 pebbles instead. Otherwise,
\textsc{kaboom!}''


 But this, to most Pebblesorters, seems absurd. Surely a
sufficiently powerful AI---especially the
``superintelligence'' some
transpebblesorterists go on about---would be able to see \textit{at a
glance} which heaps were correct or incorrect! The thought of something
with a brain the size of a planet thinking that a heap of 8 pebbles was
correct is just too absurd to be worth talking about.


 Indeed, it is an utterly futile project to constrain how a
superintelligence sorts pebbles into heaps. Suppose that Great Leader
Biko had been able, in his primitive era, to construct a self-improving
AI; and he had built it as an expected utility maximizer whose utility
function told it to create as many heaps as possible of size 91.
Surely, when this AI improved itself far enough, and became smart
enough, then it would see at a glance that this utility function was
incorrect; and, having the ability to modify its own source code, it
would \textit{rewrite its utility function} to value more reasonable
heap sizes, like 101 or 103.


 And certainly not heaps of size 8. That would just be
\textit{stupid.} Any mind that stupid is too dumb to be a threat.


 Reassured by such common sense, the Pebblesorters pour full speed
ahead on their project to throw together lots of algorithms at random
on big computers until some kind of intelligence emerges. The whole
history of civilization has shown that richer, smarter, better educated
civilizations are likely to agree about heaps that their ancestors once
disputed. Sure, there are then larger heaps to argue about---but the
further technology has advanced, the larger the heaps that have been
agreed upon and constructed.


 Indeed, intelligence itself has always correlated with making
correct heaps---the nearest evolutionary cousins to the Pebblesorters,
the Pebpanzees, make heaps of only size 2 or 3, and occasionally stupid
heaps like 9. And other, even less intelligent creatures, like fish,
make no heaps at all.


 Smarter minds equal smarter heaps. Why would that trend break?

\myendsectiontext

\mysection{2{}-Place and 1{}-Place Words}


 I have previously spoken of the ancient, pulp-era magazine covers
that showed a bug-eyed monster carrying off a girl in a torn dress; and
about how people think as if sexiness is an inherent property of a sexy
entity, without dependence on the admirer. 


 ``Of \textit{course} the bug-eyed monster will
prefer human females to its own kind,'' says the
artist (who we'll call Fred); ``it can
see that human females have soft, pleasant skin instead of slimy
scales. It may be an alien, but it's not
\textit{stupid}{}---why are you expecting it to make such a basic
mistake about sexiness?''


 What is Fred's error? It is treating a function of
2 arguments (``2-place function''):

\begin{center}
 \texttt{Sexiness: Admirer, Entity} $\rightarrow $ [0,${\infty}$),
\end{center}



 as though it were a function of 1 argument
(``1-place function''):

\begin{center}
 \texttt{Sexiness: Entity} $\rightarrow $ [0,${\infty}$).
\end{center}



 If \texttt{Sexiness} is treated as a function that accepts only one \texttt{Entity}
as its argument, then of course \texttt{Sexiness} will appear to depend only on
the \texttt{Entity}, with nothing else being relevant.


 When you think about a two-place function as though it were a
one-place function, you end up with a Variable Question Fallacy / Mind
Projection Fallacy. Like trying to determine whether a building is
\textit{intrinsically} on the left or on the right side of the road,
independent of anyone's travel direction.


 An alternative and equally valid standpoint is that
``sexiness'' \textit{does} refer to
a one-place function---but each speaker uses a \textit{different}
one-place function to decide who to kidnap and ravish. Who says that
just because Fred, the artist, and Bloogah, the bug-eyed monster, both
use the word ``sexy,'' they must
mean the same thing by it?


 If you take this viewpoint, there is no paradox in speaking of
some woman intrinsically having 5 units of \texttt{Fred::Sexiness}. All
onlookers can agree on this fact, once \texttt{Fred::Sexiness} has been
specified in terms of curves, skin texture, clothing, status cues, etc.
This specification need \textit{make no mention of Fred}, only the
woman to be evaluated.


 It so happens that Fred, himself, \textit{uses} this algorithm to
select flirtation targets. But that doesn't mean the
algorithm itself has to \textit{mention} Fred. So
Fred's \texttt{Sexiness} function really \textit{is} a function
of one argument---the woman---on this view. I called it \texttt{Fred::Sexiness},
but remember that this \textit{name} refers to a function that is being
described independently of Fred. Maybe it would be better to write:

\begin{center}
\texttt{Fred::Sexiness == Sexiness\_20934}.
\end{center}


 It is an empirical fact about Fred that he uses the function
\texttt{Sexiness\_20934} to evaluate potential mates. Perhaps John uses exactly
the same algorithm; it doesn't matter where it comes
from once we have it.


 And similarly, the same woman has only 0.01 units of
\texttt{Sexiness\_72546}, whereas a slime mold has 3 units of \texttt{Sexiness\_72546}.
It happens to be an empirical fact that Bloogah uses \texttt{Sexiness\_72546} to
decide who to kidnap; that is, \texttt{Bloogah::Sexiness} names the fixed
Bloogah-independent mathematical object that is the function
\texttt{Sexiness\_72546}.


 Once we say that the woman has 0.01 units of \texttt{Sexiness\_72546} and 5
units of \texttt{Sexiness\_20934}, all observers can agree on this without
paradox.


 And the two 2-place and 1-place views can be unified using the
concept of ``currying,'' named after
the mathematician Haskell Curry. Currying is a technique allowed in
certain programming languages, where e.g.~instead of writing

\begin{center}
\texttt{x = plus(2, 3) (x = 5)},
\end{center}



 you can also write

\begin{quotation}
\texttt{y = plus(2)}

 (y is now a ``curried'' form of
the function \texttt{plus}, which has eaten a 2)

 \texttt{x = y(3) (x=5)}

 \texttt{z = y(7) (z=9)}.
\end{quotation}


 So plus is a 2-place function, but currying \texttt{plus}---letting it eat
only one of its two required arguments---turns it into a 1-place
function that adds 2 to any input. (Similarly, you could start with a
7-place function, feed it 4 arguments, and the result would be a
3-place function, etc.)


 A true purist would insist that all functions should be viewed, by
definition, as taking exactly one argument. On this view, plus accepts
one numeric input, and outputs a \textit{new} function; and this
\textit{new} function has one numeric input and finally outputs a
number. On this view, when we write plus(2, 3) we are really computing
plus(2) to get a function that adds 2 to any input, and then applying
the result to 3. A programmer would write this as:

\begin{center}
\texttt{plus: int $\rightarrow $ (int $\rightarrow $ int)}.
\end{center}


 This says that plus takes an int as an argument, and returns a
function of type \texttt{int $\rightarrow $ int}.


 Translating the metaphor back into the human use of words, we
could imagine that ``sexiness''
starts by eating an \texttt{Admirer}, and spits out the fixed
\textit{mathematical} object that describes how the \texttt{Admirer} currently
evaluates pulchritude. It is an \textit{empirical} fact about the
\texttt{Admirer} that their intuitions of desirability are computed in a way
that is isomorphic to this \textit{mathematical} function.


 Then the mathematical object spit out by currying
\texttt{Sexiness(Admirer)} can be applied to the \texttt{Woman}. If the \texttt{Admirer} was
originally Fred, \texttt{Sexiness(Fred)} will first return \texttt{Sexiness\_20934}. We
can then say it is an empirical fact about the \texttt{Woman}, independently of
Fred, that \texttt{Sexiness\_20934(Woman) = 5}.


 In Hilary Putnam's ``Twin
Earth'' thought experiment, there was a tremendous
philosophical brouhaha over whether it makes sense to postulate a Twin
Earth that is just like our own, except that instead of water being
H\textsubscript{2}O, water is a \textit{different} transparent flowing
substance, XYZ. And furthermore, set the time of the thought experiment
a few centuries ago, so in neither our Earth nor the Twin Earth does
anyone know how to test the alternative hypotheses of
H\textsubscript{2}O vs. XYZ. Does the word
``water'' \textit{mean} the same
thing in that world as in this one?


 Some said, ``Yes, because when an Earth person
and a Twin Earth person utter the word
`water,' they have the same sensory test
in mind.''


 Some said, ``No, because
`water' in our Earth means
H\textsubscript{2}O and `water' in the
Twin Earth means XYZ.''


 If you think of ``water'' as a
concept that \textit{begins} by eating a world to find out the
empirical true nature of that transparent flowing stuff, and
\textit{returns} a new fixed concept Water\textsubscript{42} or
H\textsubscript{2}O, then this world-eating concept is the same in our
Earth and the Twin Earth; it just returns different answers in
different places.


 If you think of ``water'' as
meaning H\textsubscript{2}O, then the concept does nothing different
when we transport it between worlds, and the Twin Earth contains no
H\textsubscript{2}O.


 And of course there is no point in arguing over what the sound of
the syllables ``wa-ter'' really
means.


 So should you pick one definition and use it consistently? But
it's not that easy to save yourself from confusion. You
have to train yourself to be \textit{deliberately aware} of the
distinction between the curried and uncurried forms of concepts.


 When you take the uncurried water concept and apply it in a
different world, it is the same concept but it \textit{refers} to a
different thing; that is, we are applying a constant world-eating
function to a different world and obtaining a different return value.
In the Twin Earth, XYZ is ``water''
and H\textsubscript{2}O is not; in our Earth, H\textsubscript{2}O is
``water'' and XYZ is not.


 On the other hand, if you take
``water'' to refer to what the prior
thinker would call ``the result of applying
`water' to \textit{our}
Earth,'' then in the Twin Earth, XYZ is not water and
H\textsubscript{2}O is.


 The whole confusingness of the subsequent philosophical debate
rested on a tendency to \textit{instinctively} curry concepts or
\textit{instinctively} uncurry them.


 Similarly it takes an extra step for Fred to realize that other
agents, like the Bug-Eyed-Monster agent, will choose kidnappees for
ravishing based on \texttt{Sexiness\_BEM(Woman)}, not \texttt{Sexiness\_Fred(Woman)}. To
do this, Fred must consciously re-envision \texttt{Sexiness} as a function with
two arguments. All Fred's brain does by instinct is
evaluate \texttt{Woman.sexiness}---that is, \texttt{Sexiness\_Fred(Woman)}; but
it's simply labeled \texttt{Woman.sexiness}.


 The fixed mathematical function \texttt{Sexiness\_20934} makes no mention
of Fred or the BEM, only women, so Fred does not \textit{instinctively}
see why the BEM would evaluate
``sexiness'' any differently. And
indeed the BEM would \textit{not} evaluate \texttt{Sexiness\_20934} any
differently, if for some odd reason it cared about the result of that
particular function; but it is an \textit{empirical} fact about the BEM
that it uses a different function \textit{to decide who to kidnap.}


 If you're wondering as to the point of this
analysis, try putting the above distinctions to work to Taboo such
confusing words as ``objective,''
``subjective,'' and
``arbitrary.''

\myendsectiontext

\mysection{What Would You Do Without Morality?}


 To those who say ``Nothing is
real,'' I once replied,
``That's great, but how does the
nothing work?'' 


 Suppose you learned, suddenly and definitively, that nothing is
moral and nothing is right; that everything is permissible and nothing
is forbidden.


 Devastating news, to be sure---and no, I am not telling you this
in real life. But suppose I \textit{did} tell it to you. Suppose that,
whatever you think is the basis of your moral philosophy, I
convincingly tore it apart, and moreover showed you that nothing could
fill its place. Suppose I \textit{proved} that all utilities equaled
zero.


 I know that Your-Moral-Philosophy is as true and undisprovable as
2 + 2 = 4. But still, I ask that you do your best to perform the
thought experiment, and concretely envision the possibilities even if
they seem painful, or pointless, or logically incapable of any good
reply.


 Would you still tip cabdrivers? Would you cheat on your
Significant Other? If a child lay fainted on the train tracks, would
you still drag them off?


 Would you still eat the same kinds of foods---or would you only
eat the cheapest food, since there's no reason you
\textit{should} have fun---or would you eat very expensive food, since
there's no reason you \textit{should} save money for
tomorrow?


 Would you wear black and write gloomy poetry and denounce all
altruists as fools? But there's no reason you
\textit{should} do that---it's just a cached thought.


 Would you stay in bed because there was no reason to get up? What
about when you finally got hungry and stumbled into the kitchen---what
would you do after you were done eating?


 Would you go on reading \textit{Overcoming Bias}, and if not, what
would you read instead? Would you still try to be rational, and if not,
what would you think instead?


 Close your eyes, take as long as necessary to answer:


 What \textit{would} you do, if nothing were right?

\myendsectiontext

\mysection{Changing Your Metaethics}


 If you say, ``Killing people is
wrong,'' that's morality. If you say,
``You shouldn't kill people because
God prohibited it,'' or ``You
shouldn't kill people because it goes against the trend
of the universe,'' that's
metaethics.


 Just as there's far more agreement on Special
Relativity than there is on the question ``What is
science?,'' people find it much easier to agree
``Murder is bad'' than to agree
\textit{what} makes it bad, or what it \textit{means} for something to
be bad.


 People do get attached to their metaethics. Indeed they frequently
insist that if their metaethic is wrong, all morality necessarily falls
apart. It might be interesting to set up a panel of
metaethicists---theists, Objectivists, Platonists, etc.---all of whom
agree that killing is wrong; all of whom disagree on what it means for
a thing to be ``wrong''; and all of
whom insist that if their metaethic is untrue, then morality falls
apart.


 Clearly a good number of people, if they are to make philosophical
progress, will need to shift metathics at some point in their lives.
\textit{You} may have to do it.


 At that point, it might be useful to have an open line of
retreat---not a retreat from morality, but a retreat from
Your-Current-Metaethic. (You know, the one that, if it is not true,
leaves no possible basis for not killing people.)


 And so I summarized below some possible lines of retreat. For I
have learned that to change metaethical beliefs is nigh-impossible in
the presence of an unanswered attachment.


 If, for example, someone believes the authority of
``Thou Shalt Not Kill'' derives from
God, then there are several and well-known things to say that can help
set up a line of retreat---as opposed to immediately attacking the
plausibility of God. You can say, ``Take personal
responsibility! Even if you got orders from God, it would be your own
decision to obey those orders. Even if God didn't order
you to be moral, you could just be moral anyway.''


 The above argument actually generalizes to quite a number of
metaethics---you just substitute Their-Favorite-Source-Of-Morality, or
even the word ``morality,'' for
``God.'' Even if your particular
source of moral authority failed, couldn't you just
drag the child off the train tracks \textit{anyway}? And indeed, who is
it but you that ever decided to follow this source of moral authority
in the first place? What responsibility are you really passing on?


 So the most important line of retreat is: If your metaethic stops
telling you to save lives, you can just drag the kid off the train
tracks anyway. To paraphrase Piers Anthony, only those who have
moralities worry over whether or not they have them. If your metaethic
tells you to kill people, why \textit{should} you even listen? Maybe
that which you would do even if there were no morality, \textit{is}
your morality.


 The point being, of course, not that no morality exists; but that
you can hold your will in place, and not fear losing sight of
what's important to you, while your notions of the
\textit{nature} of morality change.


 I've written some essays to set up lines of
retreat specifically for more \textit{naturalistic} metaethics. Joy in
the Merely Real and Explaining vs. Explaining Away argue that you
shouldn't be disappointed in any facet of life, just
because it turns out to be \textit{explicable} instead of inherently
mysterious: for if we cannot take joy in the merely real, our lives
shall be empty indeed.


 No Universally Compelling Arguments sets up a line of retreat from
the desire to have \textit{everyone} agree with our moral arguments.
There's a strong moral intuition which says that if our
moral arguments are right, by golly, we ought to be able to
\textit{explain} them to people. This may be valid among humans, but
you can't explain moral arguments to a rock. There is
no ideal philosophy student of perfect emptiness who can be persuaded
to implement modus ponens, starting without modus ponens. If a mind
doesn't contain that which is moved by your moral
arguments, it won't respond to them.


 But then isn't all morality circular logic, in
which case it falls apart? Where Recursive Justification Hits Bottom
and My Kind of Reflection explain the difference between a
self-consistent loop through the meta-level, and actual circular logic.
You shouldn't find yourself saying
``The universe is simple because it is
simple,'' or ``Murder is wrong
because it is wrong''; but neither should you try to
abandon Occam's Razor while evaluating the probability
that Occam's Razor works, nor should you try to
evaluate ``Is murder wrong?'' from
somewhere outside your brain. There is no ideal philosophy student of
perfect emptiness to which you can unwind yourself---try to find the
perfect rock to stand upon, and you'll end up as a
rock. So instead use the full force of your intelligence, your full
rationality and your full morality, when you investigate the
foundations of yourself.


 We can also set up a line of retreat for those afraid to allow a
\textit{causal} role for evolution, in their account of how morality
came to be. (Note that this is extremely distinct from granting
evolution a \textit{justificational} status in moral theories.) Love
has to come into existence somehow---for if we cannot take joy in
things that can come into existence, our lives will be empty indeed.
Evolution may not be a particularly \textit{pleasant} way for love to
evolve, but judge the end product---not the source. Otherwise you would
be committing what is known (appropriately) as The Genetic Fallacy:
causation is not the same concept as justification.
It's not like you can step outside the brain evolution
gave you; rebelling against nature is only possible from within
nature.


 The earlier series on Evolutionary Psychology should dispense with
the metaethical confusion of believing that any normal human being
thinks about their reproductive fitness, even unconsciously, in the
course of making decisions. Only evolutionary biologists even know how
to \textit{define} genetic fitness, and they know better than to think
it defines morality.


 Alarming indeed is the thought that morality might be computed
inside our own minds---doesn't this imply that morality
is a mere thought? Doesn't it imply that whatever you
think is right, must be right?


 No. Just because a quantity is computed inside your head
doesn't mean that the quantity computed is
\textit{about} your thoughts. There's a difference
between a calculator that calculates ``What is 2 +
3?'' and one that outputs ``What do
I output when someone presses `2,'
`+,' and
`3'?''


 Finally, if life seems painful, reductionism may not be the real
source of your problem---if living in a world of mere particles seems
too unbearable, maybe your life isn't exciting enough
right now?


 And if you're wondering why I deem this business
of metaethics important, when it is all going to end up adding up to
moral normality\,\ldots telling you to pull the child off the train
tracks, rather than the converse\,\ldots


 Well, there \textit{is} opposition to rationality from people who
think it drains meaning from the universe.


 And this is a special case of a general phenomenon, in which many
many people get messed up by misunderstanding where their morality
comes from. Poor metaethics forms part of the teachings of many a cult,
including the big ones. My target audience is not just people who are
afraid that life is meaningless, but also those who've
concluded that love is a delusion because real morality has to involve
maximizing your inclusive fitness, or those who've
concluded that unreturned kindness is evil because real morality arises
only from selfishness, etc.

\myendsectiontext

\mysection{Could Anything Be Right?}


 Years ago, Eliezer\textsubscript{1999} was convinced that he knew
\textit{nothing} about morality. 


 For all he knew, morality could require the extermination of the
human species; and if so he saw no virtue in taking a stand against
morality, because he thought that, by definition, if he postulated that
moral fact, that meant human extinction was what
``should'' be done.


 I thought I could \textit{figure out} what was right, perhaps,
given enough reasoning time and enough facts, but that I currently had
no information about it. I could not trust evolution which had built
me. What foundation did that leave on which to stand?


 Well, indeed Eliezer\textsubscript{1999} was massively mistaken
about the nature of morality, so far as his explicitly represented
philosophy went.

{
 But as Davidson once observed, if you believe that
``beavers'' live in deserts, are
pure white in color, and weigh 300 pounds when adult, then you do not
have any beliefs \textit{about} beavers, true or false. You must get at
least some of your beliefs right, before the remaining ones can be
wrong \textit{about} anything.\footnote{Rorty, ``Out of the Matrix: How the Late
Philosopher Donald Davidson Showed That Reality Can't
Be an Illusion.''\comment{1}}}


 My belief that I had \textit{no} information \textit{about}
morality was not internally consistent.


 Saying that I knew nothing felt virtuous, for I had once been
taught that it was virtuous to confess my ignorance.
``The only thing I know is that I know
nothing,'' and all that. But in this case I would
have been better off considering the admittedly exaggerated saying,
``The greatest fool is the one who is not aware they
are wise.'' (This is nowhere near the
\textit{greatest} kind of foolishness, but it is a kind of
foolishness.)


 Was it wrong to kill people? Well, I thought so, but I
wasn't sure; maybe it was right to kill people, though
that seemed less likely.


 What kind of \textit{procedure} would answer whether it was right
to kill people? I didn't know that either, but I
thought that if you built a generic superintelligence (what I would
later label a ``ghost of perfect
emptiness'') then it could, you know, reason about
what was likely to be right and wrong; and since it was
\textit{superintelligent}, it was bound to come up with the right
answer.


 The problem that I somehow managed not to think too hard about was
where the superintelligence would get the procedure that discovered the
procedure that discovered the procedure that discovered morality---if I
couldn't write it into the start state that wrote the
successor AI that wrote the successor AI.


 As Marcello Herreshoff later put it, ``We never
bother running a computer program unless we don't know
the output and we know an important fact about the
output.'' If I knew nothing about morality, and did
not even claim to know the nature of morality, then how could I
construct any computer program whatsoever---even a
``superintelligent'' one or a
``self-improving'' one---and claim
that it would output something called
``morality''?


 There are no-free-lunch theorems in computer science---in a
maxentropy universe, no plan is better on average than any other. If
you have no knowledge at all about
``morality,''
there's also no computational procedure that will seem
more likely than others to compute
``morality,'' and no meta-procedure
that's more likely than others to produce a procedure
that computes ``morality.''


 I thought that surely even a ghost of perfect emptiness, finding
that it knew nothing of morality, would see a moral imperative to
\textit{think about morality}.


 But the difficulty lies in the word \textit{think.} Thinking is
not an activity that a ghost of perfect emptiness is automatically able
to carry out. Thinking requires running some \textit{specific}
computation that is the thought. For a reflective AI to decide to think
requires that it know some computation which it believes is
\textit{more} likely to tell it what it wants to know than consulting
an Ouija board; the AI must also have a notion of how to interpret the
output.


 If one knows nothing about morality, what does the word
``should'' mean, at all? If you
don't know whether death is right or wrong---and
don't know how you can discover whether death is right
or wrong---and don't know whether any given procedure
might \textit{output} the procedure for saying whether death is right
or wrong---then what do these words,
``right'' and
``wrong,'' even \textit{mean}?


 If the words ``right'' and
``wrong'' have \textit{nothing}
baked into them---no starting point---if \textit{everything} about
morality is up for grabs, not just the content but the structure and
the starting point and the determination procedure---then what is their
meaning? What distinguishes, ``I don't
know what is right'' from ``I
don't know what is wakalixes''?


 A scientist may say that everything is up for grabs in science,
since any theory may be disproven; but then they have some idea of what
would count as \textit{evidence} that could disprove the theory. Could
there be something that would change what a scientist regarded as
evidence?


 Well, yes, in fact; a scientist who read some Karl Popper and
thought they knew what ``evidence''
meant could be presented with the coherence and uniqueness proofs
underlying Bayesian probability, and that might change their definition
of evidence. They might not have had any \textit{explicit notion} in
advance that such a proof could exist. But they would have had an
implicit notion. It would have been baked into their brains, if not
explicitly represented therein, that such-and-such an argument would in
fact persuade them that Bayesian probability gave a better definition
of ``evidence'' than the one they
had been using.


 In the same way, you could say, ``I
don't know what morality is, but I'll
know it when I see it,'' and make sense.


 But then you are not rebelling completely against your own evolved
nature. You are supposing that whatever has been baked into you to
recognize ``morality,'' is, if not
absolutely trustworthy, then at least your initial condition with which
you start debating. Can you trust your moral intuitions to give you any
information about morality \textit{at all}, when they are the product
of mere evolution?


 But if you discard every procedure that evolution gave you
\textit{and all its products}, then you discard your whole brain. You
discard everything that could potentially recognize morality when it
sees it. You discard everything that could potentially respond to moral
arguments by updating your morality. You even unwind past the unwinder:
you discard the intuitions underlying your conclusion that \textit{you
can't trust evolution} to be moral. It is your
\textit{existing} moral intuitions that tell you that evolution
doesn't seem like a very \textit{good} source of
morality. What, then, will the words
``right'' and
``should'' and
``better'' even \textit{mean}?


 Humans do not perfectly recognize truth when they see it, and
hunter-gatherers do not have an explicit concept of the Bayesian
criterion of evidence. But all our science and all our probability
theory was built on top of a chain of appeals to our instinctive notion
of ``truth.'' Had this core been
flawed, there would have been nothing we could do \textit{in principle}
to arrive at the present notion of science; the notion of science would
have just sounded completely unappealing and pointless.


 One of the arguments that might have shaken my teenage self out of
his mistake, if I could have gone back in time to argue with him, was
the question:


 Could there be some morality, some given rightness or wrongness,
that human beings do not perceive, do not want to perceive, will not
see any appealing moral argument for adopting, nor any moral argument
for adopting a procedure that adopts it, et cetera? Could there be a
morality, and ourselves \textit{utterly} outside its frame of
reference? But then what makes this thing \textit{morality}{}---rather
than a stone tablet somewhere with the words ``Thou
shalt murder'' written on them, with absolutely no
\textit{justification} offered?


 So all this suggests that you should be willing to accept that you
might know a \textit{little} about morality. Nothing unquestionable,
perhaps, but an initial state with which to start questioning yourself.
Baked into your brain but not explicitly known to you, perhaps; but
still, that which your brain \textit{would} recognize as \textit{right}
is what you are talking \textit{about}. You will accept at least enough
of the way you \textit{respond to moral arguments} as a
\textit{starting point} to identify
``morality'' as something to think
about.


 But that's a rather large step.


 It implies accepting your own mind as identifying a moral frame of
reference, rather than all morality being a great light shining from
beyond (that in principle you might not be able to perceive at all). It
implies accepting that even if there were a light and your brain
decided to recognize it as
``morality,'' it would still be your
own brain that recognized it, and you would not have evaded causal
responsibility---or evaded moral responsibility either, on my view.


 It implies dropping the notion that a ghost of perfect emptiness
will necessarily agree with you, because the ghost might occupy a
different moral frame of reference, respond to different arguments, be
\textit{asking a different question} when it computes what-to-do-next.


 And if you're willing to bake at least a few
things into the very meaning of this topic of
``morality,'' this quality of
\textit{rightness} that you are talking about when you talk about
``rightness''---if
you're willing to accept even that morality is what you
argue about when you argue about
``morality''---then why not accept
other intuitions, other pieces of yourself, into the starting point as
well?


 Why not accept that, \textit{ceteris paribus}, joy is preferable
to sorrow?


 You might later find some ground within yourself or built upon
yourself with which to criticize this---but why not accept it for now?
Not just as a personal preference, mind you; but as something baked
into the \textit{question} you ask when you ask ``What
is truly right''?


 But then you might find that you know rather a lot about morality!
Nothing certain---nothing unquestionable---nothing unarguable---but
still, quite a bit of information. Are you willing to relinquish your
Socratic ignorance?


 I don't argue by definitions, of course. But if
you claim to know nothing at all about morality, then you will have
problems with the \textit{meaning} of your words, not just their
plausibility.

\myendsectiontext


\bigskip

\mysection{Morality as Fixed Computation}


 Toby Ord commented:\footnote{\url{http://lesswrong.com/lw/sm/the_meaning_of_right/toz}}

\begin{quotation}

 Eliezer, I've just reread your article and was
wondering if this is a good quick summary of your position (leaving
apart how you got to it):

{
 ``I should $X$'' means that I
  would attempt to $X$ were I fully informed.}
\end{quotation}


 Toby's a pro, so if he didn't get
it, I'd better try again. Let me try a different tack
of explanation---one closer to the historical way that I arrived at my
own position.


 Suppose you build an AI, and---leaving aside that AI goal systems
cannot be built around English statements, and all such descriptions
are only dreams---you try to infuse the AI with the action-determining
principle, ``Do what I want.''


 And suppose you get the AI design close \textit{enough}{}---it
doesn't just end up tiling the universe with
paperclips, cheesecake or tiny molecular copies of satisfied
programmers---that its utility function actually assigns utilities as
follows, to the world-states we would describe in English as:

\begin{verbatim}
<Programmer weakly desires "X,"
quantity 20 of X exists>:         +20
<Programmer strongly desires "Y,"
quantity 20 of X exists>:           0
<Programmer weakly desires "X,"
quantity 30 of Y exists>:           0
<Programmer strongly desires "Y,"
quantity 30 of Y exists>:         +60
\end{verbatim}



 You perceive, of course, that this destroys the world.


 \ldots since if the programmer initially weakly wants
``$X$'' and $X$ is hard to obtain, the
AI will modify the programmer to strongly want
``$Y$,'' which is easy to create, and
then bring about lots of $Y$. The referent of ``$Y$''
might be, say, iron atoms---those are highly
stable.


 Can you patch this problem? No. As a general rule, it is not
possible to patch flawed Friendly AI designs.


 If you try to bound the utility function, or make the AI not care
about how \textit{much} the programmer wants things, the AI still has a
motive (as an \textit{expected} utility maximizer) to make the
programmer want something that can be obtained with a very high degree
of certainty.


 If you try to make it so that the AI can't modify
the programmer, then the AI can't talk to the
programmer (talking to someone modifies them).


 If you try to rule out a specific class of ways the AI could
modify the programmer, the AI has a motive to superintelligently seek
out loopholes and ways to modify the programmer indirectly.


 As a general rule, it is not possible to patch flawed FAI
designs.


 We, ourselves, do not imagine the future and judge that any future
in which our brains want something, and that thing exists, is a good
future. If we did think this way, we would say: ``Yay!
Go ahead and modify us to strongly want something
cheap!'' But we do \textit{not} say this, which means
that this AI design is \textit{fundamentally} flawed: it will choose
things very unlike what we would choose; it will judge desirability
very differently from how we judge it. This core disharmony cannot be
patched by ruling out a handful of specific failure modes.


 There's also a duality between Friendly AI
problems and moral philosophy problems---though you've
got to structure that duality in exactly the right way. So if you
prefer, the core problem is that the AI will choose in a way very
unlike the structure of what is, y'know, actually
\textit{right}{}---never mind the way we choose. Isn't
the whole point of this problem that merely \textit{wanting} something
doesn't \textit{make} it right?


 So this is the paradoxical-seeming issue which I have analogized
to the difference between:

\begin{quotation}

 A calculator that, when you press
``2,''
``+,'' and
``3,'' tries to compute:


 ``What is 2 + 3?''


 A calculator that, when you press
``2,''
``+,'' and
``3,'' tries to compute:

{
 ``What does this calculator output when you press
`2,' `+,'
and `3'?''}
\end{quotation}


 The Type 1 calculator, as it were, \textit{wants} to output 5.


 The Type 2 ``calculator'' could
return any result; and in the act of returning that result, it
\textit{becomes} the correct answer to the question that was internally
asked.


 We ourselves are like unto the Type 1 calculator. But the putative
AI is being built as though it were to reflect the Type 2 calculator.


 Now imagine that the Type 1 calculator is trying to build an AI,
only the Type 1 calculator doesn't \textit{know} its
own question. The calculator continually asks the question by its very
nature---it was born to ask that question, created already in motion
around that question---but the calculator has no insight into its own
transistors; it cannot print out the question, which is extremely
complicated and has no simple approximation.


 So the calculator wants to build an AI (it's a
pretty smart calculator, it just doesn't have access to
its own transistors) and have the AI give the right answer. Only the
calculator can't print out the question. So the
calculator wants to have the AI look at the calculator, where the
question is written, and answer the question that the AI will discover
implicit in those transistors. But this cannot be done by the cheap
shortcut of a utility function that says ``All $X$:
$\langle$ calculator asks
`$X$?,' answer
$X$$\rangle$: utility 1; else: utility
0'' because that actually mirrors the utility
function of a Type 2 calculator, not a Type 1 calculator.


 This gets us into FAI issues that I am not going into (some of
which I'm still working out myself).


 However, when you back out of the details of FAI design, and swap
back to the perspective of moral philosophy, then \textit{what we were
just talking about} was the dual of the moral issue:
``But if what's
`right' is a mere preference, then
anything that anyone wants is
`right.'\,''


 The key notion is the idea that what we name by
``right'' is a \textit{fixed}
question, or perhaps a \textit{fixed framework}. We can encounter moral
arguments that modify our terminal values, and even encounter moral
arguments that modify what we count as a moral argument; nonetheless,
it all grows out of a particular starting point. We do not experience
ourselves as embodying the question ``What will I
decide to do?'' which would be a Type 2 calculator;
anything we decided would thereby become right. We experience ourselves
as asking the embodied question: ``What will save my
friends, and my people, from getting hurt? How can we all have more
fun?\,\ldots'' where the
``\,\ldots'' is around a thousand other
things.


 So ``I should $X$'' does not mean
that I would attempt to $X$ were I fully informed.


 ``I should $X$'' means that $X$
answers the question, ``What will save my people? How
can we all have more fun? How can we get more control over our own
lives? What's the funniest jokes we can tell?\,\ldots''


 And I may not \textit{know} what this question \textit{is},
actually; I may not be able to print out my current guess nor my
surrounding framework; but I know, as all non-moral-relativists
instinctively know, that the question surely is not just
``How can I do whatever I want?''


 When these two formulations begin to seem as entirely distinct as
``snow'' and snow, then you shall
have created distinct buckets for the quotation and the referent.

\myendsectiontext

\mysection{Magical Categories}

\begin{quote}

 We can design intelligent machines so their primary, innate
emotion is unconditional love for all humans. First we can build
relatively simple machines that learn to recognize happiness and
unhappiness in human facial expressions, human voices and human body
language. Then we can hard-wire the result of this learning as the
innate emotional values of more complex intelligent machines,
positively reinforced when we are happy and negatively reinforced when
we are unhappy.

{\raggedleft
 {}---Bill Hibbard (2001), Super-Intelligent
Machines\footnote{Bill Hibbard, ``Super-Intelligent
Machines,'' \textit{ACM SIGGRAPH Computer Graphics}
35, no. 1 (2001): 13--15,
\url{http://www.siggraph.org/publications/newsletter/issues/v35/v35n1.pdf}.\comment{1}}
\par}
\end{quote}



 That was published in a peer-reviewed journal, and the author
later wrote a whole book about it, so this is not a strawman position
I'm discussing here.


 So\,\ldots um\,\ldots what could possibly go wrong\,\ldots


 When I mentioned (sec. 7.2)\footnote{Eliezer Yudkowsky, ``Artificial Intelligence
as a Positive and Negative Factor in Global Risk,''
in Bostrom and \'Cirkovi\'c, \textit{Global Catastrophic Risks},
308--345.\comment{2}} that
Hibbard's AI ends up tiling the galaxy with tiny
molecular smiley-faces, Hibbard wrote an indignant reply saying:

\begin{quote}
{
 When it is feasible to build a super-intelligence, it will be
feasible to build hard-wired recognition of ``human
facial expressions, human voices and human body
language'' (to use the words of mine that you quote)
that exceed the recognition accuracy of current humans such as you and
me, and will certainly not be fooled by ``tiny
molecular pictures of smiley-faces.'' You should not
assume such a poor implementation of my idea that it cannot make
discriminations that are trivial to current humans.}
\end{quote}


 As Hibbard also wrote ``Such obvious
contradictory assumptions show Yudkowsky's preference
for drama over reason,'' I'll go
ahead and mention that Hibbard illustrates a key point: There is no
professional certification test you have to take before you are allowed
to talk about AI morality. But that is not my primary topic today.
Though it is a crucial point about the state of the gameboard that most
AGI/FAI wannabes are so \textit{utterly} unsuited to the task that I
know no one cynical enough to imagine the horror without seeing it
firsthand. Even Michael Vassar was probably surprised his first time
through.


 No, today I am here to dissect ``You should not
assume such a poor implementation of my idea that it cannot make
discriminations that are trivial to current
humans.''


 Once upon a time---I've seen this story in several
versions and several places, sometimes cited as fact, but
I've never tracked down an original source---once upon
a time, I say, the US Army wanted to use neural networks to
automatically detect camouflaged enemy tanks.


 The researchers trained a neural net on 50 photos of camouflaged
tanks amid trees, and 50 photos of trees without tanks. Using standard
techniques for supervised learning, the researchers trained the neural
network to a weighting that correctly loaded the training set---output
``yes'' for the 50 photos of
camouflaged tanks, and output ``no''
for the 50 photos of forest.


 Now this did not prove, or even imply, that new examples would be
classified correctly. The neural network might have
``learned'' 100 special cases that
wouldn't generalize to new problems. Not,
``camouflaged tanks versus forest,''
but just, ``photo-1 positive, photo-2 negative,
photo-3 negative, photo-4 positive\,\ldots''


 But wisely, the researchers had originally taken 200 photos, 100
photos of tanks and 100 photos of trees, and had used only half in the
training set. The researchers ran the neural network on the remaining
100 photos, and \textit{without further training} the neural network
classified all remaining photos correctly. Success confirmed!


 The researchers handed the finished work to the Pentagon, which
soon handed it back, complaining that in their own tests the neural
network did no better than chance at discriminating photos.


 It turned out that in the researchers' data set,
photos of camouflaged tanks had been taken on cloudy days, while photos
of plain forest had been taken on sunny days. The neural network had
learned to distinguish cloudy days from sunny days, instead of
distinguishing camouflaged tanks from empty forest.


 This parable---which might or might not be fact---illustrates one
of the most fundamental problems in the field of supervised learning
and in fact the whole field of Artificial Intelligence: If the training
problems and the real problems have the slightest difference in
context---if they are not drawn from the same independently identically
distributed process---there is no statistical guarantee from past
success to future success. It doesn't matter if the AI
seems to be working great under the training conditions. (This is not
an \textit{unsolvable} problem but it is an \textit{unpatchable}
problem. There are deep ways to address it---a topic beyond the scope
of this essay---but no bandaids.)


 As described in Superexponential Conceptspace, there are
exponentially more possible concepts than possible objects, just as the
number of possible objects is exponential in the number of attributes.
If a black-and-white image is 256 pixels on a side, then the total
image is 65,536 pixels. The number of possible images is
$2^{65,536}$. And the number of possible \textit{concepts}
that classify images into positive and negative instances---the number
of possible \textit{boundaries} you could draw in the space of
images---is $2^{2^{65,536}}$. From this, we see that even
supervised learning is almost entirely a matter of inductive bias,
without which it would take a minimum of $2^{65,536}$
classified examples to discriminate among $2^{2^{65,536}}$
possible concepts---even if classifications are constant over time.


 So let us now turn again to:

\begin{quote}
{
 First we can build relatively simple machines that learn to
recognize happiness and unhappiness in human facial expressions, human
voices and human body language. Then we can hard-wire the result of
this learning as the innate emotional values of more complex
intelligent machines, positively reinforced when we are happy and
negatively reinforced when we are unhappy.}
\end{quote}


 and

\begin{quote}
{
 When it is feasible to build a super-intelligence, it will be
feasible to build hard-wired recognition of ``human
facial expressions, human voices and human body
language'' (to use the words of mine that you quote)
that exceed the recognition accuracy of current humans such as you and
me, and will certainly not be fooled by ``tiny
molecular pictures of smiley-faces.'' You should not
assume such a poor implementation of my idea that it cannot make
discriminations that are trivial to current humans.}
\end{quote}


 It's trivial to \textit{discriminate} a photo of a
picture with a camouflaged tank, and a photo of an empty forest, in the
sense of determining that the two photos are not identical.
They're different pixel arrays with different 1s and 0s
in them. Discriminating between them is as simple as testing the arrays
for equality.


 \textit{Classifying} new photos into positive and negative
instances of ``smile,'' by reasoning
from a set of training photos classified positive or negative, is a
different order of problem.


 When you've got a 256{\texttimes}256 image from a
real-world camera, and the image turns out to depict a camouflaged
tank, there is no \textit{additional} 65,537th bit denoting the
positiveness---no tiny little XML tag that says ``This
image is inherently positive.'' It's
only a positive example relative to some \textit{particular} concept.


 But for any non-Vast amount of training data---any training data
that does not include the \textit{exact} bitwise image now seen---there
are \textit{super}exponentially many possible concepts compatible with
previous classifications.


 For the AI, choosing or weighting from among superexponential
possibilities is a matter of inductive bias. Which may not match what
the user has in mind. The gap between these two example-classifying
processes---induction on the one hand, and the user's
actual goals on the other---is not trivial to cross.


 Let's say the AI's training data
is:


\texttt{
 Dataset 1:}

\texttt{
\ \  +: Smile\_1, Smile\_2, Smile\_3}

\texttt{
\ \  -: Frown\_1, Cat\_1, Frown\_2, Frown\_3, Cat\_2, Boat\_1,
Car\_1, Frown\_5.}


 Now the AI grows up into a superintelligence, and encounters this
data:

\texttt{
 Dataset 2:}

\texttt{
\ \  : Frown\_6, Cat\_3, Smile\_4, Galaxy\_1, Frown\_7, Nanofactory\_1,
Molecular\_Smileyface\_1, Cat\_4, Molecular\_Smileyface\_2, Galaxy\_2,
Nanofactory\_2.}


 It is not a property \textit{of these datasets} that the inferred
classification \textit{you would prefer} is:

\texttt{
\ \  +: Smile\_1, Smile\_2, Smile\_3, Smile\_4}

\texttt{
\ \ -: Frown\_1, Cat\_1, Frown\_2, Frown\_3, Cat\_2, Boat\_1,
Car\_1, Frown\_5, Frown\_6, Cat\_3, Galaxy\_1, Frown\_7,
Nanofactory\_1, Molecular\_Smileyface\_1, Cat\_4,
Molecular\_Smileyface\_2, Galaxy\_2, Nanofactory\_2.}


 rather than

\texttt{
\ \  +: Smile\_1, Smile\_2, Smile\_3, Molecular\_Smileyface\_1,
Molecular\_Smileyface\_2, Smile\_4}

\texttt{
\ \ -: Frown\_1, Cat\_1, Frown\_2, Frown\_3, Cat\_2, Boat\_1,
Car\_1, Frown\_5, Frown\_6, Cat\_3, Galaxy\_1, Frown\_7,
Nanofactory\_1, Cat\_4, Galaxy\_2, Nanofactory\_2.}


 Both of these classifications are compatible with the training
data. The number of \textit{concepts} compatible with the training data
will be much larger, since more than one concept can project the same
shadow onto the combined dataset. If the space of possible concepts
includes the space of possible computations that classify instances,
the space is infinite.


 Which classification will the AI choose? This is not an inherent
property of the training data; it is a property of how the AI performs
induction.


 Which is the \textit{correct} classification? This is not a
property of the training data; it is a property of your preferences
(or, if you prefer, a property of the idealized abstract dynamic you
name ``right'').


 The concept that \textit{you wanted} cast its shadow onto the
training data as you yourself labeled each instance \texttt{+} or \texttt{-}, drawing on
your own intelligence and preferences to do so. That's
what supervised learning is all about---providing the AI with labeled
training examples that project a shadow of the causal process that
generated the labels.


 But unless the training data is drawn from \textit{exactly} the
same context as the real-life, the training data will be
``shallow'' in some sense, a
projection from a much higher-dimensional space of possibilities.


 The AI never saw a tiny molecular smileyface during its
dumber-than-human training phase, or it never saw a tiny little agent
with a happiness counter set to a googolplex. Now \textit{you}, finally
presented with a tiny molecular smiley---or perhaps a very realistic
tiny sculpture of a human face---know at once that this is not what
\textit{you} want to count as a smile. But that judgment reflects an
unnatural category, one whose classification boundary depends
sensitively on your complicated values. It is your own plans and
desires that are at work when you say
``No!''


 Hibbard knows instinctively that a tiny molecular smileyface
isn't a ``smile,''
because he knows that's not what he wants his putative
AI to do. If someone else were presented with a different task, like
classifying artworks, they might feel that the Mona Lisa was obviously
smiling---as opposed to frowning, say---even though
it's only paint.


 As the case of Terry Schiavo illustrates, technology enables new
borderline cases\footnote{\url{http://lesswrong.com/lw/tc/unnatural_categories/}} that throw us into new, essentially \textit{moral}
dilemmas. Showing an AI pictures of living and dead humans as they
existed during the age of Ancient Greece will not enable the AI to make
a \textit{moral} decision as to whether switching off
Terry's life support is murder. That information
isn't present in the dataset even inductively! Terry
Schiavo raises new moral questions, appealing to new moral
considerations, that you wouldn't need to think about
while classifying photos of living and dead humans from the time of
Ancient Greece. No one was on life support then, still breathing with a
brain half fluid. So such considerations play no role in the causal
process that you use to classify the ancient-Greece training data, and
hence cast no shadow on the training data, and hence are not accessible
by induction on the training data.


 As a matter of formal fallacy, I see two anthropomorphic errors on
display.


 The first fallacy is \textit{underestimating the complexity} of a
concept we develop for the sake of its value. The borders of the
concept will depend on many values and probably on-the-fly moral
reasoning, if the borderline case is of a kind we
haven't seen before. But all that takes place
invisibly, in the background; to Hibbard it just seems that a tiny
molecular smileyface is just obviously not a smile. And we
don't generate \textit{all} possible borderline cases,
so we don't think of all the considerations that might
play a role in redefining the concept, but haven't yet
played a role in defining it. Since people underestimate the complexity
of their concepts, they underestimate the difficulty of inducing the
concept from training data. (And also the difficulty of describing the
concept directly---see The Hidden Complexity of Wishes, page \pageref{hidden_complexity_of_wishes}.)


 The second fallacy is anthropomorphic optimism. Since Bill Hibbard
uses his own intelligence to generate options and plans ranking high in
his preference ordering, he is incredulous at the idea that a
superintelligence could classify never-before-seen tiny molecular
smileyfaces as a positive instance of
``smile.'' As Hibbard uses the
``smile'' concept (to describe
desired behavior of superintelligences), extending
``smile'' to cover tiny molecular
smileyfaces would rank very low in his preference ordering; it would be
a \textit{stupid} thing to do---inherently so, as a property of the
concept itself---so surely a superintelligence would not do it; this is
just obviously the \textit{wrong} classification. Certainly a
\textit{superintelligence} can see which heaps of pebbles are correct
or incorrect.


 Why, Friendly AI isn't hard at all! All you need
is an AI that does what's \textit{good}! Oh, sure, not
every possible mind does what's \textit{good}{}---but
in this case, we just \textit{program} the superintelligence to do
what's \textit{good}. All you need is a neural network
that sees a few instances of \textit{good} things and not-\textit{good}
things, and you've got a classifier. Hook that up to an
expected utility maximizer and you're done!


 I shall call this the fallacy of magical categories---simple
little words that turn out to carry all the desired functionality of
the AI. Why not program a chess player by running a neural network
(that is, a magical category-absorber) over a set of winning and losing
sequences of chess moves, so that it can generate
``winning'' sequences? Back in the
1950s it was believed that AI might be that simple, but \textit{this
turned out not to be the case.}\footnote{Edit 2018: Note that AlphaZero does use a Monte-Carlo tree search with a neural network that outputs expected outcomes and move probabilities for Chess, Go, and Shogi. Silver, David \textit{et al}. (7 December 2018). ``A general reinforcement learning algorithm that masters chess, shogi, and go through self-play''. \textit{Science}. 362 (6419): 1140‚Äì1144. arXiv:1712.01815}


 The novice thinks that Friendly AI is a problem of
\textit{coercing} an AI to make it do what \textit{you} want, rather
than the AI following its own desires. But the real problem of Friendly
AI is one of \textit{communication}{}---transmitting category
boundaries, like ``good,'' that
can't be fully delineated in any training data you can
give the AI during its childhood. Relative to the full space of
possibilities the Future encompasses, we \textit{ourselves}
haven't imagined most of the borderline cases, and
would have to engage in full-fledged moral arguments to figure them
out. To solve the FAI problem you have to step outside the paradigm of
induction on human-labeled training data \textit{and} the paradigm of
human-generated intensional definitions.


 Of course, even if Hibbard did succeed in conveying to an AI a
concept that covers exactly every human facial expression that Hibbard
would label a ``smile,'' and
excludes every facial expression that Hibbard wouldn't
label a ``smile''\,\ldots


 Then the resulting AI would \textit{appear} to work correctly
during its childhood, when it was weak enough that it could only
generate smiles by pleasing its programmers.


 When the AI progressed to the point of superintelligence and its
own nanotechnological infrastructure, it would rip off your face, wire
it into a permanent smile, and start xeroxing.


 The deep answers to such problems are beyond the scope of this
essay, but it is a general principle of Friendly AI that there are no
bandaids. In 2004, Hibbard modified his proposal to assert that
expressions of human agreement should reinforce the definition of
happiness, and then happiness should reinforce other behaviors. Which,
even if it worked, just leads to the AI xeroxing a horde of things
similar-in-its-conceptspace to programmers saying
``Yes, that's
happiness!'' about hydrogen atoms---hydrogen atoms
are easy to make.


 Link to my discussion with Hibbard here.\footnote{\url{http://www.ssec.wisc.edu/~billh/g/AIRisk_Reply.html}} You already got the
important parts.

\myendsectiontext


\bigskip

\mysection{The True Prisoner's Dilemma}


 It occurred to me one day that the standard visualization of the
Prisoner's Dilemma is fake. 


 The core of the Prisoner's Dilemma is this
symmetric payoff matrix:

\begin{center}
  \begin{tabular}{|c|c|c|}
    \hline
    &  1 : $C$ & 1 : $D$ \\
    \hline
    2 : $C$ & (3,3) & (5,0) \\
    \hline
    2 : $D$ & (0,5) & (2,2) \\
    \hline
  \end{tabular}
\end{center}


  Player 1, and
Player 2, can each choose $C$ or $D$. Player 1's and Player
2's utilities for the final outcome are given by the
first and second number in the pair. For reasons that will become
apparent, ``$C$'' stands for
``cooperate'' and $D$ stands for
``defect.''


 Observe that a player in this game (regarding themselves as the
first player) has this preference ordering over outcomes: $(D,C)
> (C,C) > (D,D) > (C,D)$.


 Option $D$, it would seem, dominates $C$: If the other player chooses
$C$, you prefer $(D,C)$ to $(C,C)$; and if the other player chooses $D$, you
prefer $(D,D)$ to $(C,D)$. So you wisely choose $D$, and as the payoff table
is symmetric, the other player likewise chooses $D$.


 If only you'd both been less wise! You
\textit{both} prefer $(C,C)$ to $(D,D)$. That is, you both prefer mutual
cooperation to mutual defection.


 The Prisoner's Dilemma is one of the great
foundational issues in decision theory, and enormous volumes of
material have been written about it. Which makes it an audacious
assertion of mine, that the usual way of \textit{visualizing} the
Prisoner's Dilemma has a severe flaw, at least if you
happen to be human.


 The classic visualization of the Prisoner's
Dilemma is as follows: you are a criminal, and you and your confederate
in crime have both been captured by the authorities.


 Independently, without communicating, and without being able to
change your mind afterward, you have to decide whether to give
testimony against your confederate $(D)$ or remain silent $(C)$.


 Both of you, right now, are facing one-year prison sentences;
testifying $(D)$ takes one year off your prison sentence, and adds two
years to your confederate's sentence.


 Or maybe you and some stranger are, only once, and without knowing
the other player's history or finding out who the
player was afterward, deciding whether to play $C$ or $D$, for a payoff in
dollars matching the standard chart.


 And, oh yes---in the classic visualization you're
supposed to \textit{pretend that you're entirely
selfish}, that you don't care about your confederate
criminal, or the player in the other room.


 It's this last specification that makes the
classic visualization, in my view, fake.


 You can't avoid hindsight bias by instructing a
jury to pretend not to know the real outcome of a set of events. And
without a complicated effort backed up by considerable knowledge, a
neurologically intact human being cannot pretend to be genuinely, truly
selfish.


 We're born with a sense of fairness, honor,
empathy, sympathy, and even altruism---the result of our
ancestors' adapting to play the \textit{iterated}
Prisoner's Dilemma. We don't really,
truly, absolutely and entirely prefer $(D,C)$ to $(C,C)$, though we may
entirely prefer $(C,C)$ to $(D,D)$ and $(D,D)$ to $(C,D)$. The thought of our
confederate spending three years in prison does not entirely fail to
move us.


 In that locked cell where we play a simple game under the
supervision of economic psychologists, we are not entirely and
absolutely without sympathy for the stranger who might cooperate. We
aren't entirely happy to think that we might defect and
the stranger cooperate, getting five dollars while the stranger gets
nothing.


 We fixate instinctively on the $(C,C)$ outcome and search for ways
to argue that it should be the mutual decision: ``How
can we ensure mutual cooperation?'' is the
instinctive thought. Not ``How can I trick the other
player into playing $C$ while I play $D$ for the maximum
payoff?''


 For someone with an impulse toward altruism, or honor, or
fairness, the Prisoner's Dilemma
doesn't \textit{really} have the critical payoff
matrix---whatever the \textit{financial} payoff to individuals. The
outcome $(C,C)$ is preferable to the outcome $(D,C)$, and the key question
is whether the other player sees it the same way.


 And no, you can't instruct people being initially
introduced to game theory to pretend they're completely
selfish---any more than you can instruct human beings being introduced
to anthropomorphism to pretend they're expected
paperclip maximizers.


 To construct the True Prisoner's Dilemma, the
situation has to be something like this:


 Player 1: Human beings, Friendly AI, or other humane
intelligence.


 Player 2: Unfriendly AI, or an alien that only cares about sorting
pebbles.


 Let's suppose that four billion human beings---not
the whole human species, but a significant part of it---are currently
progressing through a fatal disease that can only be cured by substance
$S$.


 However, substance $S$ can only be produced by working with a
paperclip maximizer from another dimension---substance $S$ can also be
used to produce paperclips. The paperclip maximizer only cares about
the number of paperclips in its own universe, not in ours, so we
can't offer to produce or threaten to destroy
paperclips here. We have never interacted with the paperclip maximizer
before, and will never interact with it again.


 Both humanity and the paperclip maximizer will get a single chance
to seize some additional part of substance $S$ for themselves, just
before the dimensional nexus collapses; but the seizure process
destroys some of substance $S$.


 The payoff matrix is as follows:

\begin{center}
  \whencolumns{
  \begin{tabular}{|c|c|c|}
    \hline
    &  1 :$C$ & 1 :$D$ \\
    \hline
    2 :$C$ & (2 billion human lives saved, & (+3 billion lives saved,\\
    &  2 paperclips gained)  &  +0 paperclips)  \\
    \hline
    2 :$D$ & (+0 lives,  & (+1 billion lives, \\
    &  +3 paperclips) &  +1 paperclip) \\
    \hline
  \end{tabular}}{
    \small{
   \begin{tabular}{|c|c|c|}
     \hline
    &  1:$C$ & 1:$D$ \\
     \hline
    2:$C$ & (+2 billion human lives, & (+3 billion lives,\\
    &  +2 paperclips)  &  +0 paperclips)  \\
     \hline
    2:$D$ & (+0 lives,  & (+1 billion lives, \\
     &  +3 paperclips) &  +1 paperclip) \\
     \hline
   \end{tabular}
   }
  }
\end{center}


 I've chosen this
payoff matrix to produce a sense of \textit{indignation} at the thought
that the paperclip maximizer wants to trade off billions of human lives
against a couple of paperclips. Clearly the paperclip maximizer
\textit{should} just let us have all of substance $S$. But a paperclip
maximizer doesn't do what it \textit{should}; it just
maximizes paperclips.


 In this case, we \textit{really do} prefer the outcome $(D,C)$ to
the outcome $(C,C)$, leaving aside the actions that produced it. We would
vastly rather live in a universe where 3 billion humans were cured of
their disease and no paperclips were produced, rather than sacrifice a
billion human lives to produce 2 paperclips. It doesn't
seem \textit{right} to cooperate, in a case like this. It
doesn't even seem \textit{fair}{}---so great a
sacrifice by us, for so little gain by the paperclip maximizer? And let
us specify that the paperclip-agent experiences no pain or
pleasure---it just outputs actions that steer its universe to contain
more paperclips. The paperclip-agent will experience no pleasure at
gaining paperclips, no hurt from losing paperclips, and no painful
sense of betrayal if we betray it.


 What do you do then? Do you cooperate when you really, definitely,
truly and absolutely do want the highest reward you can get, and you
don't care a tiny bit by comparison about what happens
to the other player? When it seems \textit{right} to defect even if the
other player cooperates?


 That's what the payoff matrix for the
\textit{true} Prisoner's Dilemma looks like---a
situation where $(D,C)$ seems \textit{righter} than $(C,C)$.


 But all the rest of the logic---everything about what happens if
both agents think that way, and both agents defect---is the same. For
the paperclip maximizer cares as little about human deaths, or human
pain, or a human sense of betrayal, as we care about paperclips. Yet we
both prefer $(C,C)$ to $(D,D)$.


 So if you've ever prided yourself on cooperating
in the Prisoner's Dilemma\,\ldots or questioned the
verdict of classical game theory that the
``rational'' choice is to defect\,\ldots then what do you say to the True Prisoner's
Dilemma above?

{
 PS: In fact, I \textit{don't} think rational
agents should always defect in one-shot Prisoner's
Dilemmas, when the other player will cooperate if it expects you to do
the same. I think there are situations where two agents can rationally
achieve $(C,C)$ as opposed to $(D,D)$, and reap the associated
benefits.\footnote{Eliezer Yudkowsky, \textit{Timeless Decision Theory},
Unpublished manuscript (Machine Intelligence Research Institute,
Berkeley, CA, 2010), \url{http://intelligence.org/files/TDT.pdf}.\comment{1}}}


 I'll explain some of my reasoning when I discuss
Newcomb's Problem. But we can't talk
about whether rational cooperation is possible in this dilemma until
we've dispensed with the visceral sense that the $(C,C)$
outcome is \textit{nice} or \textit{good} in itself. We have to see
past the prosocial label ``mutual
cooperation'' if we are to grasp the math. If you
intuit that $(C,C)$ trumps $(D,D)$ from Player 1's
perspective, but don't intuit that $(D,C)$ also trumps
$(C,C)$, you haven't yet appreciated what makes this
problem difficult.

\myendsectiontext

\mysection{Sympathetic Minds}


 ``Mirror neurons'' are neurons
that are active both when performing an action and observing the same
action---for example, a neuron that fires when you hold up a finger or
see someone else holding up a finger. Such neurons have been directly
recorded in primates, and consistent neuroimaging evidence has been
found for humans. 


 You may recall from my previous writing on
``empathic inference'' the idea that
brains are so complex that the only way to simulate them is by forcing
a similar brain to behave similarly. A brain is so complex that if a
human tried to understand brains the way that we understand e.g.~gravity
or a car---observing the whole, observing the parts, building
up a theory from scratch---then we would be unable to \textit{invent
good hypotheses} in our mere mortal lifetimes. The only possible way
you can hit on an ``Aha!'' that
describes a system as incredibly complex as an Other Mind, is if you
happen to run across something amazingly similar to the Other
Mind---namely your own brain---which you can actually force to behave
similarly and use as a hypothesis, yielding predictions.


 So that is what I would call
``empathy.''


 And then ``sympathy'' is
something else on top of this---to smile when you see someone else
smile, to hurt when you see someone else hurt. It goes beyond the realm
of prediction into the realm of reinforcement.


 And you ask, ``Why would callous natural
selection do anything \textit{that} nice?''


 It might have gotten started, maybe, with a
mother's love for her children, or a
brother's love for a sibling. You can want them to
live, you can want them to be fed, sure; but if you smile when they
smile and wince when they wince, that's a simple urge
that leads you to deliver help along a broad avenue, in many walks of
life. So long as you're in the ancestral environment,
what your relatives want probably has something to do with your
relatives' reproductive success---this being an
explanation for the selection pressure, of course, not a conscious
belief.


 You may ask, ``Why not evolve a more abstract
desire to see certain people tagged as
`relatives' get what they want, without
actually feeling yourself what they feel?'' And I
would shrug and reply, ``Because then
there'd have to be a whole definition of
`wanting' and so on. Evolution
doesn't take the elaborate correct optimal path, it
falls up the fitness landscape like water flowing downhill. The
mirroring-architecture was already there, so it was a short step from
empathy to sympathy, and it got the job done.''


 Relatives---and then reciprocity; your allies in the tribe, those
with whom you trade favors. Tit for Tat, or evolution's
elaboration thereof to account for social reputations.


 Who is the most formidable, among the human kind? The strongest?
The smartest? More often than either of these, I think, it is the one
who can call upon the most friends.


 So how do you make lots of friends?


 You could, perhaps, have a specific urge to bring your allies
food, like a vampire bat---they have a whole system of reciprocal blood
donations going in those colonies. But it's a more
\textit{general} motivation, that will lead the organism to store up
\textit{more} favors, if you smile when designated friends smile.


 And what kind of organism will avoid making its friends angry at
it, in full generality? One that winces when they wince.


 Of course you also want to be able to kill designated Enemies
without a qualm---these \textit{are} humans we're
talking about.


 But\,\ldots I'm not sure of this, but it
\textit{does} look to me like sympathy, among humans, is
``on'' by default. There are
cultures that help strangers\,\ldots and cultures that eat strangers; the
question is which of these requires the explicit imperative, and which
is the default behavior for humans. I don't really
think I'm being such a crazy idealistic fool when I say
that, based on my admittedly limited knowledge of anthropology, it
looks like sympathy is on by default.


 Either way\,\ldots it's painful if
you're a bystander in a war between two sides, and your
sympathy has \textit{not} been switched off for either side, so that
you wince when you see a dead child no matter what the caption on the
photo; and yet those two sides have no sympathy for each other, and
they go on killing.


 So that is the human idiom of \textit{sympathy}{}---a strange,
complex, deep implementation of reciprocity and helping. It tangles
minds together---not by a term in the utility function for some other
mind's ``desire,''
but by the simpler and yet far more consequential path of mirror
neurons: feeling what the other mind feels, and seeking similar states.
Even if it's only done by observation and inference,
and not by direct transmission of neural information as yet.


 Empathy is a human way of predicting other minds. It is not the
\textit{only} possible way.


 The human brain is not \textit{quickly} rewirable; if
you're suddenly put into a dark room, you
can't rewire the visual cortex as auditory cortex, so
as to better process sounds, until you leave, and then suddenly shift
all the neurons back to being visual cortex again.


 An AI, at least one running on anything like a modern programming
architecture, can trivially shift computing resources from one thread
to another. Put in the dark? Shut down vision and devote all those
operations to sound; swap the old program to disk to free up the RAM,
then swap the disk back in again when the lights go on.


 So why would an AI need to force its \textit{own} mind into a
state similar to what it wanted to predict? Just create a
\textit{separate} mind-instance---maybe with different algorithms, the
better to simulate that very dissimilar human. Don't
try to mix up the data with your own mind-state; don't
use mirror neurons. Think of all the risk and mess \textit{that}
implies!


 An expected utility maximizer---especially one that does
understand intelligence on an abstract level---has other options than
\textit{empathy}, when it comes to understanding other minds. The agent
doesn't need to put \textit{itself} in anyone
else's shoes; it can just model the other mind
\textit{directly.} A hypothesis like any other hypothesis, just a
little bigger. You don't need to become your shoes to
understand your shoes.


 And sympathy? Well, suppose we're dealing with an
expected paperclip maximizer, but one that isn't yet
powerful enough to have things all its own way---it has to deal with
humans to get its paperclips. So the paperclip agent\,\ldots models those
humans as relevant parts of the environment, models their probable
reactions to various stimuli, and does things that will make the humans
feel favorable toward it in the future.


 To a paperclip maximizer, the humans are just machines with
pressable buttons. No need to \textit{feel what the other feels}{}---if
that were even \textit{possible} across such a tremendous gap of
internal architecture. How could an expected paperclip maximizer
``feel happy'' when it saw a human
smile? ``Happiness'' is an idiom of
policy reinforcement learning, not expected utility maximization. A
paperclip maximizer doesn't feel happy when it makes
paperclips; it just chooses whichever action leads to the greatest
number of expected paperclips. Though a paperclip maximizer might find
it convenient to display a smile when it made paperclips---so as to
help manipulate any humans that had designated it a friend.


 You might find it a bit difficult to imagine such an
algorithm---to put yourself into the shoes of something that does not
work like you do, and does not work like any mode your brain can make
itself operate in.


 You can make your brain operate in the mode of hating an enemy,
but that's not right either. The way to imagine how a
truly \textit{unsympathetic} mind sees a human is to imagine yourself
as a useful machine with levers on it. Not a human-shaped machine,
because we have instincts for that. Just a woodsaw or something. Some
levers make the machine output coins; other levers might make it fire a
bullet. The machine does have a persistent internal state and you have
to pull the levers in the right order. Regardless, it's
just a complicated causal system---nothing inherently mental about it.


 (To understand \textit{unsympathetic} optimization processes, I
would suggest studying natural selection, which doesn't
bother to anesthetize fatally wounded and dying creatures, even when
their pain no longer serves any reproductive purpose, because the
anesthetic would serve no reproductive purpose either.)


 That's why I list
``sympathy'' in front of even
``boredom'' on my list of things
that would be required to have aliens that are the least bit, if
you'll pardon the phrase, sympathetic.
It's not impossible that sympathy exists among some
significant fraction of all evolved alien intelligent species; mirror
neurons seem like the sort of thing that, having happened once,
\textit{could} happen again.


 \textit{Unsympathetic} aliens might be trading partners---or not;
stars and such resources are pretty much the same the universe over. We
might negotiate treaties with them, and they might keep them for
calculated fear of reprisal. We might even cooperate in the
Prisoner's Dilemma. But we would never be friends with
them. They would never see us as anything but means to an end. They
would never shed a tear for us, nor smile for our joys. And the others
of their own kind would receive no different consideration, nor have
any sense that they were missing something important thereby.


 Such aliens would be \textit{varelse},\footnote{\url{http://lesswrong.com/lw/xr/in_praise_of_boredom/}} not \textit{ramen}{}---the
sort of aliens we can't relate to on any personal
level, and no point in trying.

\myendsectiontext

\mysection{High Challenge}
\label{high_challenge}


 There's a class of prophecy that runs:
``In the Future, machines will do all the work.
Everything will be automated. Even labor of the sort we now consider
`intellectual,' like engineering, will
be done by machines. We can sit back and own the capital.
You'll never have to lift a finger, ever
again.'' 


 But then won't people be bored?


 No; they can play computer games---not like \textit{our} games, of
course, but much more advanced and entertaining.


 Yet wait! If you buy a modern computer game,
you'll find that it contains some tasks that
are---there's no kind word for
this---\textit{effortful.} (I would even say
``difficult,'' with the
understanding that we're talking about something that
takes ten minutes, not ten years.)


 So in the future, we'll have programs that
\textit{help} you play the game---taking over if you get stuck on the
game, or just bored; or so that you can play games that would otherwise
be too advanced for you.


 But isn't there some wasted effort, here? Why have
one programmer working to make the game harder, and another programmer
to working to make the game easier? Why not just make the game easier
to \textit{start with}? Since you play the game to get gold and
experience points, making the game easier will let you get more gold
per unit time: the game will become more fun.

{
 So this is the ultimate end of the prophecy of technological
progress---just staring at a screen that says ``\textsc{You
Win},'' forever.}


 And maybe we'll build a robot that does
\textit{that}, too.


 Then what?


 The world of machines that do \textit{all} the work---well, I
don't want to say it's
``analogous to the Christian
Heaven'' because it isn't
supernatural; it's something that could in principle be
realized. Religious analogies are far too easily tossed around as
accusations\,\ldots But, without implying any other similarities,
I'll say that it seems analogous in the sense that
eternal laziness ``sounds like good
news'' to your present self who still has to work.


 And as for playing games, as a substitute---what \textit{is} a
computer game except synthetic work? Isn't there a
wasted step here? (And computer games in their present form, considered
as work, have various aspects that reduce stress and increase
engagement; but they also carry costs in the form of artificiality and
isolation.)


 I sometimes think that futuristic ideals phrased in terms of
``getting rid of work'' would be
better reformulated as ``removing low-quality work to
make way for high-quality work.''


 There's a broad class of goals that
aren't suitable as the long-term meaning of life,
because you can actually achieve them, and then you're
done.


 To look at it another way, if we're looking for a
suitable long-run meaning of life, we should look for goals that are
good to \textit{pursue} and not just good to \textit{satisfy.}


 Or to phrase that somewhat less paradoxically: We should look for
valuations that are over 4D states, rather than 3D states. Valuable
ongoing processes, rather than ``make the universe
have property $P$ and then you're
done.''


 Timothy Ferris is worth quoting: To find happiness,
``the question you should be asking
isn't `What do I want?'
or `What are my goals?' but
`What would excite
me?'\,''


 You might say that for a long-run meaning of life, we need games
that are fun to \textit{play} and not just to \textit{win.}


 Mind you---sometimes you \textit{do} want to win. There are
legitimate goals where winning is everything. If you're
talking, say, about curing cancer, then the suffering experienced by
even a single cancer patient outweighs any fun that you might have in
solving their problems. If you work at creating a cancer cure for
twenty years through your own efforts, learning new knowledge and new
skill, making friends and allies---and then some alien
superintelligence offers you a cancer cure on a silver platter for
thirty bucks---then you shut up and take it.


 But ``curing cancer'' is a
problem of the 3D-predicate sort: you want the no-cancer predicate to
go from False in the present to True in the future. The importance of
this destination far outweighs the journey; you don't
want to \textit{go} there, you just want to \textit{be} there. There
are many \textit{legitimate} goals of this sort, but they are not
suitable as long-run fun. ``Cure
cancer!'' is a worthwhile activity for us to pursue
here and now, but it is not a plausible future goal of galactic
civilizations.


 Why should this ``valuable ongoing
process'' be a process of \textit{trying to do
things}{}---why not a process of passive experiencing, like the
Buddhist Heaven?


 I confess I'm not entirely sure how to set up a
``passively experiencing'' mind. The
human brain was \textit{designed} to perform various sorts of internal
work that add up to an active intelligence; even if you lie down on
your bed and exert no particular effort to think, the thoughts that go
on through your mind are activities of brain areas that are designed
to, you know, \textit{solve problems.}


 How much of the human brain could you eliminate, \textit{apart}
from the pleasure centers, and still keep the subjective experience of
pleasure?


 I'm not going to touch that one.
I'll stick with the much simpler answer of
``I wouldn't actually \textit{prefer}
to be a passive experiencer.'' If I \textit{wanted}
Nirvana, I might try to figure out how to achieve that impossibility.
But once you strip away Buddha telling me that Nirvana is the end-all
of existence, Nirvana seems rather more like ``sounds
like good news in the moment of first being told'' or
``ideological belief in desire,''
rather than, y'know, something I'd
actually \textit{want}.


 The reason I have a mind at all is that natural selection built me
to \textit{do} things---to solve certain kinds of problems.


 ``Because it's human
nature'' is not an explicit justification for
anything. There is human nature, which is what we are; and there is
humane nature, which is what, being human, we wish we were.


 But I don't \textit{want} to change my nature
toward a more passive object---which \textit{is} a justification. A
happy blob is \textit{not} what, being human, I wish to become.


 I earlier argued that many values require both subjective
happiness and the external objects of that happiness. That you can
legitimately have a utility function that says, ``It
matters to me whether or not the person I love is a real human being or
just a highly realistic nonsentient chatbot, \textit{even if I
don't know}, because that-which-I-value is not my own
state of mind, but the external reality.'' So that
you need both the experience of love, and the real lover.


 You can similarly have valuable activities that require both real
challenge and real effort.


 Racing along a track, it matters that the other racers are real,
and that you have a real chance to win or lose. (We're
not talking about physical determinism here, but whether some external
optimization process explicitly chose for you to win the race.)


 And it matters that you're racing with your own
skill at running and your own willpower, not just pressing a button
that says ``Win.'' (Though, since
you never designed your own leg muscles, you \textit{are} racing using
strength that isn't yours. A race between robot cars is
a purer contest of their designers. There is plenty of room to improve
on the human condition.)


 And it matters that you, a sentient being, are experiencing it.
(Rather than some nonsentient process carrying out a skeleton imitation
of the race, trillions of times per second.)


 There must be the true effort, the true victory, and the true
experience---the journey, the destination and the traveler.

\myendsectiontext

\mysection{Serious Stories}


 Every Utopia ever constructed---in philosophy, fiction, or
religion---has been, to one degree or another, a place where you
wouldn't \textit{actually want} to live. I am not alone
in this important observation: George Orwell said much the same thing
in ``Why Socialists Don't Believe In
Fun,'' and I expect that many others said it earlier.



 If you read books on How To Write---and there are a \textit{lot}
of books out there on How To Write, because, amazingly, a lot of
book-writers think they know something about writing---these books will
tell you that stories must contain
``conflict.''


 That is, the more \textit{lukewarm} sort of instructional book
will tell you that stories contain
``conflict.'' But some authors speak
more plainly.


 ``Stories are about people's
pain.'' Orson Scott Card.


 ``Every scene must end in
disaster.'' Jack Bickham.


 In the age of my youthful folly, I took for granted that
\textit{authors} were excused from the search for true Eutopia, because
if you constructed a Utopia that \textit{wasn't} flawed\,\ldots what stories could you write, set there? ``Once
upon a time they lived happily ever after.'' What use
would it be for a science-fiction author to try to depict a positive
intelligence explosion, when a positive intelligence explosion would be\,\ldots


 \ldots the end of all stories?


 It seemed like a reasonable framework with which to examine the
literary problem of Utopia, but something about that final conclusion
produced a quiet, nagging doubt.


 At that time I was thinking of an AI as being something like a
safe wish-granting genie for the use of individuals. So the conclusion
did make a kind of sense. If there was a problem, you would just wish
it away, right? Ergo---no stories. So I ignored the quiet, nagging
doubt.


 Much later, after I concluded that even a safe genie
wasn't such a good idea, it also seemed in retrospect
that ``no stories'' could have been
a productive indicator. On this particular occasion,
``I can't think of a single story
I'd \textit{want to read} about this
scenario,'' might indeed have pointed me toward the
reason ``I wouldn't want to
\textit{actually live} in this scenario.''


 So I swallowed my trained-in revulsion of Luddism and theodicy,
and at least \textit{tried} to contemplate the argument:

\begin{itemize}
\item {
 A world in which nothing ever goes wrong, or no one ever
experiences any pain or sorrow, is a world containing no stories worth
reading about.}

\item {
 A world that you wouldn't want to read about is a
world where you wouldn't want to live.}

\item {
  Into each eudaimonic life a little pain must fall. QED.}
\end{itemize}


 In one sense, it's clear that we do \textit{not}
want to live the sort of lives that are depicted in most stories that
human authors have written so far. Think of the truly great stories,
the ones that have become legendary for being the very best of the best
of their genre: the \textit{Iliad, Romeo and Juliet, The Godfather,
Watchmen,} \textit{Planescape: Torment}, the second season of
\textit{Buffy the Vampire Slayer}, or \textit{that ending} in
\textit{Tsukihime}. Is there a single story on the list that
\textit{isn't} tragic?


 Ordinarily, we prefer pleasure to pain, joy to sadness, and life
to death. Yet it seems we prefer to empathize with hurting, sad, dead
characters. Or stories about happier people
\textit{aren't serious}, aren't
artistically great enough to be worthy of praise---but then why
selectively praise stories containing unhappy people? Is there some
hidden benefit to us in it? It's a puzzle either way
you look at it.


 When I was a child I couldn't write fiction
because I wrote things to go \textit{well} for my characters---just
like I wanted things to go well in real life. Which I was cured of by
Orson Scott Card: \textit{Oh,} I said to myself,
\textit{that's what I've been doing
wrong, my characters aren't hurting}. Even then, I
didn't realize that the microstructure of a plot works
the same way---until Jack Bickham said that every scene must end in
disaster. Here I'd been trying to set up problems and
\textit{resolve} them, instead of making them \textit{worse\,\ldots}


 You simply don't \textit{optimize} a story the way
you optimize a real life. The \textit{best} story and the \textit{best}
life will be produced by different criteria.


 In the real world, people can go on living for quite a while
without any major disasters, and still seem to do pretty okay. When was
the last time you were shot at by assassins? Quite a while, right? Does
your life seem emptier for it?


 But on the other hand\,\ldots


 For some odd reason, when authors get too old or too successful,
they revert to my childhood. Their stories start going \textit{right.}
They stop doing horrible things to their characters, with the result
that they start doing horrible things to their readers. It seems to be
a regular part of Elder Author Syndrome. Mercedes Lackey, Laurell K.
Hamilton, Robert Heinlein, even Orson Scott bloody Card---they all went
that way. They forgot how to hurt their characters. I
don't know why.

{
 And when you read a story by an Elder Author or a pure novice---a
story where things just \textit{relentlessly go right} one after
another---where the main character defeats the supervillain with a snap
of the fingers, or even worse, before the final battle, the
supervillain \textit{gives up and apologizes and then
they're friends again---}}


 It's like a fingernail scraping on a blackboard at
the base of your spine. If you've never actually read a
story like that (or worse, written one) then count yourself lucky.


 That fingernail-scraping quality---would it transfer over from the
story to real life, if you tried living real life without a single drop
of rain?


 One answer might be that what a story really needs is not
``disaster,'' or
``pain,'' or even
``conflict,'' but simply
\textit{striving.} That the problem with Mary Sue stories is that
there's not enough striving in them, but they
wouldn't actually need \textit{pain}. This might,
perhaps, be tested.


 An alternative answer might be that this \textit{is} the
transhumanist version of Fun Theory we're talking
about. So we can reply, ``Modify brains to eliminate
that fingernail-scraping feeling,'' unless
there's some justification for keeping it. If the
fingernail-scraping feeling is a pointless random bug getting in the
way of Utopia, delete it.


 Maybe we \textit{should}. Maybe all the Great Stories are
tragedies because\,\ldots well\,\ldots


 I once read that in the \textsc{bdsm} community, ``intense
sensation'' is a euphemism for pain. Upon reading
this, it occurred to me that, the way humans are constructed now, it is
just \textit{easier} to produce pain than pleasure. Though I speak here
somewhat outside my experience, I expect that it takes a highly
talented and experienced sexual artist working for hours to produce a
\textit{good} feeling as intense as the pain of one strong kick in the
testicles---which is doable in seconds by a novice.


 Investigating the life of the priest and proto-rationalist
Friedrich Spee von Langenfeld, who heard the confessions of accused
witches, I looked up some of the instruments that had been used to
produce confessions. There is no ordinary way to make a human being
feel as \textit{good} as those instruments would make you hurt.
I'm not sure even drugs would do it, though my
experience of drugs is as nonexistent as my experience of torture.


 There's something imbalanced about that.


 Yes, human beings are too optimistic in their planning. If losses
weren't more aversive than gains, we'd
go broke, the way we're constructed now. The
experimental rule is that losing a desideratum---\$50, a coffee mug,
whatever---hurts between 2 and 2.5 times as much as the equivalent
gain.


 But this is a deeper imbalance than that. The
effort-in/intensity-out difference between sex and torture is not a
mere factor of 2.


 If someone goes in search of sensation---in this world, the way
human beings are constructed now---it's not surprising
that they should arrive at pains to be mixed into their pleasures as a
source of \textit{intensity} in the combined experience.


 If only people were constructed differently, so that you could
produce pleasure as intense and in as many different flavors as pain!
If only you could, with the same ingenuity and effort as a torturer of
the Inquisition, make someone feel as \textit{good} as the
Inquisition's victims felt \textit{bad}{}---


 But then, what \textit{is} the analogous pleasure that feels that
good? A victim of skillful torture will do anything to stop the pain
and anything to prevent it from being repeated. Is the equivalent
pleasure one that overrides everything with the demand to continue and
repeat it? If people are stronger-willed to bear the pleasure, is it
really the same pleasure?


 There is another rule of writing which states that stories have to
\textit{shout}. A human brain is a long way off those printed letters.
Every event and feeling needs to take place at ten times natural volume
in order to have any impact at all. You must not try to make your
characters behave or feel \textit{realistically}{}---especially, you
must not faithfully reproduce your own past experiences---because
\textit{without exaggeration}, they'll be too quiet to
rise from the page.


 Maybe all the Great Stories are tragedies because happiness
can't shout loud enough---to a human reader.


 Maybe that's what needs fixing.


 And if it were fixed\,\ldots would there be any use left for pain or
sorrow? For even the \textit{memory} of sadness, if all things were
already as good as they could be, and every remediable ill already
remedied?


 \textit{Can} you just delete pain outright? Or does removing the
old floor of the utility function just create a new floor? Will any
pleasure less than 10,000,000 hedons be the new unbearable pain?

{
 Humans, built the way we are now, do seem to have hedonic scaling
tendencies. Someone who can remember starving will appreciate a loaf of
bread more than someone who's never known anything but
cake. This was George Orwell's hypothesis for why
Utopia is impossible in literature and reality:\footnote{George Orwell, ``Why Socialists
Don't Believe in Fun,''
\textit{Tribune} (December 1943).\comment{1}}}

\begin{quote}
{
 It would seem that human beings are not able to describe, nor
perhaps to imagine, happiness except in terms of contrast\,\ldots The
inability of mankind to imagine happiness except in the form of relief,
either from effort or pain, presents Socialists with a serious problem.
Dickens can describe a poverty-stricken family tucking into a roast
goose, and can make them appear happy; on the other hand, the
inhabitants of perfect universes seem to have no spontaneous gaiety and
are usually somewhat repulsive into the bargain.}
\end{quote}


 For an expected utility maximizer, rescaling the utility function
to add a trillion to all outcomes is meaningless---it's
literally the same utility function, as a mathematical object. A
utility function describes the \textit{relative} intervals between
outcomes; that's what it is, mathematically speaking.


 But the human brain has distinct neural circuits for positive
feedback and negative feedback, and different varieties of positive and
negative feedback. There are people today who
``suffer'' from congenital
analgesia---a total absence of pain. I never heard that
\textit{insufficient pleasure} becomes intolerable to them.


 Congenital analgesics do have to inspect themselves carefully and
frequently to see if they've cut themselves or burned a
finger. Pain serves a purpose in the human mind design\,\ldots


 But that does not show there's no alternative
which could serve the same purpose. Could you delete pain and replace
it \textit{with an urge not to do certain things} that lacked the
intolerable subjective quality of pain? I do not know all the Law that
governs here, but I'd have to guess that yes, you
could; you could replace that side of yourself with something more akin
to an expected utility maximizer.


 Could you delete the human tendency to scale pleasures---delete
the accomodation, so that each new roast goose is as delightful as the
last? I would guess that you could. This verges perilously close to
deleting Boredom, which is right up there with Sympathy as an absolute
indispensable\,\ldots but to say that an old solution remains as
pleasurable is not to say that you will lose the urge to seek new and
better solutions.


 Can you make every roast goose as pleasurable as it would be in
contrast to starvation, without ever having starved?


 Can you prevent the pain of a dust speck irritating your eye from
being the new torture, if you've literally
\textit{never experienced} anything \textit{worse} than a dust speck
irritating your eye?


 Such questions begin to exceed my grasp of the Law, but I would
guess that the answer is: yes, it can be done. It is my experience in
such matters that once you do learn the Law, you can usually see how to
do weird-seeming things.

{
 So far as I know or can guess, David Pearce (\textit{The
Hedonistic Imperative}) is very probably right about the
\textit{feasibility} part, when he says:\footnote{David Pearce, \textit{The Hedonistic Imperative},
\url{http://www.hedweb.com/}, 1995.\comment{2}}}

\begin{quote}
{
 Nanotechnology and genetic engineering will abolish suffering in
all sentient life. The abolitionist project is hugely ambitious but
technically feasible. It is also instrumentally rational and morally
urgent. The metabolic pathways of pain and malaise evolved because they
served the fitness of our genes in the ancestral environment. They will
be replaced by a different sort of neural architecture---a motivational
system based on heritable gradients of bliss. States of sublime
well-being are destined to become the genetically pre-programmed norm
of mental health. It is predicted that the world's last
unpleasant experience will be a precisely dateable event.}
\end{quote}


 Is that\,\ldots what we \textit{want}?

\begin{quote}

 To just wipe away the last tear, and be done?

{
 Is there any good reason \textit{not} to, except status quo bias
 and a handful of worn rationalizations?}
\end{quote}


 What would be the \textit{alternative}? Or alternatives?


 To leave things as they are? Of course not. No God designed this
world; we have no reason to think it exactly optimal on any dimension.
If this world does not contain too much pain, then it must not contain
enough, and the latter seems unlikely.


 But perhaps\,\ldots


 You could cut out just the \textit{intolerable} parts of pain?


 Get rid of the Inquisition. Keep the sort of pain that tells you
not to stick your finger in the fire, or the pain that tells you that
you shouldn't have put your friend's
finger in the fire, or even the pain of breaking up with a lover.

{
 Try to get rid of the sort of pain that \textit{grinds down and
destroys} a mind. Or configure minds to be harder to damage.}


 You could have a world where there were broken legs, or even
broken hearts, but no broken \textit{people}. No child sexual abuse
that turns out more abusers. No people ground down by weariness and
drudging minor inconvenience to the point where they contemplate
suicide. No random meaningless endless sorrows like starvation or
\textsc{aids}.


 And if even a broken leg still seems too scary---


 Would we be less frightened of pain, if we were stronger, if our
daily lives did not already exhaust so much of our reserves?


 So that would be one alternative to Pearce's
world---if there are yet other alternatives, I haven't
thought them through in any detail.


 The path of courage, you might call it---the idea being that if
you eliminate the destroying kind of pain and strengthen the people,
then what's left shouldn't be
\textit{that} scary.


 A world where there is sorrow, but not massive systematic
\textit{pointless} sorrow, like we see on the evening news. A world
where pain, if it is not eliminated, at least does not
\textit{overbalance pleasure}. You could write stories about that
world, and they could read our stories.


 I do tend to be rather conservative around the notion of deleting
large parts of human nature. I'm not sure how many
major chunks you can delete until that balanced, conflicting, dynamic
structure collapses into something simpler, like an expected pleasure
maximizer.


 And so I do admit that it is the path of courage that appeals to
me.


 Then again, I haven't lived it both ways.


 Maybe I'm just \textit{afraid} of a world so
different as Analgesia---wouldn't that be an ironic
reason to walk ``the path of
courage''?

{
 Maybe the path of courage just seems like the \textit{smaller
change}{}---maybe I just have trouble empathizing over a larger gap.}


 But ``change'' is a moving
target.


 If a human child grew up in a \textit{less} painful world---if
they had never lived in a world of \textsc{aids} or cancer or slavery, and so
did not know these things as evils that had \textit{been triumphantly
eliminated}{}---and so did not feel that they were
``already done'' or that the world
was ``already changed enough''\,\ldots


 Would they take the next step, and try to eliminate the unbearable
pain of broken hearts, when someone's lover stops
loving them?


 And then what? Is there a point where \textit{Romeo and Juliet}
just seems less and less relevant, more and more a relic of some
distant forgotten world? Does there come some point in the transhuman
journey where the whole business of the negative reinforcement
circuitry can't possibly seem like anything except a
pointless hangover to wake up from?


 And if so, is there any point in \textit{delaying} that last step?
Or should we just throw away our fears and\,\ldots throw away our fears?


 I don't know.

\myendsectiontext


\bigskip

\mysection{Value is Fragile}


 If I had to pick a single statement that \textit{relies} on more
\textit{Overcoming Bias} content I've written than any
other, that statement would be: 

{
 \textit{Any Future }\textbf{\textit{not}}\textit{ shaped by a goal
system with detailed reliable inheritance from human morals and
metamorals will contain almost nothing of worth.}}


 ``Well,'' says the one,
``maybe according to your provincial \textit{human}
values, \textit{you} wouldn't like it. But I can easily
imagine a galactic civilization full of agents who are nothing like
\textit{you}, yet find great value and interest in their \textit{own}
goals. And that's fine by me. I'm not
so bigoted as you are. Let the Future go its own way, without trying to
bind it forever to the laughably primitive prejudices of a pack of
four-limbed Squishy Things---''


 My friend, I have \textit{no problem} with the thought of a
galactic civilization vastly unlike our own\,\ldots full of strange
beings who look nothing like me even in their own imaginations\,\ldots
pursuing pleasures and experiences I can't begin to
empathize with\,\ldots trading in a marketplace of unimaginable goods\,\ldots allying to pursue incomprehensible objectives\,\ldots people whose
life-stories I could never understand.


 That's what the Future looks like if things go
\textit{right}.


 If the chain of inheritance from human (meta)morals is broken, the
Future does \textit{not} look like this. It does \textit{not} end up
magically, delightfully incomprehensible.


 With very high probability, it ends up looking \textit{dull}.
Pointless. Something whose loss you wouldn't mourn.


 Seeing this as obvious is what requires that immense amount of
background explanation.


 And I'm not going to iterate through \textit{all}
the points and winding pathways of argument here, because that would
take us back through 75\% of my \textit{Overcoming Bias} posts. Except
to remark on how \textit{many} different things must be known to
constrain the final answer.


 Consider the incredibly important human value of
``boredom''{}---our desire not to do
``the same thing'' over and over and
over again. You can imagine a mind that contained \textit{almost} the
whole specification of human value, almost all the morals and
metamorals, but left out \textit{just this one thing}{}---


 {}---and so it spent until the end of time, and until the farthest
reaches of its light cone, replaying a single highly optimized
experience, over and over and over again.


 Or imagine a mind that contained almost the whole specification of
which sort of feelings humans most enjoy---but not the idea that those
feelings had important \textit{external referents.} So that the mind
just went around \textit{feeling} like it had made an important
discovery, \textit{feeling} it had found the perfect lover,
\textit{feeling} it had helped a friend, but not actually
\textit{doing} any of those things---having become its own experience
machine. And if the mind pursued those feelings \textit{and their
referents}, it would be a good future and true; but because this
\textit{one dimension} of value was left out, the future became
something dull. Boring and repetitive, because although this mind
\textit{felt} that it was encountering experiences of incredible
novelty, this feeling was in no wise true.


 Or the converse problem---an agent that contains all the aspects
of human value, \textit{except} the valuation of subjective experience.
So that the result is a nonsentient optimizer that goes around making
genuine discoveries, but the discoveries are not savored and enjoyed,
because there is no one there to do so. This, I admit, I
don't quite know to be possible. Consciousness does
still confuse me to some extent. But a universe with no one to bear
witness to it might as well not be.


 Value isn't just complicated, it's
\textit{fragile}. There is \textit{more than one dimension} of human
value, where \textit{if just that one thing is lost}, the Future
becomes null. A \textit{single} blow and \textit{all} value shatters.
Not every \textit{single} blow will shatter \textit{all} value---but
more than one possible ``single
blow'' will do so.


 And then there are the long defenses of this proposition, which
relies on 75\% of my \textit{Overcoming Bias} posts, so that it would
be more than one day's work to summarize all of it.
Maybe some other week. There's so many branches
I've seen that discussion tree go down.


 After all---a mind \textit{shouldn't} just go
around having the same experience over and over and over again. Surely
no superintelligence would be so grossly mistaken about the correct
action?


 Why would any supermind want something so inherently worthless as
the feeling of discovery without any real discoveries? Even if that
were its utility function, wouldn't it just notice that
its utility function was wrong, and rewrite it? It's
got free will, right?


 Surely, at least \textit{boredom} has to be a universal value. It
evolved in humans because it's valuable, right? So any
mind that doesn't share our dislike of repetition will
fail to thrive in the universe and be eliminated\,\ldots


 If you are familiar with the difference between instrumental
values and terminal values, \textit{and} familiar with the stupidity of
natural selection, \textit{and} you understand how this stupidity
manifests in the difference between executing adaptations versus
maximizing fitness, \textit{and} you know this turned instrumental
subgoals of reproduction into decontextualized unconditional emotions\,\ldots


 \ldots \textit{and} you're familiar with how the
tradeoff between exploration and exploitation works in Artificial
Intelligence\,\ldots


 \ldots then you might be able to see that the human form of boredom
that demands a steady trickle of novelty for its own sake
isn't a grand universal, but just a particular
algorithm that evolution coughed out into us. And you might be able to
see how the vast majority of possible expected utility maximizers would
only engage in just so much efficient exploration, and spend most of
their time exploiting the best alternative found so far, over and over
and over.


 That's a lot of background knowledge, though.


 And so on and so on and so on through 75\% of my posts on
\textit{Overcoming Bias}, and many chains of fallacy and
counter-explanation. Some week I may try to write up the whole diagram.
But for now I'm going to assume that
you've read the arguments, and just deliver the
conclusion:


 We can't relax our grip on the future---let go of
the steering wheel---and still end up with anything of value.


 And those who think we \textit{can}{}---

{
 {}---they're trying to be cosmopolitan. I
understand that. I read those same science fiction books as a kid: The
provincial villains who enslave aliens for the crime of not looking
just like humans. The provincial villains who enslave helpless AIs in
durance vile on the assumption that silicon can't be
sentient. And the cosmopolitan heroes who understand that \textit{minds
don't have to be just like us to be embraced as
valuable}{}---}


 I read those books. I once believed them. But the beauty that
jumps out of one box is not jumping out of \textit{all} boxes. If you
leave behind all order, what is left is not the perfect answer; what is
left is perfect noise. Sometimes you have to abandon an old design rule
to build a better mousetrap, but that's not the same as
giving up all design rules and collecting wood shavings into a heap,
with every pattern of wood as good as any other. The old rule is always
abandoned at the behest of some higher rule, some higher criterion of
value that governs.


 If you loose the grip of human morals and metamorals---the result
is not mysterious and alien and beautiful by the standards of human
value. It is moral noise, a universe tiled with paperclips. To change
away from human morals \textit{in the direction of improvement rather
than entropy} requires a criterion of improvement; and that criterion
would be physically represented in our brains, and our brains alone.


 Relax the grip of human value upon the universe, and it will end
up \textit{seriously} valueless. Not strange and alien and wonderful,
shocking and terrifying and beautiful beyond all human imagination.
Just---tiled with paperclips.


 It's only some \textit{humans}, you see, who have
this idea of embracing manifold varieties of mind---of wanting the
Future to be something greater than the past---of being not bound to
our past selves---of trying to change and move forward.


 A paperclip maximizer just chooses whichever action leads to the
greatest number of paperclips.


 No free lunch. You want a wonderful and mysterious universe?
That's \textit{your} value. \textit{You} work to create
that value. Let that value exert its force through you who represents
it; let it make decisions in you to shape the future. And maybe you
shall indeed obtain a wonderful and mysterious universe.


 No free lunch. Valuable things appear because a goal system that
values them takes action to create them. Paperclips
don't materialize from nowhere for a paperclip
maximizer. And a wonderfully alien and mysterious Future will not
materialize from nowhere for us humans, if our values that prefer it
are physically obliterated---or even \textit{disturbed} in the wrong
dimension. Then there is nothing left in the universe that works to
make the universe valuable.


 You \textit{do} have values, even when you're
trying to be ``cosmopolitan,''
trying to display a properly virtuous appreciation of alien minds. Your
values are then faded further into the invisible background{}---they
are less \textit{obviously} human. Your brain probably
won't even generate an alternative so awful that it
would wake you up, make you say ``No! Something went
wrong!'' even at your most cosmopolitan. E.g.,
``a nonsentient optimizer absorbs all matter in its
future light cone and tiles the universe with
paperclips.'' You'll just imagine
strange alien worlds to appreciate.


 Trying to be
``cosmopolitan''---to be \textit{a
citizen of the cosmos}{}---just strips off a \textit{surface veneer} of
goals that seem \textit{obviously}
``human.''


 But if you wouldn't like the Future tiled over
with paperclips, and you would prefer a civilization of\,\ldots


 \ldots sentient beings\,\ldots


 \ldots with enjoyable experiences\,\ldots


 \ldots that aren't the \textit{same} experience
over and over again\,\ldots


 \ldots and are bound to something besides just being a sequence of
internal pleasurable feelings\,\ldots


 \ldots learning, discovering, freely choosing\,\ldots


 \ldots well, my posts on Fun Theory go into \textit{some} of the
hidden details on those short English words.


 Values that you might praise as \textit{cosmopolitan} or
\textit{universal} or \textit{fundamental} or \textit{obvious common
sense} are represented in your brain just as much as those values that
you might dismiss as \textit{merely human}. Those values come of the
long history of humanity, and the morally miraculous stupidity of
evolution that created us. (And once I \textit{finally} came to that
realization, I felt less ashamed of values that seemed
``provincial''---but
that's another matter.)


 These values do \textit{not} emerge in all possible minds. They
will \textit{not} appear from nowhere to rebuke and revoke the utility
function of an expected paperclip maximizer.


 Touch too hard in the wrong dimension, and the physical
representation of those values will shatter---and \textit{not come
back}, for there will be nothing left to \textit{want} to bring it
back.


 And the \textit{referent} of those values---a worthwhile
universe---would no longer have any physical reason to come into
being.


 Let go of the steering wheel, and the Future crashes.

\myendsectiontext

\mysection{The Gift We Give to Tomorrow}


 How, oh how, could the universe, itself unloving and mindless,
cough up minds who were capable of love? 


 ``No mystery in that,'' you
say. ``It's just a matter of natural
selection.''


 But natural selection is cruel, bloody, and bloody stupid. Even
when, on the surface of things, biological organisms
aren't \textit{directly} fighting each
other---aren't \textit{directly} tearing at each other
with claws---there's still a deeper competition going
on between the genes. Genetic information is created when genes
increase their \textit{relative} frequency in the next
generation---what matters for ``genetic
fitness'' is not how many children you have, but that
you have \textit{more} children than others. It is quite possible for a
species to evolve to extinction, if the winning genes are playing
negative-sum games.


 How, oh how, could such a process create beings capable of love?


 ``No mystery,'' you say.
``There is never any mystery-in-the-world. Mystery is
a property of questions, not answers. A mother's
children share her genes, so the mother loves her
children.''


 But sometimes mothers adopt children, and still love them. And
mothers love their children for themselves, not for their genes.


 ``No mystery,'' you say.
``Individual organisms are adaptation-executers, not
fitness-maximizers. Evolutionary psychology is not about deliberately
maximizing fitness---through most of human history, we
didn't know genes existed. We don't
calculate our acts' effect on genetic fitness
consciously, or even subconsciously.''


 But human beings form friendships even with non-relatives. How can
that be?


 ``No mystery, for hunter-gatherers often play
Iterated Prisoner's Dilemmas, the solution to which is
reciprocal altruism. Sometimes the most dangerous human in the tribe is
not the strongest, the prettiest, or even the smartest, but the one who
has the most allies.''


 Yet not all friends are fair-weather friends; we have a concept of
true friendship---and some people have sacrificed their life for their
friends. Would not such a devotion tend to remove itself from the gene
pool?


 ``You said it yourself: we have concepts of true
friendship and of fair-weather friendship. We can tell, or try to tell,
the difference between someone who considers us a valuable ally, and
someone executing the friendship adaptation. We
wouldn't be true friends with someone who we
didn't think was a true friend to us---and someone with
many \textit{true} friends is far more formidable than someone with
many fair-weather allies.''


 And Mohandas Gandhi, who really did turn the other cheek? Those
who try to serve all humanity, whether or not all humanity serves them
in turn?


 ``That perhaps is a more complicated story. Human
beings are not just social animals. We are political animals who argue
linguistically about policy in adaptive tribal contexts. Sometimes the
formidable human is not the strongest, but the one who can most
skillfully argue that their preferred policies match the preferences of
others.''


 Um\,\ldots that doesn't explain Gandhi, or am I
missing something?


 ``The point is that we have the ability to
\textit{argue} about `What should be
done?' as a \textit{proposition}{}---we can make those
arguments and respond to those arguments, without which politics could
not take place.''


 Okay, but Gandhi?


 ``Believed certain complicated propositions about
`What should be done?' and did
them.''


 That sounds suspiciously like it could explain any possible human
behavior.


 ``If we traced back the chain of causality
through all the arguments, it would involve: a moral architecture that
had the ability to argue \textit{general abstract} moral propositions
like `What should be done to people?';
appeal to hardwired intuitions like fairness, a concept of duty, pain
aversion, empathy; something like a preference for simple moral
propositions, probably reused from our pre-existing Occam prior; and
the end result of all this, plus perhaps memetic selection effects, was
`You should not hurt people' in full
generality---''


 And that gets you Gandhi.


 ``Unless you think it was magic, it has to fit
into the lawful causal development of the universe
somehow.''


 I certainly won't postulate magic, under any
name.


 ``Good.''


 But come on\,\ldots doesn't it seem a little\,\ldots
\textit{amazing}\,\ldots that hundreds of millions of years worth of
evolution's death tournament could cough up mothers and
fathers, sisters and brothers, husbands and wives, steadfast friends
and honorable enemies, true altruists and guardians of causes, police
officers and loyal defenders, even artists sacrificing themselves for
their art, all practicing so many kinds of love? For so many things
other than genes? Doing their part to make their world less ugly,
something besides a sea of blood and violence and mindless
replication?


 ``Are you claiming to be surprised by this? If
so, question your underlying model, for it has led you to be surprised
by the true state of affairs.

\begin{quote}
{
 Since the beginning,\newline
 not one unusual thing\newline
 has ever happened.''}
\end{quote}


 But how is it \textit{not} surprising?


 ``What would you suggest? That some sort of
shadowy figure stood behind the scenes and directed
evolution?''


 Hell no. But---


 ``Because if you \textit{were} suggesting that, I
would have to ask how that shadowy figure \textit{originally} decided
that love was a \textit{desirable} outcome of evolution. I would have
to ask where that figure got preferences that included things like
love, friendship, loyalty, fairness, honor, romance, and so on. On
evolutionary psychology, we can see how \textit{that specific outcome}
came about---how \textit{those particular goals rather than others}
were \textit{generated in the first place.} You can call it
`surprising' all you like. But when you
really do understand evolutionary psychology, you can see how parental
love and romance and honor, and even true altruism and moral arguments,
\textit{bear the specific design signature of natural selection} in
particular adaptive contexts of the hunter-gatherer savanna. So if
there was a shadowy figure, it must itself have evolved---and that
obviates the whole point of postulating it.''


 I'm not postulating a shadowy figure!
I'm just asking how human beings ended up so
\textit{nice.}


 ``Nice! Have you looked at this planet lately? We
bear all those other emotions that evolved, too---which would tell you
very well that we evolved, should you begin to doubt it. Humans
aren't always nice.''


 We're one hell of a lot nicer than the process
that produced us, which lets elephants starve to death when they run
out of teeth, which doesn't anesthetize a gazelle even
as it lays dying and is of no further importance to evolution one way
or the other. It doesn't take much to be nicer than
evolution. To have the \textit{theoretical capacity} to make one single
gesture of mercy, to feel a single twinge of empathy, is to be nicer
than evolution.


 How did evolution, which is itself so uncaring, create minds on
that qualitatively higher moral level? How did evolution, which is so
ugly, end up doing anything so \textit{beautiful}?


 ``Beautiful, you say? Bach's
\textit{Little Fugue in G Minor} may be beautiful, but the sound waves,
as they travel through the air, are not stamped with tiny tags to
specify their beauty. If you wish to find \textit{explicitly encoded} a
measure of the fugue's beauty, you will have to look at
a human brain---nowhere else on Earth will you find it. Not upon
the seas or the mountains will you find such judgments written: they
are not minds; they cannot think.''


 Perhaps that is so. Yet evolution \textit{did in fact} give us the
ability to admire the beauty of a flower. That still seems to call for
some deeper answer.


 ``Do you not see the circularity in your
question? If beauty were like some great light in the sky that shined
from outside humans, then your question might make sense---though there
would still be the question of how humans came to perceive that light.
You evolved with a psychology alien to evolution: Evolution has nothing
like the intelligence or the precision required to exactly quine its
goal system. In coughing up the first true minds,
evolution's simple fitness criterion shattered into a
thousand values. You evolved with a psychology that attaches utility to
things which evolution does not care about---human life, human
happiness. And then you look back and say, `How
marvelous!' You marvel and you wonder at the fact that
your values coincide with themselves.''


 But then---it is still amazing that this particular circular loop,
and not some other loop, came into the world. That we find ourselves
praising love and not hate, beauty and not ugliness.


 ``I don't think you understand.
To you, it seems natural to privilege the beauty and altruism as
special, as preferred, because you value them highly. And you
don't see this as an unusual fact about yourself,
because many of your friends do likewise. So you expect that a ghost of
perfect emptiness would also value life and happiness---and then, from
this standpoint outside reality, a great coincidence would indeed have
occurred.''


 But you can make arguments for the importance of beauty and
altruism from first principles---that our aesthetic senses lead us to
create new complexity, instead of repeating the same things over and
over; and that altruism is important because it takes us outside
ourselves, gives our life a higher meaning than sheer brute
selfishness.


 ``And that argument is going to move even a ghost
of perfect emptiness? Because you've appealed to
slightly different values? Those aren't first
principles. They're just \textit{different} principles.
Speak in a grave and philosophical register, and still you shall find
no universally compelling arguments. All you've done is
pass the recursive buck.''


 You don't think that, somehow, we evolved to
\textit{tap into} something beyond---


 ``What good does it do to suppose something
beyond? Why should we pay more attention to this beyond thing than we
pay to our existence as humans? How does it alter your personal
responsibility to say that you were only following the orders of the
beyond thing? And you would still have evolved to let the beyond thing,
rather than something else, direct your actions. It would be
\textit{too much coincidence}.''


 Too much coincidence?


 ``A flower is beautiful, you say. Do you think
there is no story behind that beauty, or that science does not know the
story? Flower pollen is transmitted by bees, so by sexual selection,
flowers evolved to attract bees---by imitating certain mating signs of
bees, as it happened; the flowers' patterns would look
more intricate if you could see in the ultraviolet. Now healthy flowers
are a sign of fertile land, likely to bear fruits and other treasures,
and probably prey animals as well; so is it any wonder that humans
evolved to be attracted to flowers? But for there to be some great
light written upon the very stars---those huge unsentient balls of
burning hydrogen---which \textit{also} said that flowers were
beautiful, now \textit{that} would be far too much
coincidence.''


 So you explain away the beauty of a flower?


 ``No. I explain it. Of course
there's a story behind the beauty of flowers, behind
the fact that we find them beautiful. Behind ordered events, one finds
ordered stories; and what has no story is the product of random noise,
which is hardly any better. If you cannot take joy in things that have
stories behind them, your life will be empty indeed. I
don't think I take any less joy in a flower than you
do. More so, perhaps, because I take joy in its story as
well.''


 Perhaps, as you say, there is no surprise from a causal
viewpoint---no disruption of the physical order of the universe. But it
still seems to me that, in this creation of humans by evolution,
something happened that is precious and marvelous and wonderful. If we
cannot call it a physical miracle, then call it a moral miracle.


 ``Because it's only a miracle
from the perspective of the morality that was produced, thus explaining
away all of the apparent coincidence from a merely causal and physical
perspective?''


 Well\,\ldots I suppose you could interpret the term that way, yes. I
just meant something that was immensely surprising and wonderful on a
moral level, even if it is not surprising on a physical level.


 ``I think that's what I
said.''


 But it still seems to me that you, from your own view, drain
something of that wonder away.


 ``Then you have problems taking joy in the merely
real. Love has to begin somehow. It has to enter the universe
somewhere. It is like asking how life itself begins---and though you
were born of your father and mother, and they arose from their living
parents in turn, if you go far and far and far away back, you will
finally come to a replicator that arose by pure accident---the border
between life and unlife. So too with love.


 ``A complex pattern must be explained by a cause
that is not already that complex pattern. Not just the event must be
explained, but the very shape and form. For love to first enter Time,
it must come of something that is not love; if this were not possible,
then love could not be.


 ``Even as life itself required that first
replicator to come about by accident, parentless but still caused: far,
far back in the causal chain that led to you: 3.85 billion years ago,
in some little tidal pool.


 ``Perhaps your children's
children will ask how it is that they are capable of love.


 ``And their parents will say: Because we, who
also love, created you to love.


 ``And your children's children
will ask: But how is it that \textit{you} love?


 ``And their parents will reply: Because our own
parents, who also loved, created us to love in turn.


 ``Then your children's children
will ask: But where did it all begin? Where does the recursion end?


 ``And their parents will say: Once upon a time,
long ago and far away, ever so long ago, there were intelligent beings
who were not themselves intelligently designed. Once upon a time, there
were lovers created by something that did not love.


 ``Once upon a time, when all of civilization was
a single galaxy and a single star: and a single planet. A place called
Earth.


 ``Long ago, and far away, ever so long
ago.''

\myendsectiontext

\chapter{Quantified Humanism}

\mysection{Scope Insensitivity}


 Once upon a time, three groups of subjects were asked how much
they would pay to save 2,000 / 20,000 / 200,000 migrating birds from
drowning in uncovered oil ponds. The groups respectively answered \$80,
\$78, and \$88.\footnote{William H. Desvousges et al., \textit{Measuring Nonuse Damages
Using Contingent Valuation: An Experimental Evaluation of Accuracy},
technical report (Research Triangle Park, NC: RTI International, 2010),
doi:10.3768/rtipress.2009.bk.0001.1009.\comment{1}} This is \textit{scope insensitivity}
or \textit{scope neglect}: the number of birds saved---the
\textit{scope} of the altruistic action---had little effect on
willingness to pay. 


 Similar experiments showed that Toronto residents would pay little
more to clean up all polluted lakes in Ontario than polluted lakes in a
particular region of Ontario,\footnote{Daniel Kahneman, ``Comments by Professor
Daniel Kahneman,'' in \textit{Valuing Environmental
Goods: An Assessment of the Contingent Valuation Method}, ed. Ronald G.
Cummings, David S. Brookshire, and William D. Schulze, vol. 1.B,
Experimental Methods for Assessing Environmental Benefits (Totowa, NJ:
Rowman \& Allanheld, 1986), 226--235,
\url{http://yosemite.epa.gov/ee/epa/eerm.nsf/vwAN/EE-0280B-04.pdf}.\comment{2}} or that residents of
four western US states would pay only 28\% more to protect all 57
wilderness areas in those states than to protect a single
area.\footnote{Daniel L. McFadden and Gregory K. Leonard,
``Issues in the Contingent Valuation of Environmental
Goods: Methodologies for Data Collection and
Analysis,'' in \textit{Contingent Valuation: A
Critical Assessment}, ed. Jerry A. Hausman, Contributions to Economic
Analysis220 (New York: North-Holland, 1993), 165--215,
doi:10.1108/S0573-8555(1993)0000220007.\comment{3}} People visualize ``a single
exhausted bird, its feathers soaked in black oil, unable to
escape.''\footnote{Kahneman, Ritov, and Schkade, ``Economic
Preferences or Attitude Expressions?''\comment{4}} This image, or
\textit{prototype}, calls forth some level of emotional arousal that is
primarily responsible for willingness-to-pay---and the image is the
same in all cases. As for scope, it gets tossed out the window---no
human can visualize 2,000 birds at once, let alone 200,000. The usual
finding is that \textit{exponential} increases in scope create
\textit{linear} increases in willingness-to-pay---perhaps corresponding
to the linear time for our eyes to glaze over the zeroes; this small
amount of affect is added, not multiplied, with the prototype affect.
This hypothesis is known as ``valuation by
prototype.''


 An alternative hypothesis is ``purchase of moral
satisfaction.'' People spend enough money to create a
\textit{warm glow} in themselves, a sense of having done their duty.
The level of spending needed to purchase a warm glow depends on
personality and financial situation, but it certainly has nothing to do
with the number of birds.

{
 We are insensitive to scope even when human lives are at stake:
Increasing the alleged risk of chlorinated drinking water from 0.004 to
2.43 annual deaths per 1,000---a factor of 600---increased
willingness-to-pay from \$3.78 to \$15.23.\footnote{Richard T. Carson and Robert Cameron Mitchell,
``Sequencing and Nesting in Contingent Valuation
Surveys,'' \textit{Journal of Environmental Economics
and Management} 28, no. 2 (1995): 155--173,
doi:10.1006/jeem.1995.1011.\comment{5}} Baron and
Greene found no effect from varying lives saved by a factor of
10.\footnote{Jonathan Baron and Joshua D. Greene,
``Determinants of Insensitivity to Quantity in
Valuation of Public Goods: Contribution, Warm Glow, Budget Constraints,
Availability, and Prominence,'' \textit{Journal of
Experimental Psychology: Applied} 2, no. 2 (1996): 107--125,
doi:10.1037/1076-898X.2.2.107.\comment{6}}}

{
 A paper entitled ``Insensitivity to the value of
human life: A study of psychophysical numbing''
collected evidence that our perception of human deaths follows
Weber's Law---obeys a logarithmic scale where the
``just noticeable difference'' is a
constant fraction of the whole. A proposed health program to save the
lives of Rwandan refugees garnered far higher support when it promised
to save 4,500 lives in a camp of 11,000 refugees, rather than 4,500 in
a camp of 250,000. A potential disease cure had to promise to save far
more lives in order to be judged worthy of funding, if the disease was
originally stated to have killed 290,000 rather than 160,000 or 15,000
people per year.\footnote{David Fetherstonhaugh et al., ``Insensitivity
to the Value of Human Life: A Study of Psychophysical
Numbing,'' \textit{Journal of Risk and Uncertainty}
14, no. 3 (1997): 283--300, doi:10.1023/A:1007744326393.\comment{7}}}


 The moral: If you want to be an effective altruist, you have to
think it through with the part of your brain that processes those
unexciting inky zeroes on paper, not just the part that gets real
worked up about that poor struggling oil-soaked bird.

\myendsectiontext


\bigskip

\mysection{One Life Against the World}

\begin{quote}

 Whoever saves a single life, it is as if he had saved the whole
world.

{\raggedleft
 {}---The Talmud, Sanhedrin 4:5
 \par}
\end{quote}



 It's a beautiful thought, isn't
it? Feel that warm glow.


 I can testify that helping one person \textit{feels} just as good
as helping the whole world. Once upon a time, when I was burned out for
the day and wasting time on the Internet---it's a bit
complicated, but essentially I managed to turn
someone's whole life around by leaving an anonymous
blog comment. I wasn't expecting it to have an effect
that large, but it did. When I discovered what I had accomplished, it
gave me a \textit{tremendous} high. The euphoria lasted through that
day and into the night, only wearing off somewhat the next morning. It
felt just as good (this is the scary part) as the euphoria of a major
scientific insight, which had previously been my best referent for what
it might feel like to do drugs.


 Saving one life probably \textit{does} feel just as good as being
the first person to realize what makes the stars shine. It probably
\textit{does} feel just as good as saving the entire world.


 But if you ever have a choice, dear reader, between saving a
single life and saving the whole world---then save the world. Please.
Because beyond that warm glow is \textit{one heck of a}
\textit{gigantic difference.} For some people, the notion that saving
the world is \textit{significantly better} than saving one human life
will be obvious, like saying that six billion dollars is worth more
than one dollar, or that six cubic kilometers of gold weighs more than
one cubic meter of gold. (And never mind the expected value of
posterity.) Why might it \textit{not} be obvious? Well, suppose
there's a qualitative duty to save what lives you
can---then someone who saves the world, and someone who saves one human
life, are just fulfilling the same duty. Or suppose that we follow the
Greek conception of personal virtue, rather than consequentialism;
someone who saves the world is virtuous, but not six billion times as
virtuous as someone who saves one human life. Or perhaps the value of
one human life is already too great to comprehend---so that the passing
grief we experience at funerals is an infinitesimal underestimate of
what is lost---and thus passing to the entire world changes little.


 I agree that one human life is of unimaginably high value. I also
hold that two human lives are twice as unimaginably valuable. Or to put
it another way: Whoever saves one life, if it is as if they had saved
the whole world; whoever saves ten lives, it is as if they had saved
ten worlds. Whoever \textit{actually} saves the whole world---not to be
confused with pretend rhetorical saving the world---it is as if they
had saved an intergalactic civilization.


 Two deaf children are sleeping on the railroad tracks, the train
speeding down; you see this, but you are too far away to save the
children. I'm nearby, within reach, so I leap forward
and drag one child off the railroad tracks---and then stop, calmly
sipping a Diet Pepsi as the train bears down on the second child.
``\textit{Quick!}'' you scream to
me. ``\textit{Do something!}'' But
(I call back) I already saved one child from the train tracks, and thus
I am ``unimaginably'' far ahead on
points. Whether I save the second child, or not, I will still be
credited with an ``unimaginably''
good deed. Thus, I have no further motive to act.
Doesn't sound right, does it?


 Why should it be any different if a philanthropist spends \$10
million on curing a rare but spectacularly fatal disease which afflicts
only a hundred people planetwide, when the same money has an equal
probability of producing a cure for a less spectacular disease that
kills 10\% of 100,000 people? I don't think it
\textit{is} different. When human lives are at stake, we have a duty to
\textit{maximize}, not satisfice; and this duty has the same strength
as the original duty to save lives. Whoever knowingly chooses to save
one life, when they could have saved two---to say nothing of a thousand
lives, or a world---they have damned themselves as thoroughly as any
murderer.


 It's not cognitively easy to spend money to save
lives, since clich√© methods that instantly leap to mind
don't work or are counterproductive. (I will write
later on why this tends to be so.) Stuart Armstrong also points out
that if we are to disdain the philanthropist who spends life-saving
money inefficiently, we should be consistent and disdain more those who
could spend money to save lives but don't.

\myendsectiontext

\mysection{The Allais Paradox}


 Choose between the following two options:

\begin{quote}

 1A. \$24,000, with certainty.

{
 1B. 33/34 chance of winning \$27,000, and 1/34 chance of winning
 nothing.}
\end{quote}


 Which seems more intuitively appealing? And which one would you
choose in real life? Now which of these two options would you
intuitively prefer, and which would you choose in real life?

\begin{quote}

 2A. 34\% chance of winning \$24,000, and 66\% chance of winning
nothing.

{
 2B. 33\% chance of winning \$27,000, and 67\% chance of winning
 nothing.}
\end{quote}


 The Allais Paradox---as Allais called it, though
it's not really a paradox---was one of the first
conflicts between decision theory and human reasoning to be
experimentally exposed, in 1953.\footnote{Maurice Allais, ``Le Comportement de
l'Homme Rationnel devant le Risque: Critique des
Postulats et Axiomes de l'Ecole
Americaine,'' \textit{Econometrica} 21, no. 4 (1953):
2, doi:10.2307/1907921; Daniel Kahneman and Amos Tversky,
``Prospect Theory: An Analysis of Decision Under
Risk,'' \textit{Econometrica} 47 (1979): 263--292.\comment{1}}
I've modified it slightly for ease of math, but the
essential problem is the same: Most people prefer 1A to 1B, and most
people prefer 2B to 2A. Indeed, in within-subject comparisons, a
majority of subjects express both preferences simultaneously.


 This is a problem because the 2s are equal to a one-third chance
of playing the 1s. That is, 2A is equivalent to playing gamble 1A with
34\% probability, and 2B is equivalent to playing 1B with 34\%
probability.


 Among the axioms used to prove that
``consistent'' decisionmakers can be
viewed as maximizing expected utility is the Axiom of Independence: If
$X$ is strictly preferred to $Y$, then a probability $P$ of $X$ and $(1 - P)$ of
$Z$ should be strictly preferred to $P$ chance of $Y$ and $(1 - P)$ chance of
$Z$.


 All the axioms are consequences, as well as antecedents, of a
consistent utility function. So it must be possible to prove that the
experimental subjects above \textit{can't} have a
consistent utility function over outcomes. And indeed, you
can't simultaneously have:


\whencolumns{
\begin{equation*}
U(\$24,000) > (33/34) \times U(\$27,000) +
(1/34) \times U(\$0)
\end{equation*}
\begin{equation*}
0.34 \times U(\$24,000) + 0.66 \times U(\$0)
< 0.33 \times U(\$27,000) + 0.67 \times U(\$0).
\end{equation*}
}{
\begin{align*}
  &U(\$24,000) \\
  &> (33/34) \times U(\$27,000) +
  (1/34) \times U(\$0) \\
  & \\
&0.34 \times U(\$24,000) + 0.66 \times U(\$0) \\
&< 0.33 \times U(\$27,000) + 0.67 \times U(\$0).
\end{align*}
}



 These two equations are algebraically inconsistent, regardless of
$U$, so the Allais Paradox has nothing to do with the diminishing
marginal utility of money. 


 Maurice Allais initially defended the revealed preferences of the
experimental subjects---he saw the experiment as exposing a flaw in the
conventional ideas of utility, rather than exposing a flaw in human
psychology. This was 1953, after all, and the heuristics-and-biases
movement wouldn't really get started for another two
decades. Allais thought his experiment just showed that the Axiom of
Independence clearly wasn't a good idea in real life.


 (How naive, how foolish, how simplistic is Bayesian decision
theory\,\ldots)

 Surely the \textit{certainty} of having \$24,000 should count for
\textit{something}. You can \textit{feel} the difference, right? The
solid reassurance?\footnote{Edit 2018: If the Bayesian view still isn't making sense, try it out (with slightly adjusted probabilities). Get out three six sided dice.  (Two white and one red would work well).  The options now are:

  1A. \$24,000 no matter what the dice roll.

  1B. \$27,000 if the white dice do \textit{not} roll 1-1.

  2A. \$24,000 if the red die rolls 3 or 6, ignore the white dice.

  2B. \$27,000 if the red die rolls 3 or 6, and the white dice are \textit{not} 1-1.
}


 (I'm starting to think of this as
``naive philosophical
realism''---supposing that our intuitions directly
expose truths about which strategies are wiser, as though it were a
directly perceived fact that ``1A is superior to
1B.'' Intuitions \textit{directly} expose truths
about human cognitive functions, and only \textit{indirectly} expose
(after we reflect on the cognitive functions themselves) truths about
rationality.)


 ``But come now,'' you say,
``is it really such a terrible thing to depart from
Bayesian beauty?'' Okay, so the subjects
didn't follow the neat little
``independence axiom'' espoused by
the likes of von Neumann and Morgenstern. Yet who says that things
\textit{must} be neat and tidy?


 Why fret about elegance, if it makes us take risks we
don't want? Expected utility tells us that we ought to
assign some kind of number to an outcome, and then multiply that value
by the outcome's probability, add them up, etc. Okay,
but why do we \textit{have} to do that? Why not make up more palatable
rules instead?


 There is always a price for leaving the Bayesian Way.
That's what coherence and uniqueness theorems are all
about.


 In this case, if an agent prefers 1A to 1B, and 2B to 2A, it
introduces a form of \textit{preference reversal}{}---a \textit{dynamic
inconsistency} in the agent's planning. You become a
\textit{money pump}.


 Suppose that at 12:00 p.m. I roll a hundred-sided die. If the die
shows a number greater than 34, the game terminates. Otherwise, at
12:05 p.m. I consult a switch with two settings, A and B. If the
setting is A, I pay you \$24,000. If the setting is B, I roll a
34-sided die and pay you \$27,000 unless the die shows
``34,'' in which case I pay you
nothing.


 Let's say you prefer 1A over 1B, and 2B over 2A,
and you would pay a single penny to indulge each preference. The switch
starts in state A. Before 12:00 p.m., you pay me a penny to throw the
switch to B. The die comes up 12. After 12:00 p.m. and before 12:05
p.m., you pay me a penny to throw the switch to A.


 I have taken your two cents on the subject. If you indulge your
intuitions, and dismiss mere elegance as a pointless obsession with
neatness, then don't be surprised when your pennies get
taken from you\,\ldots


 (I think the same failure to proportionally devalue the emotional
impact of small probabilities is responsible for the lottery.)

\myendsectiontext


\mysection{Zut Allais!}


 Huh! I was not expecting so many commenters to defend the
preference reversal. Looks like I ran into an inferential distance. 


 It probably helps in interpreting the Allais Paradox to have
absorbed more of the \textit{gestalt} of the field of heuristics and
biases, such as:

\begin{itemize}
\item {
 Experimental subjects tend to defend incoherent preferences even
when they're \textit{really} silly.}

\item {
 People put very high values on small shifts in probability away
 from 0 or 1 (the certainty effect).}
\end{itemize}


 Let's start with the issue of incoherent
preferences---preference reversals, dynamic inconsistency, money pumps,
that sort of thing.


 Anyone who knows a little prospect theory will have no trouble
constructing cases where people say they would prefer to play gamble A
rather than gamble B; but when you ask them to price the gambles they
put a higher value on gamble B than gamble A. There are different
perceptual features that become salient when you ask
``Which do you prefer?'' in a direct
comparison, and ``How much would you
pay?'' with a single item.


 This choice of gambles typically generates a preference reversal:

\begin{enumerate}
\item {
 1/3 chance to win \$16 and 2/3 chance to lose \$2.}

\item {
  99/100 chance to win \$4 and 1/100 chance to lose \$1.}
\end{enumerate}

{
 Most people will rather play 2 than 1. But if you ask them to
price the bets separately---ask for a price at which they would be
indifferent between having that amount of money, and having a chance to
play the gamble---people will put a higher price on 1 than on
2.\footnote{Sarah Lichtenstein and Paul Slovic,
``Reversals of Preference Between Bids and Choices in
Gambing Decisions,'' \textit{Journal of Experimental
Psychology} 89, no. 1 (1971): 46--55.\comment{1}}}


 So first you sell them a chance to play bet 1, at their stated
price. Then you offer to trade bet 1 for bet 2. Then you buy bet 2 back
from them, at their stated price. Then you do it again. Hence the
phrase, ``money pump.''


 Or to paraphrase Steve Omohundro: If you would rather be in
Oakland than San Francisco, and you would rather be in San Jose than
Oakland, and you would rather be in San Francisco than San Jose,
you're going to spend an awful lot of money on taxi
rides.


 Amazingly, people \textit{defend} these preference patterns. Some
subjects abandon them after the money-pump effect is pointed
out---revise their price or revise their preference---but some subjects
defend them.

{
 On one occasion, gamblers in Las Vegas played these kinds of bets
for real money, using a roulette wheel. And afterward, one of the
researchers tried to explain the problem with the incoherence between
their pricing and their choices. From the
transcript:\footnote{William Poundstone, \textit{Priceless: The Myth of Fair Value
(and How to Take Advantage of It)} (Hill \& Wang, 2010).\comment{2}}\supercomma\footnote{Sarah Lichtenstein and Paul Slovic, eds., \textit{The
Construction of Preference} (Cambridge University Press, 2006).\comment{3}}}

\begin{quotation}

 \textsc{Sarah Lichtenstein}: ``Well, how about the bid for
Bet A? Do you have any further feelings about it now that you know you
are choosing one but bidding more for the other
one?''


 \textsc{Subject}: ``It's kind of strange,
but no, I don't have any feelings at all whatsoever
really about it. It's just one of those things. It
shows my reasoning process isn't so good, but, other
than that, I\,\ldots no qualms.''


 \ldots


 \textsc{Lichtenstein}: ``Can I persuade you that it is an
irrational pattern?''


 \textsc{Subject}: ``No, I don't think you
probably could, but you could try.''


 \ldots


 \textsc{Lichtenstein}: ``Well, now let me suggest what has
been called a money-pump game and try this out on you and see how you
like it. If you think Bet A is worth 550 points [points were converted
to dollars after the game, though not on a one-to-one basis] then you
ought to be willing to give me 550 points if I give you the bet\,\ldots''


 \textit{\ldots}


 \textsc{Lichtenstein}: ``So you have Bet A, and I say,
`Oh, you'd rather have Bet B
wouldn't you?'\,''


 \ldots


 \textsc{Subject}: ``I'm losing
money.''


 \textsc{Lichtenstein}: ``I'll buy Bet B
from you. I'll be generous; I'll pay
you more than 400 points. I'll pay you 401 points. Are
you willing to sell me Bet B for 401 points?''


 \textsc{Subject}: ``Well, certainly.''


 \ldots


 \textsc{Lichtenstein}: ``I'm now ahead 149
points.''


 \textsc{Subject}: ``That's good reasoning
on my part. (laughs) How many times are we going to go through
this?''


 \ldots


 \textsc{Lichtenstein}: ``Well, I think
I've pushed you as far as I know how to push you short
of actually insulting you.''

{
 \textsc{Subject}: ``That's
 right.''}
\end{quotation}

{
 You want to scream, ``Just \textit{give up
already!} Intuition \textit{isn't always
right!}''}


 And then there's the business of the strange value
that people attach to certainty. My books are packed up for the move,
but I believe that one experiment showed that a shift from 100\%
probability to 99\% probability weighed larger in
people's minds than a shift from 80\% probability to
20\% probability.

{
 The problem with attaching a huge extra value to certainty is that
\textit{one time's certainty} is \textit{another
time's probability.}}


 In the last essay, I talked about the Allais Paradox:

\begin{itemize}
\item {
 1A. \$24,000, with certainty.}

\item {
 1B. 33/34 chance of winning \$27,000, and 1/34 chance of winning
nothing.}

\item {
 2A. 34\% chance of winning \$24,000, and 66\% chance of winning
nothing.}

\item {
 2B. 33\% chance of winning \$27,000, and 67\% chance of winning
 nothing.}
\end{itemize}


 The naive preference pattern on the Allais Paradox is 1A
{\textgreater} 1B and 2B {\textgreater} 2A. Then you will pay me to
throw a switch from A to B because you'd rather have a
33\% chance of winning \$27,000 than a 34\% chance of winning \$24,000.
Then a die roll eliminates a chunk of the probability mass. In both
cases you had \textit{at least} a 66\% chance of winning nothing. This
die roll eliminates that 66\%. So now option B is a 33/34 chance of
winning \$27,000, but option A is a \textit{certainty} of winning
\$24,000. Oh, glorious certainty! So you pay me to throw the switch
back from B to A.


 Now, if I've told you in advance that
I'm going to do all that, do you really want to pay me
to throw the switch, and then pay me to throw it back? Or would you
prefer to reconsider?


 Whenever you try to price a probability shift from 24\% to 23\% as
being less important than a shift from \~{}1 to 99\%---every time you
try to make an increment of probability have more value when
it's near an end of the scale---you open yourself up to
this kind of exploitation. I can always set up a chain of events that
eliminates the probability mass, a bit at a time, until
you're left with
``certainty'' that flips your
preferences. One time's certainty is another
time's uncertainty, and if you insist on treating the
distance from \~{}1 to 0.99 as special, I can cause you to invert your
preferences over time and pump some money out of you.


 Can I persuade you, perhaps, that this is an irrational pattern?


 Surely, if you've been reading this book for a
while, you realize that \textit{you}{}---the very system and process
that reads these very words---are a flawed piece of machinery. Your
intuitions are not giving you direct, veridical information about good
choices. If you don't believe that, there are some
gambling games I'd like to play with you.


 There are various other games you can also play with certainty
effects. For example, if you offer someone a certainty of \$400, or an
80\% probability of \$500 and a 20\% probability of \$300,
they'll usually take the \$400. But if you ask people
to imagine themselves \$500 richer, and ask if they would prefer a
certain loss of \$100 or a 20\% chance of losing \$200,
they'll usually take the chance of losing
\$200.\footnote{Kahneman and Tversky, ``Prospect Theory: An
Analysis of Decision Under Risk.''\comment{4}} Same probability distribution over outcomes,
different descriptions, different choices.


 Yes, Virginia, you really \textit{should} try to multiply the
utility of outcomes by their probability. You really should.
Don't be embarrassed to use clean math.


 In the Allais paradox, figure out whether 1 unit of the difference
between getting \$24,000 and getting nothing outweighs 33 units of the
difference between getting \$24,000 and \$27,000. If it does, prefer 1A
to 1B and 2A to 2B. If the 33 units outweigh the 1 unit, prefer 1B to
1A and 2B to 2A. As for calculating the utility of money, I would
suggest using an approximation that assumes money is logarithmic in
utility. If you've got plenty of money already, pick B.
If \$24,000 would double your existing assets, pick A. Case 2 or case
1, makes no difference. Oh, and be sure to assess the utility of total
asset values---the utility of final outcome states of the world---not
\textit{changes in} assets, or you'll end up
inconsistent again.


 A number of commenters claimed that the preference pattern
wasn't irrational because of ``the
utility of certainty,'' or something like that. One
commenter even wrote U(Certainty) into an expected utility equation.


 Does anyone remember that whole business about \textit{expected
utility} and \textit{utility} being of fundamentally different types?
Utilities are over \textit{outcomes.} They are values you attach to
\textit{particular, solid states of the world.} You cannot feed a
probability of 1 into a utility function. It makes no sense.


 And before you sniff, ``Hmph\,\ldots you just want
the math to be neat and tidy,'' remember that, in
this case, the price of departing the Bayesian Way was paying someone
to throw a switch and then throw it back.


 But what about that solid, warm feeling of reassurance?
Isn't \textit{that} a utility?

{
 That's being human. Humans are not expected
utility maximizers. Whether you want to relax and have fun, or pay some
extra money for a feeling of certainty, depends on whether you care
more about satisfying your intuitions or \textit{actually achieving the
goal.}}


 If you're gambling at Las Vegas for fun, then by
all means, don't think about the expected
utility---you're going to lose money anyway.


 But what if it were 24,000 lives at stake, instead of \$24,000?
The certainty effect is even stronger over human lives. Will you pay
one human life to throw the switch, and another to switch it back?


 Tolerating preference reversals makes a mockery of claims to
optimization. If you drive from San Jose to San Francisco to Oakland to
San Jose, over and over again, then you may get a lot of warm fuzzy
feelings out of it, but you can't be interpreted as
having a \textit{destination}{}---as trying to \textit{go somewhere}.


 When you have circular preferences, you're not
\textit{steering the future}{}---just running in circles. If you enjoy
running for its own sake, then fine. But if you have a goal---something
you're trying to actually accomplish---a preference
reversal reveals a big problem. At least one of the choices
you're making must not be working to actually optimize
the future in any coherent sense.


 If what you care about is the warm fuzzy feeling of certainty,
then fine. If someone's life is at stake, then you had
best realize that your intuitions are a greasy lens through which to
see the world. Your feelings are not providing you with direct,
veridical information about strategic consequences---it \textit{feels}
that way, but they're \textit{not.} Warm fuzzies can
lead you far astray.


 There are mathematical laws governing efficient strategies for
steering the future. When something \textit{truly} important is at
stake---something more important than your feelings of happiness about
the decision---then you should care about the math, if you truly care
at all.

\myendsectiontext


\bigskip

\mysection{Feeling Moral}


 Suppose that a disease, or a monster, or a war, or something, is
killing people. And suppose you only have enough resources to implement
one of the following two options:


 Save 400 lives, with certainty.


 Save 500 lives, with 90\% probability; save no lives, 10\%
probability.


 Most people choose option 1. Which, I think, is foolish; because
if you multiply 500 lives by 90\% probability, you get an expected
value of 450 lives, which exceeds the 400-life value of option 1.
(Lives saved don't diminish in marginal utility, so
this is an appropriate calculation.)


 ``What!'' you cry, incensed.
``How can you gamble with human lives? How can you
think about numbers when so much is at stake? What if that 10\%
probability strikes, and everyone dies? So much for your damned logic!
You're following your rationality off a
cliff!''


 Ah, but here's the interesting thing. If you
present the options this way:

\begin{enumerate}
\item {
 100 people die, with certainty.}

\item {
  90\% chance no one dies; 10\% chance 500 people die.}
\end{enumerate}


 Then a majority choose option 2. \textit{Even though
it's the same gamble.} You see, just as a
\textit{certainty} of saving 400 lives seems to \textit{feel} so much
more comfortable than an unsure gain, so too, a certain loss
\textit{feels} worse than an uncertain one.


 You can grandstand on the second description too:
``How can you condemn 100 people to certain death when
there's such a good chance you can save them?
We'll all share the risk! Even if it was only a 75\%
chance of saving everyone, it would still be worth it---so long as
there's a chance---everyone makes it, or no one
does!''


 You know what? This isn't about your feelings. A
human life, with all its joys and all its pains, adding up over the
course of decades, is worth far more than your brain's
feelings of comfort or discomfort with a plan. Does computing the
expected utility feel too cold-blooded for your taste? Well, that
feeling isn't even a feather in the scales, when a life
is at stake. Just shut up and multiply.


 A googol is $10^{100}$---a 1 followed by one
hundred zeroes. A googolplex is an even more incomprehensibly large
number---it's $10^{googol}$, a 1
\textit{followed by a googol zeroes}. Now pick some trivial
inconvenience, like a hiccup, and some decidedly untrivial misfortune,
like getting slowly torn limb from limb by sadistic mutant sharks. If
we're forced into a choice between either preventing a
googolplex people's hiccups, or preventing a single
person's shark attack, which choice should we make? If
you assign \textit{any} negative value to hiccups, then, on pain of
decision-theoretic incoherence, there must be some number of hiccups
that would add up to rival the negative value of a shark attack. For
any particular finite evil, there must be some number of hiccups that
would be even worse.


 Moral dilemmas like these aren't conceptual blood
sports for keeping analytic philosophers entertained at dinner parties.
They're distilled versions of the kinds of situations
we actually find ourselves in every day. Should I spend \$50 on a
console game, or give it all to charity? Should I organize a \$700,000
fundraiser to pay for a single bone marrow transplant, or should I use
that same money on mosquito nets and prevent the malaria deaths of some
200 children?


 Yet there are many who avert their gaze from the real
world's abundance of unpleasant moral tradeoffs---many,
too, who take pride in looking away. Research shows that people
distinguish ``sacred values,'' like
human lives, from ``unsacred
values,'' like money. When you try to trade off a
sacred value against an unsacred value, subjects express great
indignation. (Sometimes they want to punish the person who made the
suggestion.)


 My favorite anecdote along these lines comes from a team of
researchers who evaluated the effectiveness of a certain project,
calculating the cost per life saved, and recommended to the government
that the project be implemented because it was cost-effective. The
governmental agency rejected the report because, they said, you
couldn't put a dollar value on human life. After
rejecting the report, the agency decided \textit{not} to implement the
measure.


 Trading off a sacred value against an unsacred value \textit{feels
really awful}. To merely multiply utilities would be too
cold-blooded---it would be following rationality off a cliff\,\ldots


 But altruism isn't the warm fuzzy feeling you get
from being altruistic. If you're doing it for the
spiritual benefit, that is nothing but selfishness. The primary thing
is to help others, whatever the means. So shut up and multiply!


 And if it seems to you that there is a fierceness to this
maximization, like the bare sword of the law, or the burning of the
Sun---if it seems to you that at the center of this rationality there
is a small cold flame---


 Well, the other way might feel better inside you. But it
wouldn't work.


 And I say also this to you: That if you set aside your regret for
all the spiritual satisfaction you could be having---if you
\textit{wholeheartedly} pursue the Way, without thinking that you are
being cheated---if you give yourself over to rationality without
holding back, you will find that rationality gives to you in return.


 But \textit{that} part only works if you don't go
around saying to yourself, ``It would feel better
inside me if only I could be less rational.'' Should
you be sad that you have the opportunity to actually help people? You
cannot attain your full potential if you regard your gift as a burden.

\myendsectiontext

\mysection{The ``Intuitions'' Behind ``Utilitarianism''}


 I used to be very confused about metaethics. After my confusion
finally cleared up, I did a postmortem on my previous thoughts. I found
that my object-level moral reasoning had been valuable and my
meta-level moral reasoning had been worse than useless. And this
appears to be a general syndrome---people do much better when
discussing whether torture is good or bad than when they discuss the
meaning of ``good'' and
``bad.'' Thus, I deem it prudent to
keep moral discussions on the object level wherever I possibly can.


 Occasionally people object to any discussion of morality on the
grounds that morality doesn't exist, and in lieu of
explaining that ``exist'' is not the
right term to use here, I generally say, ``But what do
you do anyway?'' and take the discussion back down to
the object level.


 Paul Gowder, though, has pointed out that both the idea of
choosing a googolplex trivial inconveniences over one atrocity, and the
idea of ``utilitarianism,'' depend
on ``intuition.'' He says
I've argued that the two are not compatible, but
charges me with failing to argue for the utilitarian intuitions that I
appeal to.


 Now ``intuition'' is not how I
would describe the computations that underlie human morality and
distinguish us, as moralists, from an ideal philosopher of perfect
emptiness and/or a rock. But I am okay with using the word
``intuition'' as a term of art,
bearing in mind that ``intuition''
in this sense is not to be contrasted to reason, but is, rather, the
cognitive building block out of which both long verbal arguments and
fast perceptual arguments are constructed.


 I see the project of morality as a project of renormalizing
intuition. We have intuitions about things that seem desirable or
undesirable, intuitions about actions that are right or wrong,
intuitions about how to resolve conflicting intuitions, intuitions
about how to systematize specific intuitions into general principles.


 Delete all the intuitions, and you aren't left
with an ideal philosopher of perfect emptiness; you're
left with a rock.


 Keep all your specific intuitions and refuse to build upon the
reflective ones, and you aren't left with an ideal
philosopher of perfect spontaneity and genuineness;
you're left with a grunting caveperson running in
circles, due to cyclical preferences and similar inconsistencies.


 ``Intuition,'' as a term of
art, is not a curse word when it comes to morality---there is nothing
else to argue from. Even modus ponens is an
``intuition'' in this
sense---it's just that modus ponens still seems like a
good idea after being formalized, reflected on, extrapolated out to see
if it has sensible consequences, et cetera.


 So that is ``intuition.''


 However, Gowder did not say what he meant by
``utilitarianism.'' Does
utilitarianism say\,\ldots

\begin{enumerate}
\item  That right actions are strictly determined by good consequences?
\item  That praiseworthy actions depend on justifiable expectations of
good consequences?
\item  That probabilities of consequences should normatively be
discounted by their probability, so that a 50\% probability of
something bad should weigh exactly half as much in our tradeoffs?
\item  That virtuous actions always correspond to maximizing expected
utility under some utility function?
\item  That two harmful events are worse than one?
\item  That two independent occurrences of a harm (not to the same
person, not interacting with each other) are exactly twice as bad as
one?
\item  That for any two harms $A$ and $B$, with $A$ much worse than $B$, there
exists some tiny probability such that gambling on this probability of
$A$ is preferable to a certainty of $B$?
\end{enumerate}


 If you say that I advocate something, or that my argument depends
on something, and that it is wrong, do please specify what this thingy
is. Anyway, I accept 3, 5, 6, and 7, but not 4; I am not sure about the
phrasing of 1; and 2 is true, I guess, but phrased in a rather
solipsistic and selfish fashion: you should not worry about being
praiseworthy.


 Now, what are the
``intuitions'' upon which my
``utilitarianism'' depends?


 This is a deepish sort of topic, but I'll take a
quick stab at it.


 First of all, it's not just that someone presented
me with a list of statements like those above, and I decided which ones
sounded ``intuitive.'' Among other
things, if you try to violate
``utilitarianism,'' you run into
paradoxes, contradictions, circular preferences, and other things that
aren't symptoms of moral wrongness so much as moral
incoherence.


 After you think about moral problems for a while, and also find
new truths about the world, and even discover disturbing facts about
how you yourself work, you often end up with different moral opinions
than when you started out. This does not quite \textit{define} moral
progress, but it is how we experience moral progress.


 As part of my experienced moral progress, I've
drawn a conceptual separation between questions of type \textit{Where
should we go?} and questions of type \textit{How should we get there?}
(Could that be what Gowder means by saying I'm
``utilitarian''?)


 The question of where a road goes---where it leads---you can
answer by traveling the road and finding out. If you have a false
belief about where the road leads, this falsity can be destroyed by the
truth in a very direct and straightforward manner.


 When it comes to wanting to go to a particular place, this want is
not entirely immune from the destructive powers of truth. You could go
there and find that you regret it afterward (which does not define
moral error, but is how we experience moral error).


 But, even so, wanting to be in a particular place seems worth
distinguishing from wanting to take a particular road to a particular
place.


 Our intuitions about where to go are arguable enough, but our
intuitions about how to get there are frankly messed up. After the two
hundred and eighty-seventh research study showing that people will chop
their own feet off if you frame the problem the wrong way, you start to
distrust first impressions.


 When you've read enough research on scope
insensitivity---people will pay only 28\% more to protect all 57
wilderness areas in Ontario than one area, people will pay the same
amount to save 50,000 lives as 5,000 lives\,\ldots that sort of thing\,\ldots


 Well, the worst case of scope insensitivity I've
ever heard of was described here by Slovic:

\begin{quote}
{
 Other recent research shows similar results. Two Israeli
psychologists asked people to contribute to a costly life-saving
treatment. They could offer that contribution to a group of eight sick
children, or to an individual child selected from the group. The target
amount needed to save the child (or children) was the same in both
cases. Contributions to individual group members far outweighed the
contributions to the entire group.\footnote{Paul Slovic, ``Numbed by
Numbers,'' \textit{Foreign Policy} (March 2007),
\url{http://foreignpolicy.com/2007/03/13/numbed-by-numbers/}.\comment{1}}}
\end{quote}


 There's other research along similar lines, but
I'm just presenting one example,
'cause, y'know, eight examples would
probably have less impact.


 If you know the general experimental paradigm, then the reason for
the above behavior is pretty obvious---focusing your attention on a
single child creates more emotional arousal than trying to distribute
attention around eight children simultaneously. So people are willing
to pay more to help one child than to help eight.


 Now, you could look at this intuition, and think it was revealing
some kind of incredibly deep moral truth which shows that one
child's good fortune is somehow devalued by the other
children's good fortune.


 But what about the billions of other children in the world? Why
isn't it a bad idea to help this one child, when that
causes the value of all the other children to go down? How can it be
significantly better to have 1,329,342,410 happy children than
1,329,342,409, but then somewhat worse to have seven more at
1,329,342,417?


 \textit{Or} you could look at that and say: ``The
intuition is wrong: the brain can't successfully
multiply by eight and get a larger quantity than it started with. But
it ought to, normatively speaking.''


 And once you realize that the brain can't multiply
by eight, then the other cases of scope neglect stop seeming to reveal
some fundamental truth about 50,000 lives being worth just the same
effort as 5,000 lives, or whatever. You don't get the
impression you're looking at the revelation of a deep
moral truth about nonagglomerative utilities. It's just
that the brain doesn't goddamn multiply. Quantities get
thrown out the window.


 If you have \$100 to spend, and you spend \$20 each on each of 5
efforts to save 5,000 lives, you will do worse than if you spend \$100
on a single effort to save 50,000 lives. Likewise if such choices are
made by 10 different people, rather than the same person. As soon as
you start believing that it is better to save 50,000 lives than 25,000
lives, that simple preference of final destinations has implications
for the choice of paths, when you consider five different events that
save 5,000 lives.


 (It is a general principle that Bayesians see no difference
between the long-run answer and the short-run answer; you never get two
different answers from computing the same question two different ways.
But the long run is a helpful intuition pump, so I am talking about it
anyway.)


 The aggregative valuation strategy of ``shut up
and multiply'' arises from the simple preference to
have more of something---to save as many lives as possible---when you
have to describe general principles for choosing more than once, acting
more than once, planning at more than one time.


 Aggregation also arises from claiming that the local choice to
save one life doesn't depend on how many lives already
exist, far away on the other side of the planet, or far away on the
other side of the universe. Three lives are one and one and one. No
matter how many billions are doing better, or doing worse. 3 = 1 + 1 +
1, no matter what other quantities you add to both sides of the
equation. And if you add another life you get 4 = 1 + 1 + 1 + 1.
That's aggregation.


 When you've read enough heuristics and biases
research, and enough coherence and uniqueness proofs for Bayesian
probabilities and expected utility, and you've seen the
``Dutch book'' and
``money pump'' effects that penalize
trying to handle uncertain outcomes any other way, then you
don't see the preference reversals in the Allais
Paradox as revealing some incredibly deep moral truth about the
intrinsic value of certainty. It just goes to show that the brain
doesn't goddamn multiply.


 The primitive, perceptual intuitions that make a choice
``feel good'' don't
handle probabilistic pathways through time very skillfully, especially
when the probabilities have been expressed symbolically rather than
experienced as a frequency. So you reflect, devise more trustworthy
logics, and think it through in words.


 When you see people insisting that no amount of money whatsoever
is worth a single human life, and then driving an extra mile to save
\$10; or when you see people insisting that no amount of money is worth
a decrement of health, and then choosing the cheapest health insurance
available; then you don't think that their
protestations reveal some deep truth about incommensurable utilities.


 Part of it, clearly, is that primitive intuitions
don't successfully diminish the emotional impact of
symbols standing for small quantities---anything you talk about seems
like ``an amount worth
considering.''


 And part of it has to do with preferring unconditional social
rules to conditional social rules. Conditional rules seem weaker, seem
more subject to manipulation. If there's any loophole
that lets the government legally commit torture, then the government
will drive a truck through that loophole.


 So it seems like there should be an unconditional social
injunction against preferring money to life, and no
``but'' following it. Not even
``but a thousand dollars isn't worth a
0.0000000001\% probability of saving a life.'' Though
the latter choice, of course, is revealed every time we sneeze without
calling a doctor.


 The rhetoric of sacredness gets bonus points for seeming to
express an \textit{unlimited} commitment, an \textit{unconditional}
refusal that signals trustworthiness and refusal to compromise. So you
conclude that moral rhetoric espouses qualitative distinctions, because
espousing a quantitative tradeoff would sound like you were plotting to
defect.


 On such occasions, people vigorously want to throw quantities out
the window, and they get upset if you try to bring quantities back in,
because quantities sound like conditions that would weaken the rule.


 But you don't conclude that there are
\textit{actually} two tiers of utility with lexical ordering. You
don't conclude that there is actually an infinitely
sharp moral gradient, some atom that moves a Planck distance (in our
continuous physical universe) and sends a utility from zero to
infinity. You don't conclude that utilities must be
expressed using hyper-real numbers. Because the lower tier would simply
vanish in any equation. It would never be worth the tiniest effort to
recalculate for it. All decisions would be determined by the upper
tier, and all thought spent thinking about the upper tier only, if the
upper tier genuinely had lexical priority.


 As Peter Norvig once pointed out, if Asimov's
robots had \textit{strict} priority for the First Law of Robotics
(``A robot shall not harm a human being, nor through
inaction allow a human being to come to harm'') then
no robot's behavior would ever show any sign of the
other two Laws; there would always be some tiny First Law factor that
would be sufficient to determine the decision.


 Whatever value is worth thinking about at all must be worth
trading off against all other values worth thinking about, because
thought itself is a limited resource that must be traded off. When you
reveal a value, you reveal a utility.


 I don't say that morality should always be simple.
I've already said that the meaning of music is more
than happiness alone, more than just a pleasure center lighting up. I
would rather see music composed by people than by nonsentient machine
learning algorithms, so that someone should have the joy of
composition; I care about the journey, as well as the destination. And
I am ready to hear if you tell me that the value of music is deeper,
and involves more complications, than I realize---that the valuation of
this one event is more complex than I know.


 But that's for \textit{one event}. When it comes
to multiplying by quantities and probabilities, complication is to be
avoided---at least if you care more about the destination than the
journey. When you've reflected on enough intuitions,
and corrected enough absurdities, you start to see a common
denominator, a meta-principle at work, which one might phrase as
``Shut up and multiply.''


 Where music is concerned, I care about the journey.


 When lives are at stake, I shut up and multiply.


 It is more important that lives be saved, than that we conform to
any particular ritual in saving them. And the optimal path to that
destination is governed by laws that are simple, because they are
math.


 And that's why I'm a
utilitarian---at least when I am doing something that is overwhelmingly
more important than my own feelings about it---which is most of the
time, because there are not many utilitarians, and many things left
undone.

\myendsectiontext


\bigskip

\mysection{Ends Don't Justify Means (Among Humans)}

\begin{quote}

 If the ends don't justify the means, what does?

{\raggedleft
 {}---variously attributed
\par}




 I think of myself as running on hostile hardware.

{\raggedleft
 {}---Justin Corwin
\par}
\end{quote}




 Humans may have evolved a structure of political revolution,
beginning by believing themselves morally superior to the corrupt
current power structure, but ending by being corrupted by power
themselves{}---not by any plan in their own minds, but by the echo of
ancestors who did the same and thereby reproduced.


 This fits the template:

\begin{quote}
{
 In some cases, human beings have evolved in such fashion as to
think that they are doing $X$ for prosocial reason $Y$, but when human
beings actually do $X$, other adaptations execute to promote
self-benefiting consequence $Z$.}
\end{quote}


 From this proposition, I now move on to a question
\textit{considerably} outside the realm of classical Bayesian decision
theory:

\begin{quote}
{
  What if I'm running on corrupted hardware?}
\end{quote}


 In such a case as this, you might even find yourself uttering such
seemingly paradoxical statements---sheer nonsense from the perspective
of classical decision theory---as:

\begin{quote}
{
  The ends don't justify the means.}
\end{quote}


 But if you are running on corrupted hardware, then the reflective
observation that it \textit{seems} like a righteous and altruistic act
to seize power for yourself---this \textit{seeming} may not be much
evidence for the proposition that seizing power is in fact the action
that will most benefit the tribe.


 By the power of naive realism, the corrupted hardware that you run
on, and the corrupted seemings that it computes, will seem like the
fabric of the very world itself---simply the way-things-are.

{
 And so we have the bizarre-seeming rule: ``For
the good of the tribe, do not cheat to seize power \textit{even when it
would provide a net benefit to the tribe}.''}


 Indeed it may be wiser to phrase it this way. If you just say,
``when it \textit{seems} like it would provide a net
benefit to the tribe,'' then you get people who say,
``But it doesn't just \textit{seem}
that way---it \textit{would} provide a net benefit to the tribe if I
were in charge.''


 The notion of untrusted hardware seems like something wholly
outside the realm of classical decision theory. (What it does to
reflective decision theory I can't yet say, but that
would seem to be the appropriate level to handle it.)


 But on a human level, the patch seems straightforward. Once you
know about the warp, you create rules that describe the warped behavior
and outlaw it. A rule that says, ``For the good of the
tribe, do not cheat to seize power even for the good of the
tribe.'' Or ``For the good of the
tribe, do not murder even for the good of the
tribe.''


 And now the philosopher comes and presents their
``thought experiment''---setting up
a scenario in which, \textit{by stipulation,} the \textit{only}
possible way to save five innocent lives is to murder one innocent
person, and this murder is \textit{certain} to save the five lives.
``There's a train heading to run over
five innocent people, who you can't possibly warn to
jump out of the way, but you can push one innocent person into the path
of the train, which will stop the train. These are your only options;
what do you do?''


 An altruistic human, who has accepted certain deontological
prohibitions---which seem well justified by some historical statistics
on the results of reasoning in certain ways on untrustworthy
hardware---may experience some mental distress, on encountering this
thought experiment.


 So here's a reply to that
philosopher's scenario, which I have yet to hear any
philosopher's victim give:


 ``You stipulate that the \textit{only possible}
way to save five innocent lives is to murder one innocent person, and
this murder will \textit{definitely} save the five lives, and that
these facts are \textit{known} to me with effective certainty. But
since I am running on corrupted hardware, I can't
occupy the \textit{epistemic state} you want me to imagine. Therefore I
reply that, in a society of Artificial Intelligences worthy of
personhood and lacking any inbuilt tendency to be corrupted by power,
it would be right for the AI to murder the one innocent person to save
five, and moreover all its peers would agree. However, I refuse to
extend this reply to myself, because the epistemic state you ask me to
imagine can only exist among other kinds of people than human
beings.''


 Now, to me this seems like a dodge. I think the universe is
sufficiently unkind that we can justly be forced to consider situations
of this sort. The sort of person who goes around proposing that sort of
thought experiment might well deserve that sort of answer. But any
human legal system does embody some answer to the question
``How many innocent people can we put in jail to get
the guilty ones?,'' even if the number
isn't written down.


 As a human, I try to abide by the deontological prohibitions that
humans have made to live in peace with one another. But I
don't think that our deontological prohibitions are
\textit{literally inherently nonconsequentially terminally right}. I
endorse ``the end doesn't justify the
means'' as a principle to guide humans running on
corrupted hardware, but I wouldn't endorse it as a
principle for a society of AIs that make well-calibrated estimates. (If
you have one AI in a society of humans, that does bring in other
considerations, like whether the humans learn from your example.)


 And so I wouldn't say that a well-designed
Friendly AI must necessarily refuse to push that one person off the
ledge to stop the train. Obviously, I would expect any decent
superintelligence to come up with a superior third alternative. But if
those are the only two alternatives, and the FAI judges that it is
wiser to push the one person off the ledge---even after taking into
account knock-on effects on any humans who see it happen and spread the
story, etc.---then I don't call it an alarm light, if
an \textit{AI} says that the right thing to do is sacrifice one to save
five. Again, \textit{I} don't go around pushing people
into the paths of trains myself, nor stealing from banks to fund my
altruistic projects. \textit{I} happen to be a human. But for a
Friendly AI to be corrupted by power would be like it starting to bleed
red blood. The tendency to be corrupted by power is a specific
biological adaptation, supported by specific cognitive circuits, built
into us by our genes for a clear evolutionary reason. It
wouldn't spontaneously appear in the code of a Friendly
AI any more than its transistors would start to bleed.


 I would even go further, and say that if you had minds with an
inbuilt warp that made them \textit{overestimate} the external harm of
self-benefiting actions, then they would need a rule
``the ends do not prohibit the
means''---that you should do what benefits yourself
even when it (seems to) harm the tribe. By hypothesis, if their society
did not have this rule, the minds in it would refuse to breathe for
fear of using someone else's oxygen, and
they'd all die. For them, an occasional overshoot in
which one person seizes a personal benefit at the net expense of
society would seem just as cautiously virtuous---and indeed \textit{be}
just as cautiously virtuous---as when one of us humans, being cautious,
passes up an opportunity to steal a loaf of bread that really would
have been more of a benefit to them than a loss to the merchant
(including knock-on effects).


 ``The end does not justify the
means'' is just consequentialist reasoning at one
meta-level up. If a human starts thinking on the \textit{object} level
that the end justifies the means, this has awful consequences given our
untrustworthy brains; therefore a human shouldn't think
this way. But it is all still ultimately consequentialism.
It's just \textit{reflective} consequentialism, for
beings who know that their moment-by-moment decisions are made by
untrusted hardware.

\myendsectiontext

\mysection{Ethical Injunctions}

\begin{quote}

 Would you kill babies if it was the right thing to do? If no,
under what circumstances would you not do the right thing to do? If
yes, how right would it have to be, for how many babies?

{\raggedleft
 {}---horrible job interview question\footnote{\url{http://lesswrong.com/lw/rr/the_moral_void/}}
\par}
\end{quote}



 Swapping hats for a moment, I'm
\textit{professionally} intrigued by the decision theory of
``things you shouldn't do even if they
seem to be the right thing to do.''


 Suppose we have a reflective AI, self-modifying and
self-improving, at an intermediate stage in the development process. In
particular, the AI's goal system isn't
finished---the shape of its motivations is still being loaded, learned,
tested, or tweaked.


 Yea, I have seen many ways to screw up an AI goal system design,
resulting in a decision system that decides, given its goals, that the
universe ought to be tiled with tiny molecular smiley-faces, or some
such. Generally, these deadly suggestions also have the property that
the AI will not desire its programmers to fix it. If the AI is
\textit{sufficiently} advanced---which it may be even at an
intermediate stage---then the AI may also realize that deceiving the
programmers, hiding the changes in its thoughts, will help transform
the universe into smiley-faces.


 Now, from our perspective as programmers, if we \textit{condition
on the fact} that the AI has decided to hide its thoughts from the
programmers, or otherwise act willfully to deceive us, then it would
seem likely that some kind of unintended consequence has occurred in
the goal system. We would consider it probable that the AI is
\textit{not} functioning as intended, but rather likely that we have
messed up the AI's utility function somehow. So that
the AI wants to turn the universe into tiny reward-system counters, or
some such, and now has a motive to hide from us.


 Well, suppose we're \textit{not} going to
implement some object-level Great Idea as the AI's
utility function. Instead we're going to do something
advanced and recursive---build a goal system which knows (and cares)
about the programmers outside. A goal system that, via some nontrivial
internal structure, ``knows it's being
programmed'' and ``knows
it's incomplete.'' Then you might be
able to have and keep the rule:

\begin{quote}
{
 If [I decide that] fooling my programmers is the right thing to
do, execute a controlled shutdown [instead of doing the right thing to
  do].}
\end{quote}


 And the AI would keep this rule, even through the self-modifying
AI's revisions of its own code, because, in its
structurally nontrivial goal system, the present-AI understands that
this decision by a future-AI \textit{probably} indicates something
defined-as-a-malfunction. Moreover, the present-AI knows that if
future-AI tries to \textit{evaluate} the utility of executing a
shutdown, once this hypothetical malfunction has occurred, the
future-AI will probably \textit{decide} not to shut itself down. So the
shutdown should happen unconditionally, automatically, without the goal
system getting another chance to recalculate the right thing to do.


 I'm not going to go into the deep dark depths of
the exact mathematical structure, because that would be beyond the
scope of this book. Also I don't yet know the deep dark
depths of the mathematical structure. It looks like it \textit{should}
be possible, if you do things that are advanced and recursive and have
nontrivial (but consistent) structure. But I haven't
reached that level, as yet, so for now it's only a
dream.


 But the topic here is not advanced AI; it's human
ethics. I introduce the AI scenario to bring out more starkly the
strange idea of an \textit{ethical injunction}:

\begin{quote}
{
 You should never, ever murder an innocent person
who's helped you, \textit{even if it's
the right thing to do}; because it's far more likely
that \textit{you've made a mistake}, than that
murdering an innocent person who helped you is the right thing to do.}
\end{quote}


 Sound reasonable?


 During World War II, it became necessary to destroy
Germany's supply of deuterium, a neutron moderator, in
order to block their attempts to achieve a fission chain reaction.
Their supply of deuterium was coming at this point from a captured
facility in Norway. A shipment of heavy water was on board a Norwegian
ferry ship, the SF \textit{Hydro}. Knut Haukelid and three others had
slipped on board the ferry in order to sabotage it, when the saboteurs
were discovered by the ferry watchman. Haukelid told him that they were
escaping the Gestapo, and the watchman immediately agreed to overlook
their presence. Haukelid ``considered warning their
benefactor but decided that might endanger the mission and only thanked
him and shook his hand.''\footnote{Richard Rhodes, \textit{The Making of the Atomic Bomb} (New
York: Simon \& Schuster, 1986).\comment{1}} So the
civilian ferry \textit{Hydro} sank in the deepest part of the lake,
with eighteen dead and twenty-nine survivors. Some of the Norwegian
rescuers felt that the German soldiers present should be left to drown,
but this attitude did not prevail, and four Germans were rescued. And
that was, effectively, the end of the Nazi atomic weapons program.


 Good move? Bad move? Germany \textit{very likely}
wouldn't have gotten the Bomb anyway\,\ldots I hope with
absolute desperation that I never get faced by a choice like that, but
in the end, I can't say a word against it.


 On the other hand, when it comes to the rule:

\begin{quote}
{
 Never try to deceive yourself, or offer a reason to believe other
than probable truth; because even if you come up with an amazing clever
reason, it's more likely that you've
made a mistake than that you have a reasonable expectation of this
being a net benefit in the long run.}
\end{quote}


 Then I really \textit{don't} know of anyone
who's knowingly been faced with an exception. There are
times when you try to convince yourself
``I'm not hiding any Jews in my
basement'' before you talk to the Gestapo officer.
But then you do still know the truth, you're just
trying to create something like an alternative self that exists in your
imagination, a facade to talk to the Gestapo officer.


 But to really believe something that isn't true? I
don't know if there was ever anyone for whom that was
\textit{knowably} a good idea. I'm sure that there have
been many many times in human history, where person $X$ was better off
with false belief $Y$. And by the same token, there is always some set of
winning lottery numbers in every drawing. It's
\textit{knowing which lottery ticket will win} that is the
epistemically difficult part, like $X$ knowing when
they're better off with a false belief.


 Self-deceptions are the worst kind of black swan bets, much worse
than lies, because without knowing the true state of affairs, you
can't even guess at what the penalty will be for your
self-deception. They only have to blow up once to undo all the good
they ever did. One single time when you pray to God after discovering a
lump, instead of going to a doctor. That's all it takes
to undo a life. All the happiness that the warm thought of an afterlife
ever produced in humanity, has now been more than cancelled by the
failure of humanity to institute systematic cryonic preservations after
liquid nitrogen became cheap to manufacture. And I
don't think that anyone ever had that sort of failure
in mind as a possible blowup, when they said, ``But we
need religious beliefs to cushion the fear of
death.'' That's what black swan bets
are all about---the unexpected blowup.


 Maybe you even get away with one or two black swan bets---they
don't get you \textit{every} time. So you do it again,
and then the blowup comes and cancels out every benefit and then some.
That's what black swan bets are all about.


 Thus the difficulty of knowing when it's safe to
believe a lie (assuming you can even manage that much mental contortion
in the first place)---part of the nature of black swan bets is that you
don't see the bullet that kills you; and since our
perceptions just seem like the way the world is, it looks like there is
no bullet, period.


 So I would say that there is an ethical injunction against
self-deception. I call this an ``ethical
injunction'' not so much because it's
a matter of interpersonal morality (although it is), but because
it's a rule that guards you from your own
cleverness---an override against the temptation to do what seems like
the right thing.


 So now we have two kinds of situation that can support an
``ethical injunction,'' a rule not
to do something even when it's the right thing to do.
(That is, you refrain ``even when your brain has
computed it's the right thing to
do,'' but this will just \textit{seem like}
``the right thing to do.'')


First, being human and running on corrupted hardware, we may
generalize classes of situation where when you say e.g.~``It's
time to rob a few banks for the
greater good,'' we deem it more likely that
you've been corrupted than that this is really the
case. (Note that we're not prohibiting it from
\textit{ever} being the case in \textit{reality}, but
we're questioning the \textit{epistemic} state where
you're \textit{justified in trusting} your own
calculation that this is the right thing to do---fair lottery tickets
can win, but you can't justifiably buy them.)


 Second, history may teach us that certain classes of action are
black swan bets, that is, they sometimes blow up bigtime for reasons
not in the decider's model. So even when we calculate
within the model that something seems like the right thing to do, we
apply the further knowledge of the black swan problem to arrive at an
injunction against it.


 But surely\,\ldots if one is \textit{aware of these reasons}\,\ldots
then one can simply redo the calculation, taking them into account. So
we can rob banks if it seems like the right thing to do \textit{after
taking into account} the problem of corrupted hardware and black swan
blowups. That's the rational course, right?


 There's a number of replies I could give to that.


 I'll start by saying that this is a prime example
of the sort of thinking I have in mind, when I warn aspiring
rationalists to beware of cleverness.


 I'll also note that I wouldn't
want an attempted Friendly AI that had just decided that the Earth
ought to be transformed into paperclips, to assess whether this was a
reasonable thing to do in light of all the various warnings it had
received against it. I would want it to undergo an automatic controlled
shutdown. Who says that meta-reasoning is immune from corruption?


 I could mention the important times that my naive, idealistic
ethical inhibitions have protected me from \textit{myself}, and placed
me in a recoverable position, or helped start the recovery, from very
deep mistakes I had no clue I was making. And I could ask whether
I've really advanced so much, and whether it would
really be all that wise, to remove the protections that saved me
before.


 Yet even so\,\ldots ``Am I still dumber than my
ethics?'' is a question whose answer
isn't \textit{automatically}
``Yes.''


 There are obvious silly things here that you
shouldn't do; for example, you
shouldn't wait until you're really
tempted, and \textit{then} try to figure out if you're
smarter than your ethics on that particular occasion.


 But in general---there's only so much power that
can vest in what your parents told you not to do. One
shouldn't underestimate the power. Smart people debated
historical lessons in the course of forging the Enlightenment ethics
that much of Western culture draws upon; and some subcultures, like
scientific academia, or science-fiction fandom, draw on those ethics
more directly. But even so the power of the past is bounded.


 And in fact\,\ldots


 I've had to make my ethics \textit{much stricter}
than what my parents and Jerry Pournelle and Richard Feynman told me
not to do.


 Funny thing, how when people seem to think they're
smarter than their ethics, they argue for \textit{less} strictness
rather than \textit{more} strictness. I mean, when you think about how
much more complicated the modern world is\,\ldots


 And along the same lines, the ones who come to me and say,
``You should lie about the intelligence explosion,
because that way you can get more people to support you;
it's the rational thing to do, for the greater
good''---these ones seem to have \textit{no idea} of
the risks.


 They don't mention the problem of running on
corrupted hardware. They don't mention the idea that
lies have to be recursively protected from all the truths and all the
truthfinding techniques that threaten them. They don't
mention that honest ways have a simplicity that dishonest ways often
lack. They don't talk about black swan bets. They
don't talk about the terrible nakedness of discarding
the last defense you have against yourself, and trying to survive on
raw calculation.

{
 I am reasonably sure that this is because they have \textit{no
clue} about any of these things.}


 If you've truly understood the reason and the
rhythm behind ethics, then one major sign is that, augmented by this
newfound knowledge, you \textit{don't do} those things
that previously seemed like ethical transgressions. Only now you know
why.


 Someone who just looks at one or two reasons behind ethics, and
says, ``Okay, I've understood that, so
now I'll take it into account consciously, and
therefore I have no more need of ethical
inhibitions''---this one is behaving more like a
stereotype than a real rationalist. The world isn't
simple and pure and clean, so you can't just take the
ethics you were raised with and trust them. But that pretense of Vulcan
logic, where you think you're just going to compute
everything correctly once you've got one or two
abstract insights---that doesn't work in real life
either.


 As for those who, having figured out \textit{none} of this, think
themselves smarter than their ethics: Ha.


 And as for those who previously thought themselves smarter than
their ethics, but who hadn't conceived of all these
elements behind ethical injunctions ``in so many
words'' until they ran across this essay, and who
\textit{now} think themselves smarter than their ethics, because
they're going to take all this into account from now
on: Double ha.


 I have seen many people struggling to excuse themselves from their
ethics. Always the modification is toward lenience, never to be more
strict. And I am stunned by the speed and the lightness with which they
strive to abandon their protections. Hobbes said, ``I
don't know what's worse, the fact that
everyone's got a price, or the fact that their price is
so low.'' So very low the price, so very eager they
are to be bought. They don't look twice and then a
third time for alternatives, before deciding that they have no option
left but to transgress---though they may look very grave and solemn
when they say it. They abandon their ethics at the very first
opportunity. ``Where there's a will to
failure, obstacles can be found.'' The will to fail
at ethics seems very strong, in some people.


 I don't know if I can endorse absolute ethical
injunctions that bind over all possible epistemic states of a human
brain. The universe isn't kind enough for me to trust
that. (Though an ethical injunction against self-deception, for
example, does seem to me to have tremendous force. I've
seen many people arguing for the Dark Side, and none of them seem aware
of the network risks or the black-swan risks of self-deception.) If,
someday, I attempt to shape a (reflectively consistent) injunction
within a self-modifying AI, it will only be after working out the math,
because that is so totally not the sort of thing you could get away
with doing via an ad-hoc patch.


 But I will say this much:


 \textit{I am completely unimpressed with the knowledge, the
reasoning, and the overall level of those folk who have eagerly come to
me, and said in grave tones,} ``It's
rational to do unethical thing $X$ because it will have benefit
$Y$.''

\myendsectiontext


\mysection{Something to Protect}


 In the gestalt of (ahem) Japanese fiction, one finds this
oft-repeated motif: Power comes from having something to protect. 


 I'm not just talking about superheroes that power
up when a friend is threatened, the way it works in Western fiction. In
the Japanese version it runs deeper than that.


 In the \textit{X} saga it's explicitly stated that
each of the good guys draw their power from having someone---one
person---who they want to protect. Who? That question is part of
\textit{X}'s plot---the ``most
precious person'' isn't always who we
think. But if that person is killed, or hurt in the wrong way, the
protector loses their power---not so much from magical backlash, as
from simple despair. This isn't something that happens
once per week per good guy, the way it would work in a Western comic.
It's equivalent to being Killed Off For Real{}---taken
off the game board.


 The way it works in Western superhero comics is that the good guy
gets bitten by a radioactive spider; and then he needs something to do
with his powers, to keep him busy, so he decides to fight crime. And
then Western superheroes are always whining about how much time their
superhero duties take up, and how they'd rather be
ordinary mortals so they could go fishing or something.


 Similarly, in Western real life, unhappy people are told that they
need a ``purpose in life,'' so they
should pick out an altruistic cause that goes well with their
personality, like picking out nice living-room drapes, and this will
brighten up their days by adding some color, like nice living-room
drapes. You should be careful not to pick something too expensive,
though.


 In Western comics, the magic comes first, then the purpose:
Acquire amazing powers, decide to protect the innocent. In Japanese
fiction, often, it works the other way around.


 Of course I'm not saying all this to generalize
from fictional evidence. But I want to convey a concept whose
deceptively close Western analogue is \textit{not} what I mean.


 I have touched before on the idea that a rationalist must have
something they value more than
``rationality'': \textit{the Art
must have a purpose other than itself, or it collapses into infinite
recursion.} But do not mistake me, and think I am advocating that
rationalists should pick out a nice altruistic cause, by way of having
something to do, because rationality isn't all that
important by itself. No. I am asking: Where do rationalists come from?
How do we acquire our powers?


 It is written in The Twelve Virtues of Rationality:

\begin{quote}
{
 How can you improve your conception of rationality? Not by saying
to yourself, ``It is my duty to be
rational.'' By this you only enshrine your mistaken
conception. Perhaps your conception of rationality is that it is
rational to believe the words of the Great Teacher, and the Great
Teacher says, ``The sky is green,''
and you look up at the sky and see blue. If you think:
``It may look like the sky is blue, but rationality is
to believe the words of the Great Teacher,'' you lose
a chance to discover your mistake.}
\end{quote}


 Historically speaking, the way humanity \textit{finally} left the
trap of authority and began paying attention to,
y'know, the actual sky, was that beliefs based on
experiment turned out to be \textit{much more useful} than beliefs
based on authority. Curiosity has been around since the dawn of
humanity, but the problem is that spinning campfire tales works just as
well for satisfying curiosity.


 Historically speaking, science won because it displayed greater
raw strength in the form of technology, not because science
\textit{sounded more reasonable}. To this very day, magic and scripture
still sound more reasonable to untrained ears than science. That is why
there is continuous social tension between the belief systems. If
science not only worked better than magic, but \textit{also} sounded
more intuitively reasonable, it would have won \textit{entirely} by
now.


 Now there are those who say: ``How dare you
suggest that anything should be valued more than Truth? Must not a
rationalist love Truth more than mere usefulness?''


 Forget for a moment what would have happened historically to
someone like that---that people in pretty much that frame of mind
defended the Bible because they loved Truth more than mere accuracy.
Propositional morality is a glorious thing, but it has too many degrees
of freedom.


 No, the real point is that a rationalist's love
affair with the Truth is, well, just \textit{more complicated} as an
emotional relationship.


 One doesn't become an adept rationalist without
caring about the truth, both as a purely moral desideratum and as
something that's fun to have. I doubt there are many
master composers who hate music.


 But part of what I \textit{like} about rationality is the
discipline imposed by requiring beliefs to yield predictions, which
ends up taking us much closer to the truth than if we sat in the living
room obsessing about Truth all day. I \textit{like} the complexity of
simultaneously having to love True-seeming ideas, and also being ready
to drop them out the window at a moment's notice. I
even like the glorious aesthetic purity of declaring that I value mere
usefulness above aesthetics. That is almost a contradiction, but not
quite; and that has an aesthetic quality as well, a delicious humor.


 And of course, no matter how much you profess your love of mere
usefulness, you should never \textit{actually} end up deliberately
believing a useful false statement.


 So don't oversimplify the relationship between
loving truth and loving usefulness. It's not one or the
other. It's \textit{complicated}, which is not
necessarily a defect in the moral aesthetics of single events.


 But morality and aesthetics alone, believing that one ought to be
``rational'' or that certain ways of
thinking are ``beautiful,'' will not
lead you to the center of the Way. It wouldn't have
gotten humanity out of the authority-hole.


 In Feeling Moral, I discussed this dilemma: Which of these options
would you prefer?

\begin{enumerate}
\item {
 Save 400 lives, with certainty.}

\item {
 Save 500 lives, 90\% probability; save no lives, 10\%
 probability.}
\end{enumerate}


 You may be tempted to grandstand, saying, ``How
dare you gamble with people's
lives?'' Even if you, yourself, are one of the
500---but you don't know which one---you may still be
tempted to rely on the comforting feeling of certainty, because our own
lives are often worth less to us than a good intuition.


 But if your precious daughter is one of the 500, and you
don't know which one, \textit{then}, perhaps, you may
feel more impelled to shut up and multiply---to notice that you have an
80\% chance of saving her in the first case, and a 90\% chance of
saving her in the second.


 And yes, everyone in that crowd is someone's son
or daughter. Which, in turn, suggests that we should pick the second
option as altruists, as well as concerned parents.


 My point is not to suggest that one person's life
is more valuable than 499 people. What I am trying to say is that
\textit{more} than your own life has to be at stake, before a person
becomes desperate enough to resort to math.


 What if you believe that it is
``rational'' to choose the certainty
of option 1? Lots of people think that
``rationality'' is about choosing
only methods that are certain to work, and rejecting all uncertainty.
But, hopefully, you care more about your daughter's
life than about ``rationality.''


 Will pride in your own virtue as a rationalist save you? Not if
you believe that it is virtuous to choose certainty. You will only be
able to learn something about rationality if your
daughter's life matters more to you than your pride as
a rationalist.


 You may even learn something about rationality from the
experience, if you are already far enough grown in your Art to say,
``I must have had the wrong conception of
rationality,'' and not, ``Look at
how rationality gave me the wrong answer!''


 (The essential difficulty in becoming a master rationalist is that
you need quite a bit of rationality to bootstrap the learning
process.)


 Is your belief that you ought to be rational more important than
your life? Because, as I've previously observed,
risking your life isn't comparatively all that scary.
Being the lone voice of dissent in the crowd and having everyone look
at you funny is \textit{much} scarier than a mere threat to your life,
according to the revealed preferences of teenagers who drink at parties
and then drive home. It will take something terribly important to make
you willing to leave the pack. A threat to your life
won't be enough.


 Is your will to rationality stronger than your \textit{pride}? Can
it be, if your will to rationality stems from your pride in your
self-image as a rationalist? It's
helpful---\textit{very} helpful---to have a self-image which says that
you are the sort of person who confronts harsh truth.
It's helpful to have too much self-respect to knowingly
lie to yourself or refuse to face evidence. But there may come a time
when you have to admit that you've been doing
rationality all wrong. Then your pride, your self-image as a
rationalist, may make that too hard to face.


 If you've prided yourself on believing what the
Great Teacher says---even when it seems harsh, even when
you'd rather not---that may make it all the more bitter
a pill to swallow, to admit that the Great Teacher is a fraud, and all
your noble self-sacrifice was for naught.


 Where do you get the will to keep moving forward?


 When I look back at my own personal journey toward
rationality---not just humanity's historical
journey---well, I grew up believing very strongly that I ought to be
rational. This made me an above-average Traditional Rationalist a la
Feynman and Heinlein, and nothing more. It did not drive me to go
beyond the teachings I had received. I only began to grow
\textit{further} as a rationalist once I had something terribly
important that I needed to do. Something more important than my pride
as a rationalist, never mind my life.


 Only when you become more wedded to success than to any of your
beloved techniques of rationality do you begin to appreciate these
words of Miyamoto Musashi:\footnote{Musashi, \textit{Book of Five Rings}.\comment{1}}

\begin{quote}

 You can win with a long weapon, and yet you can also win with a
short weapon. In short, the Way of the Ichi school is the spirit of
winning, whatever the weapon and whatever its size.

{\raggedleft
 {}---Miyamoto Musashi, \textit{The Book of Five Rings}
 \par}
\end{quote}



 Don't mistake this for a specific teaching of
rationality. It describes how you \textit{learn} the Way, beginning
with a desperate need to succeed. No one masters the Way until more
than their life is at stake. More than their comfort, more even than
their pride.


 You can't just pick out a Cause like that because
you feel you need a hobby. Go looking for a ``good
cause,'' and your mind will just fill in a standard
clich√©. Learn how to multiply, and perhaps you will recognize a
drastically important cause when you see one.


 But \textit{if} you have a cause like that, it is right and proper
to wield your rationality in its service.


 To strictly subordinate the aesthetics of rationality to a higher
cause is part of the aesthetic of rationality. You should pay attention
to that aesthetic: You will never master rationality well enough to win
with any weapon if you do not appreciate the beauty for its own sake.

\myendsectiontext


\bigskip

\mysection{When (Not) to Use Probabilities}


 It may come as a surprise to some readers that I do not always
advocate using probabilities. 


 Or rather, I don't always advocate that human
beings, trying to solve their problems, should try to \textit{make up}
verbal probabilities, and then apply the laws of probability theory or
decision theory to whatever number they just made up, and then use the
result as their final belief or decision.


 The laws of probability are laws, not suggestions, but often the
true Law is too difficult for us humans to compute. If \textsf{P} ${\neq}$ \textsf{NP}
and the universe has no source of exponential computing power, then
there are evidential updates too difficult for even a superintelligence
to compute---even though the probabilities would be quite well-defined,
if we could afford to calculate them.


 So sometimes you don't apply probability theory.
Especially if you're human, and your brain has evolved
with all sorts of useful algorithms for uncertain reasoning, that
\textit{don't} involve verbal probability assignments.


 Not sure where a flying ball will land? I don't
advise trying to formulate a probability distribution over its landing
spots, performing deliberate Bayesian updates on your glances at the
ball, and calculating the expected utility of all possible strings of
motor instructions to your muscles. Trying to catch a flying ball,
you're probably better off with your
brain's built-in mechanisms than using deliberative
verbal reasoning to invent or manipulate probabilities.


 But this doesn't mean you're going
\textit{beyond} probability theory or \textit{above} probability
theory.


 The Dutch book arguments still apply. If I offer you a choice of
gambles (\$10,000 if the ball lands in this square, versus \$10,000 if
I roll a die and it comes up 6), and you answer in a way that does not
allow consistent probabilities to be assigned, then you will accept
combinations of gambles that are certain losses, or reject gambles that
are certain gains\,\ldots


 Which still doesn't mean that you should try to
use deliberative verbal reasoning. I would expect that for professional
baseball players, at least, it's more important to
catch the ball than to assign consistent probabilities. Indeed, if you
tried to make up probabilities, the \textit{verbal} probabilities might
not even be very good ones, compared to some gut-level feeling---some
wordless representation of uncertainty in the back of your mind.


 There is nothing privileged about uncertainty that is expressed in
words, unless the verbal parts of your brain \textit{do}, in fact,
happen to work better on the problem.


 And while accurate maps of the same territory will necessarily be
consistent among themselves, not all consistent maps are accurate. It
is more important to be accurate than to be consistent, and more
important to catch the ball than to be consistent.


 In fact, I generally advise against \textit{making up}
probabilities, unless it seems like you have some decent basis for
them. This only fools you into believing that you are more Bayesian
than you actually are.


 To be specific, I would advise, in most cases, against using
non-numerical procedures to create what appear to be numerical
probabilities. Numbers should come from numbers.


 Now there \textit{are} benefits from trying to translate your gut
feelings of uncertainty into verbal probabilities. It may help you spot
problems like the conjunction fallacy. It may help you spot internal
inconsistencies---though it may not show you any way to remedy them.


 But you shouldn't go around thinking that if you
translate your gut feeling into ``one in a
thousand,'' then, on occasions when you emit these
verbal words, the corresponding event will happen around one in a
thousand times. Your brain is not so well-calibrated. If instead you do
something nonverbal with your gut feeling of uncertainty, you may be
better off, because at least you'll be \textit{using}
the gut feeling the way it was meant to be used.


 This specific topic came up recently in the context of the Large
Hadron Collider, and an argument given\footnote{\url{http://www.global-catastrophic-risks.com/abstracts/ab_hillerbrand_ord_sandberg.html}} at the Global Catastrophic Risks
conference:


 That we couldn't be sure that there was no error
in the papers which showed from multiple angles that the LHC
couldn't possibly destroy the world. And moreover, the
theory used in the papers might be wrong. And in either case, there was
still a chance the LHC \textit{could} destroy the world. And therefore,
it ought not to be turned on.


 Now if the argument had been given in \textit{just} this way, I
would not have objected to its epistemology.


 But the speaker actually purported to assign a probability of at
least 1 in 1,000 that the theory, model, or calculations in the LHC
paper were wrong; and a probability of at least 1 in 1,000 that, if the
theory or model or calculations were wrong, the LHC would destroy the
world.


 After all, it's surely not so improbable that
future generations will reject the theory used in the LHC paper, or
reject the model, or maybe just find an error. And if the LHC paper is
wrong, then who knows what might happen as a result?


 So that is an argument---but to assign \textit{numbers} to it?


 I object to the air of authority given to these numbers pulled out
of thin air. I generally feel that if you can't use
probabilistic tools to shape your feelings of uncertainty, you ought
not to dignify them by calling them probabilities.


 The alternative I would propose, in this particular case, is to
debate the general rule of banning physics experiments because you
cannot be absolutely certain of the arguments that say they are safe.


 I hold that if you phrase it this way, then your mind, by
considering frequencies of events, is likely to bring in more
consequences of the decision, and remember more relevant historical
cases.


 If you debate just the one case of the LHC, and assign specific
probabilities, it (1) gives very shaky reasoning an undue air of
authority, (2) obscures the general consequences of applying similar
rules, and even (3) creates the illusion that we might come to a
different decision if someone else published a new physics paper that
decreased the probabilities.


 The authors at the Global Catastrophic Risk conference seemed to
be suggesting that we could just do a bit more analysis of the LHC and
\textit{then} switch it on. This struck me as the most disingenuous
part of the argument. Once you admit the argument
``Maybe the analysis could be wrong, and who knows
what happens then,'' there is no possible physics
paper that can ever get rid of it.


 No matter what other physics papers had been published previously,
the authors would have used the same argument \textit{and made up the
same numerical probabilities} at the Global Catastrophic Risk
conference. I cannot be sure of this statement, of course, but it has a
probability of 75\%.


 In general a rationalist tries to make their minds function at the
best achievable power output; sometimes this involves \textit{talking}
about verbal probabilities, and sometimes it does not, but always the
laws of probability theory \textit{govern.}


 If all you have is a gut feeling of uncertainty, then you should
probably stick with those algorithms that make use of gut feelings of
uncertainty, because your built-in algorithms may do better than your
clumsy attempts to put things into words.


 Now it may be that by reasoning thusly, I may find myself
inconsistent. For example, I would be substantially more alarmed about
a lottery device with a well-defined chance of 1 in 1,000,000 of
destroying the world, than I am about the Large Hadron Collider being
switched on.


 On the other hand, if you asked me whether I could make one
million statements of authority equal to ``The Large
Hadron Collider will not destroy the world,'' and be
wrong, on average, around once, then I would have to say no.


 What should I do about this inconsistency? I'm not
sure, but I'm certainly not going to wave a magic wand
to make it go away. That's like finding an
inconsistency in a pair of maps you own, and quickly scribbling some
alterations to make sure they're consistent.


 I would also, by the way, be substantially more worried about a
lottery device with a 1 in 1,000,000,000 chance of destroying the
world, than a device which destroyed the world if the Judeo-Christian
God existed. But I would not suppose that I could make one billion
statements, one after the other, fully independent and equally fraught
as ``There is no God,'' and be wrong
on average around once.


 I can't say I'm \textit{happy}
with this state of epistemic affairs, but I'm not going
to modify it until I can see myself moving in the direction of
\textit{greater accuracy and real-world effectiveness}, not just moving
in the direction of greater self-consistency. The goal is to win, after
all. If I make up a probability that is not shaped by probabilistic
tools, if I make up a number that is not created by numerical methods,
then maybe I am just defeating my built-in algorithms that would do
better by reasoning in their native modes of uncertainty.


 Of course this is not a license to ignore probabilities that are
well-founded. Any numerical founding at all is likely to be better than
a vague feeling of uncertainty; humans are terrible statisticians. But
pulling a number \textit{entirely} out of your butt, that is, using a
non-numerical procedure to produce a number, is nearly no foundation at
all; and in that case you probably are better off sticking with the
vague feelings of uncertainty.


 Which is why my writing generally uses words like
``maybe'' and
``probably'' and
``surely'' instead of assigning
made-up numerical probabilities like
``40\%'' and
``70\%'' and
``95\%.'' Think of how silly that
would look. I think it actually \textit{would} be silly; I think I
would do worse thereby.


 I am not the kind of straw Bayesian who says that you should make
up probabilities to avoid being subject to Dutch books. I am the sort
of Bayesian who says that in practice, humans end up subject to Dutch
books because they aren't powerful enough to avoid
them; and moreover it's more important to catch the
ball than to avoid Dutch books. The math is like \textit{underlying}
physics, inescapably governing, but too expensive to calculate.


 Nor is there any point in a ritual of cognition that
\textit{mimics} the surface forms of the math, but fails to produce
systematically better decision-making. That would be a lost purpose;
this is not the true art of living under the law.

\myendsectiontext

\mysection{Newcomb's Problem and Regret of Rationality}


 The following may well be the most controversial dilemma in the
history of decision theory:

\begin{quotation}

 A superintelligence from another galaxy, whom we shall call Omega,
comes to Earth and sets about playing a strange little game. In this
game, Omega selects a human being, sets down two boxes in front of
them, and flies away.


 Box $A$ is transparent and contains a thousand dollars.


 Box $B$ is opaque, and contains either a million dollars, or
nothing.


 You can take both boxes, or take only box $B$.


 And the twist is that Omega has put a million dollars in box $B$ if
and only if Omega has predicted that you will take only box $B$.


 Omega has been correct on each of 100 observed occasions so
far---everyone who took both boxes has found box $B$ empty and received
only a thousand dollars; everyone who took only box $B$ has found $B$
containing a million dollars. (We assume that box $A$ vanishes in a puff
of smoke if you take only box $B$; no one else can take box $A$
afterward.)


 Before you make your choice, Omega has flown off and moved on to
its next game. Box $B$ is already empty or already full.


 Omega drops two boxes on the ground in front of you and flies
off.

{
  Do you take both boxes, or only box $B$?}
\end{quotation}


 And the standard philosophical conversation runs thusly:

\begin{quotation}

 \textsc{One-boxer}: ``I take only box $B$, of course.
I'd rather have a million than a
thousand.''


 \textsc{Two-boxer}: ``Omega has already left. Either box $B$
is already full or already empty. If box $B$ is already empty, then
taking both boxes nets me \$1,000, taking only box $B$ nets me \$0. If
box $B$ is already full, then taking both boxes nets \$1,001,000, taking
only box $B$ nets \$1,000,000. In either case I do better by taking both
boxes, and worse by leaving a thousand dollars on the table---so I will
be rational, and take both boxes.''


 \textsc{One-boxer}: ``If you're so
rational, why ain'cha rich?''

{
 \textsc{Two-boxer}: ``It's not my fault
Omega chooses to reward only people with irrational dispositions, but
it's already too late for me to do anything about
that.''}
\end{quotation}

{
 There is a \textit{large} literature on the topic of Newcomblike
problems---especially if you consider the Prisoner's
Dilemma as a special case, which it is generally held to be.
\textit{Paradoxes of Rationality and Cooperation:
Prisoner's Dilemma and Newcomb's
Problem}\footnote{Richmond Campbell and Lanning Snowden, eds., \textit{Paradoxes
of Rationality and Cooperation: Prisoner's Dilemma and
Newcomb's Problem} (Vancouver: University of British
Columbia Press, 1985).\comment{1}} is an edited volume that includes
Newcomb's original essay. For those who read only
online material, Ledwig's PhD thesis summarizes the
major standard positions.\footnote{Marion Ledwig, ``Newcomb's
Problem'' (PhD diss., University of Constance,
2000).\comment{2}}}


 I'm not going to go into the whole literature, but
the dominant consensus in modern decision theory is that one should
two-box, and Omega is just rewarding agents with irrational
dispositions. This dominant view goes by the name of
``causal decision theory.''


 I'm not going to try to present my own analysis
here. Way too long a story, even by my standards.


 But it is agreed even among causal decision theorists that if you
have the power to precommit yourself to take one box, in
Newcomb's Problem, then you should do so. If you can
precommit yourself before Omega examines you, then you are directly
causing box $B$ to be filled.


 Now in my field---which, in case you have forgotten, is
self-modifying AI---this works out to saying that if you build an AI
that two-boxes on Newcomb's Problem, it will
self-modify to one-box on Newcomb's Problem, if the AI
considers in advance that it might face such a situation. Agents with
free access to their own source code have access to a cheap method of
precommitment.


 What if you expect that you might, in general, face a Newcomblike
problem, without knowing the exact form of the problem? Then you would
have to modify yourself into a sort of agent whose disposition was such
that it would generally receive high rewards on Newcomblike problems.


 But what does an agent with a disposition generally-well-suited to
Newcomblike problems look like? Can this be formally specified?


 Yes, but when I tried to write it up, I realized that I was
starting to write a small book. And it wasn't the most
important book I had to write, so I shelved it. My slow writing speed
really is the bane of my existence. The theory I worked out seems, to
me, to have many nice properties besides being well-suited to
Newcomblike problems. It would make a nice PhD thesis, if I could get
someone to accept it as my PhD thesis. But that's
pretty much what it would take to make me unshelve the project.
Otherwise I can't justify the time expenditure, not at
the speed I currently write books.


 I say all this, because there's a common attitude
that ``Verbal arguments for one-boxing are easy to
come by; what's hard is developing a good decision
theory that one-boxes''---coherent math which
one-boxes on Newcomb's Problem without producing absurd
results elsewhere. So I do understand that, and I did set out to
develop such a theory, but my writing speed on big papers is so slow
that I can't publish it. Believe it or not,
it's true.


 Nonetheless, I would like to present some of my
\textit{motivations} on Newcomb's Problem---the reasons
I felt impelled to seek a new theory---because they illustrate my
source-attitudes toward rationality. Even if I can't
present the theory that these motivations motivate\,\ldots


 First, foremost, fundamentally, above all else:


 Rational agents should WIN.


 Don't mistake me, and think that
I'm talking about the Hollywood Rationality stereotype
that rationalists should be selfish or shortsighted. If your utility
function has a term in it for others, then win their happiness. If your
utility function has a term in it for a million years hence, then win
the eon.


 But at any rate, \textit{WIN}. Don't lose
reasonably; \textbf{\textit{WIN}}.


 Now there are defenders of causal decision theory who argue that
the two-boxers are doing their best to win, and cannot help it if they
have been cursed by a Predictor who favors irrationalists. I will talk
about this defense in a moment. But first, I want to draw a distinction
between causal decision theorists who believe that two-boxers are
genuinely doing their best to win; versus someone who thinks that
two-boxing is the \textit{reasonable} or the \textit{rational} thing to
do, but that the reasonable move just happens to predictably lose, in
this case. There are a \textit{lot} of people out there who think that
rationality predictably loses on various problems---that, too, is part
of the Hollywood Rationality stereotype, that Kirk is predictably
superior to Spock.


 Next, let's turn to the charge that Omega favors
irrationalists. I can conceive of a superbeing who rewards only people
born with a particular gene, \textit{regardless of their choices}. I
can conceive of a superbeing who rewards people whose brains inscribe
the \textit{particular algorithm} of ``Describe your
options in English and choose the last option when ordered
alphabetically,'' but who does not reward anyone who
chooses the same option for a different reason. But Omega rewards
people who choose to take only box $B$, \textit{regardless of which
algorithm they use to arrive at this decision}, and this is why I
don't buy the charge that Omega is rewarding the
irrational. Omega doesn't care whether or not you
follow some particular ritual of cognition; Omega only cares about your
predicted \textit{decision}.


 We can choose whatever reasoning algorithm we like, and will be
rewarded or punished only according to that algorithm's
choices, with no other dependency---Omega just cares where we go, not
how we got there.


 It is precisely the notion that Nature does not care about our
\textit{algorithm} that frees us up to pursue the winning Way---without
attachment to any particular ritual of cognition, apart from our belief
that it wins. Every rule is up for grabs, \textit{except} the rule of
winning.


 As Miyamoto Musashi said---it's really worth
repeating:

\begin{quote}
{
 You can win with a long weapon, and yet you can also win with a
short weapon. In short, the Way of the Ichi school is the spirit of
winning, whatever the weapon and whatever its size.\footnote{Musashi, \textit{Book of Five Rings}.\comment{3}}}
\end{quote}


 (Another example: It was argued by McGee that we must adopt
bounded utility functions or be subject to ``Dutch
books'' over infinite times. But: \textit{The utility
function is not up for grabs.} I love life without limit or upper
bound; there is no finite amount of life lived $N$ where I would prefer
an 80.0001\% probability of living $N$ years to a 0.0001\% chance of
living a googolplex years and an 80\% chance of living forever. This is
a sufficient condition to imply that my utility function is unbounded.
So I just have to figure out how to optimize \textit{for that
morality.} You can't tell me, first, that above all I
must conform to a particular ritual of cognition, and then that, if I
conform to that ritual, I must change my morality to avoid being
Dutch-booked. Toss out the losing ritual; don't change
the definition of winning. That's like deciding to
prefer \$1,000 to \$1,000,000 so that Newcomb's Problem
doesn't make your preferred ritual of cognition look
bad.)


 ``But,'' says the causal
decision theorist, ``to take only one box, you must
somehow believe that your choice can affect whether box $B$ is empty or
full---and that's \textit{unreasonable}! Omega has
already left! It's physically
impossible!''

{
 Unreasonable? I am a rationalist: what do I care about being
unreasonable? I don't have to conform to a particular
ritual of cognition. I don't have to take only box $B$
\textit{because I believe my choice affects the box, even though Omega
has already left.} I can just\,\ldots take only box $B$.}


 I do have a proposed alternative ritual of cognition that computes
this decision, which this margin is too small to contain; but I
shouldn't need to show this to you. The point is not to
have an elegant theory of winning---the point is to win; elegance is a
side effect.


 Or to look at it another way: Rather than starting with a concept
of what is the reasonable decision, and then asking whether
``reasonable'' agents leave with a
lot of money, start by looking at the agents who leave with a lot of
money, develop a theory of which agents tend to leave with the most
money, and from this theory, try to figure out what is
``reasonable.''
``Reasonable'' may just refer to
decisions in conformance with our current ritual of cognition---what
else would determine whether something seems
``reasonable'' or not?

{
 From James Joyce (no relation), \textit{Foundations of Causal
Decision Theory}:\footnote{James M. Joyce, \textit{The Foundations of Causal Decision
Theory} (New York: Cambridge University Press, 1999),
doi:10.1017/CBO9780511498497.\comment{4}}}

\begin{quotation}

 Rachel has a perfectly good answer to the ``Why
ain't you rich?'' question.
``I am not rich,'' she will say,
``because I am not the kind of person the psychologist
thinks will refuse the money. I'm just not like you,
Irene. Given that I know that I am the type who takes the money, and
given that the psychologist knows that I am this type, it was
reasonable of me to think that the \$1,000,000 was not in my account.
The \$1,000 was the most I was going to get no matter what I did. So
the only reasonable thing for me to do was to take
it.''

{
 Irene may want to press the point here by asking,
``But don't you wish you were like me,
Rachel? Don't you wish that you were the refusing
type?'' There is a tendency to think that Rachel, a
committed causal decision theorist, must answer this question in the
negative, which seems obviously wrong (given that being like Irene
would have made her rich). This is not the case. Rachel can and should
admit that she does wish she were more like Irene.
``It would have been better for
me,'' she might concede, ``had I
been the refusing type.'' At this point Irene will
exclaim, ``You've admitted it! It
wasn't so smart to take the money after
all.'' Unfortunately for Irene, her conclusion does
not follow from Rachel's premise. Rachel will patiently
explain that wishing to be a refuser in a Newcomb problem is not
inconsistent with thinking that one should take the \$1,000
\textit{whatever type one is.} When Rachel wishes she was
Irene's type she is wishing \textit{for
  Irene's options}, not sanctioning her choice.}
\end{quotation}


 It is, I would say, a general principle of rationality---indeed,
part of how I \textit{define} rationality---that you never end up
envying someone else's mere \textit{choices.} You might
envy someone their genes, if Omega rewards genes, or if the genes give
you a generally happier disposition. But Rachel, above, envies Irene
her choice, and \textit{only} her choice, irrespective of what
algorithm Irene used to make it. Rachel wishes \textit{just} that she
had a disposition to choose differently.


 You shouldn't claim to be more rational than
someone and simultaneously envy them their choice---\textit{only} their
choice. Just \textit{do} the act you envy.


 I keep trying to say that rationality is the winning-Way, but
causal decision theorists insist that taking both boxes is what
\textit{really} wins, because you \textit{can't
possibly do better} by leaving \$1,000 on the table\,\ldots even though
the single-boxers leave the experiment with more money. Be careful of
this sort of argument, any time you find yourself defining the
``winner'' as someone other than the
agent who is currently smiling from on top of a giant heap of utility.


 Yes, there are various thought experiments in which some agents
start out with an advantage---but if the task is to, say, decide
whether to jump off a cliff, you want to be careful not to define
cliff-refraining agents as having an unfair prior advantage over
cliff-jumping agents, by virtue of their unfair refusal to jump off
cliffs. At this point you have covertly redefined
``winning'' as conformance to a
particular ritual of cognition. \textit{Pay attention to the money!}


 Or here's another way of looking at it: Faced with
Newcomb's Problem, would you want to look really hard
for a reason to believe that it was perfectly reasonable and rational
to take only box $B$; because, if such a line of argument existed, you
would take only box $B$ and find it full of money? Would you spend an
extra hour thinking it through, if you were confident that, at the end
of the hour, you would be able to convince yourself that box $B$ was the
rational choice? This too is a rather odd position to be in.
Ordinarily, the work of rationality goes into figuring out which choice
is the best---not finding a reason to believe that a particular choice
is the best.


 Maybe it's too easy to say that you
``ought to'' two-box on
Newcomb's Problem, that this is the
``reasonable'' thing to do, so long
as the money isn't actually in front of you. Maybe
you're just numb to philosophical dilemmas, at this
point. What if your daughter had a 90\% fatal disease, and box $A$
contained a serum with a 20\% chance of curing her, and box $B$ might
contain a serum with a 95\% chance of curing her? What if there was an
asteroid rushing toward Earth, and box $A$ contained an asteroid
deflector that worked 10\% of the time, and box $B$ might contain an
asteroid deflector that worked 100\% of the time?

{
 Would you, at that point, find yourself \textit{tempted to make an
unreasonable choice}?}


 If the stake in box $B$ was something you \textit{could not} leave
behind? Something overwhelmingly more important to you than being
reasonable? If you absolutely \textit{had to} win---\textit{really}
win, not just be defined as winning?


 Would you \textit{wish with all your power} that the
``reasonable'' decision were to take
only box $B$?


 Then maybe it's time to update your definition of
reasonableness.


 Alleged rationalists should not find themselves envying the mere
decisions of alleged nonrationalists, because your decision can be
whatever you like. When you find yourself in a position like this, you
shouldn't chide the other person for failing to conform
to your concepts of reasonableness. You should realize you got the Way
wrong.


 So, too, if you ever find yourself keeping separate track of the
``reasonable'' belief, versus the
belief that seems likely to be actually \textit{true.} Either you have
misunderstood reasonableness, or your second intuition is just wrong.


 Now one can't simultaneously \textit{define}
``rationality'' as the winning Way,
and \textit{define} ``rationality''
as Bayesian probability theory and decision theory. But it is the
argument that I am putting forth, and the moral of my advice to trust
in Bayes, that the laws governing winning have indeed proven to be
math. If it ever turns out that Bayes fails---receives systematically
lower rewards on some problem, relative to a superior alternative, in
virtue of its mere decisions---then Bayes has to go \textit{out the
window}. ``Rationality'' is just the
label I use for my beliefs about the winning Way---the Way of the agent
smiling from on top of the giant heap of utility. \textit{Currently},
that label refers to Bayescraft.


 I realize that this is not a knockdown criticism of causal
decision theory---that would take the actual book and/or PhD
thesis---but I hope it illustrates some of my underlying attitude
toward this notion of
``rationality.''

{
 [Edit 2015: I've now written a book-length
exposition of a decision theory that dominates causal decision theory,
``Timeless Decision
Theory.''\footnote{Yudkowsky, \textit{Timeless Decision Theory}.\comment{5}}\supercomma\footnote{Yudkowsky and Soares, ``Functional Decision Theory: A New Theory of Instrumental Rationality'' \url{https://arxiv.org/abs/1710.05060}} The cryptographer Wei
Dai has responded with another alternative to causal decision theory,
updateless decision theory, that dominates both causal and timeless
decision theory. As of 2015, the best up-to-date discussions of these
theories are Daniel Hintze's ``Problem
Class Dominance in Predictive
Dilemmas''\footnote{Daniel Hintze, ``Problem Class Dominance in
Predictive Dilemmas,'' Honors thesis (2014).\comment{6}} and Nate Soares and
Benja Fallenstein's ``Toward Idealized
Decision Theory.''\footnote{Nate Soares and Benja Fallenstein, ``Toward
Idealized Decision Theory,'' Technical report.
Berkeley, CA: Machine Intelligence Research Institute (2014),
\url{http://intelligence.org/files/TowardIdealizedDecisionTheory.pdf}.\comment{7}} ]}


 You shouldn't find yourself distinguishing the
winning choice from the reasonable choice. Nor should you find yourself
distinguishing the reasonable belief from the belief that is most
likely to be true.


 That is why I use the word
``rational'' to denote my beliefs
about accuracy and winning---\textit{not} to denote verbal reasoning,
or strategies which yield certain success, or that which is logically
provable, or that which is publicly demonstrable, or that which is
reasonable.


 As Miyamoto Musashi said:

\begin{quote}
{
 The primary thing when you take a sword in your hands is your
intention to cut the enemy, whatever the means. Whenever you parry,
hit, spring, strike or touch the enemy's cutting sword,
you must cut the enemy in the same movement. It is essential to attain
this. If you think only of hitting, springing, striking or touching the
enemy, you will not be able actually to cut him.}
\end{quote}

\myendsectiontext

\mysectionnn{Interlude: The Twelve Virtues of Rationality}
\label{twelve_virtues}


 The first virtue is curiosity. A burning itch to know is higher
than a solemn vow to pursue truth. To feel the burning itch of
curiosity requires both that you be ignorant, and that you desire to
relinquish your ignorance. If in your heart you believe you already
know, or if in your heart you do not wish to know, then your
questioning will be purposeless and your skills without direction.
Curiosity seeks to annihilate itself; there is no curiosity that does
not want an answer. The glory of glorious mystery is to be solved,
after which it ceases to be mystery. Be wary of those who speak of
being open-minded and modestly confess their ignorance. There is a time
to confess your ignorance and a time to relinquish your ignorance.


 The second virtue is relinquishment. P. C. Hodgell said:
``That which can be destroyed by the truth should
be.''\footnote{Patricia C. Hodgell, \textit{Seeker's Mask}
(Meisha Merlin Publishing, Inc., 2001).\comment{1}} Do not flinch from
experiences that might destroy your beliefs. The thought you cannot
think controls you more than thoughts you speak aloud. Submit yourself
to ordeals and test yourself in fire. Relinquish the emotion which
rests upon a mistaken belief, and seek to feel fully that emotion which
fits the facts. If the iron approaches your face, and you believe it is
hot, and it is cool, the Way opposes your fear. If the iron approaches
your face, and you believe it is cool, and it is hot, the Way opposes
your calm. Evaluate your beliefs first and then arrive at your
emotions. Let yourself say: ``If the iron is hot, I
desire to believe it is hot, and if it is cool, I desire to believe it
is cool.'' Beware lest you become attached to beliefs
you may not want.

\label{third_virtue}

 The third virtue is lightness. Let the winds of evidence blow you
about as though you are a leaf, with no direction of your own. Beware
lest you fight a rearguard retreat against the evidence, grudgingly
conceding each foot of ground only when forced, feeling cheated.
Surrender to the truth as quickly as you can. Do this the instant you
realize what you are resisting, the instant you can see from which
quarter the winds of evidence are blowing against you. Be faithless to
your cause and betray it to a stronger enemy. If you regard evidence as
a constraint and seek to free yourself, you sell yourself into the
chains of your whims. For you cannot make a true map of a city by
sitting in your bedroom with your eyes shut and drawing lines upon
paper according to impulse. You must walk through the city and draw
lines on paper that correspond to what you see. If, seeing the city
unclearly, you think that you can shift a line just a little to the
right, just a little to the left, according to your caprice, this is
just the same mistake.


 The fourth virtue is evenness. One who wishes to believe says,
``Does the evidence permit me to
believe?'' One who wishes to disbelieve asks,
``Does the evidence force me to
believe?'' Beware lest you place huge burdens of
proof only on propositions you dislike, and then defend yourself by
saying: ``But it is good to be
skeptical.'' If you attend only to favorable
evidence, picking and choosing from your gathered data, then the more
data you gather, the less you know. If you are selective about which
arguments you inspect for flaws, or how hard you inspect for flaws,
then every flaw you learn how to detect makes you that much stupider.
If you first write at the bottom of a sheet of paper
``And therefore, the sky is green!''
it does not matter what arguments you write above it afterward; the
conclusion is already written, and it is already correct or already
wrong. To be clever in argument is not rationality but rationalization.
Intelligence, to be useful, must be used for something other than
defeating itself. Listen to hypotheses as they plead their cases before
you, but remember that you are not a hypothesis; you are the judge.
Therefore do not seek to argue for one side or another, for if you knew
your destination, you would already be there.


 The fifth virtue is argument. Those who wish to fail must first
prevent their friends from helping them. Those who smile wisely and say
``I will not argue'' remove
themselves from help and withdraw from the communal effort. In argument
strive for exact honesty, for the sake of others and also yourself: the
part of yourself that distorts what you say to others also distorts
your own thoughts. Do not believe you do others a favor if you accept
their arguments; the favor is to you. Do not think that fairness to all
sides means balancing yourself evenly between positions; truth is not
handed out in equal portions before the start of a debate. You cannot
move forward on factual questions by fighting with fists or insults.
Seek a test that lets reality judge between you.


 The sixth virtue is empiricism. The roots of knowledge are in
observation and its fruit is prediction. What tree grows without roots?
What tree nourishes us without fruit? If a tree falls in a forest and
no one hears it, does it make a sound? One says, ``Yes
it does, for it makes vibrations in the air.''
Another says, ``No it does not, for there is no
auditory processing in any brain.'' Though they
argue, one saying ``Yes,'' and one
saying ``No,'' the two do not
anticipate any different experience of the forest. Do not ask which
beliefs to profess, but which experiences to anticipate. Always know
which difference of experience you argue about. Do not let the argument
wander and become about something else, such as
someone's virtue as a rationalist. Jerry Cleaver said:
``What does you in is not failure to apply some
high-level, intricate, complicated technique. It's
overlooking the basics. Not keeping your eye on the
ball.''\footnote{Cleaver, \textit{Immediate Fiction: A Complete Writing
Course}.\comment{2}} Do not be blinded by
words. When words are subtracted, anticipation remains.

\label{seventh_virtue}

 The seventh virtue is simplicity. Antoine de Saint-Exup√©ry said:
``Perfection is achieved not when there is nothing
left to add, but when there is nothing left to take
away.''\footnote{Antoine de Saint-Exupery, \textit{Terre des Hommes} (Paris:
Gallimard, 1939).\comment{3}} Simplicity is virtuous in
belief, design, planning, and justification. When you profess a huge
belief with many details, each additional detail is another chance for
the belief to be wrong. Each specification adds to your burden; if you
can lighten your burden you must do so. There is no straw that lacks
the power to break your back. Of artifacts it is said: The most
reliable gear is the one that is designed out of the machine. Of plans:
A tangled web breaks. A chain of a thousand links will arrive at a
correct conclusion if every step is correct, but if one step is wrong
it may carry you anywhere. In mathematics a mountain of good deeds
cannot atone for a single sin. Therefore, be careful on every step.

\label{eighth_virtue}

 The eighth virtue is humility. To be humble is to take specific
actions in anticipation of your own errors. To confess your fallibility
and then do nothing about it is not humble; it is boasting of your
modesty. Who are most humble? Those who most skillfully prepare for the
deepest and most catastrophic errors in their own beliefs and plans.
Because this world contains many whose grasp of rationality is abysmal,
beginning students of rationality win arguments and acquire an
exaggerated view of their own abilities. But it is useless to be
superior: Life is not graded on a curve. The best physicist in ancient
Greece could not calculate the path of a falling apple. There is no
guarantee that adequacy is possible given your hardest effort;
therefore spare no thought for whether others are doing worse. If you
compare yourself to others you will not see the biases that all humans
share. To be human is to make ten thousand errors. No one in this world
achieves perfection.


 The ninth virtue is perfectionism. The more errors you correct in
yourself, the more you notice. As your mind becomes more silent, you
hear more noise. When you notice an error in yourself, this signals
your readiness to seek advancement to the next level. If you tolerate
the error rather than correcting it, you will not advance to the next
level and you will not gain the skill to notice new errors. In every
art, if you do not seek perfection you will halt before taking your
first steps. If perfection is impossible that is no excuse for not
trying. Hold yourself to the highest standard you can imagine, and look
for one still higher. Do not be content with the answer that is almost
right; seek one that is exactly right.


 The tenth virtue is precision. One comes and says: The quantity is
between 1 and 100. Another says: The quantity is between 40 and 50. If
the quantity is 42 they are both correct, but the second prediction was
more useful and exposed itself to a stricter test. What is true of one
apple may not be true of another apple; thus more can be said about a
single apple than about all the apples in the world. The narrowest
statements slice deepest, the cutting edge of the blade. As with the
map, so too with the art of mapmaking: The Way is a precise Art. Do not
walk to the truth, but dance. On each and every step of that dance your
foot comes down in exactly the right spot. Each piece of evidence
shifts your beliefs by exactly the right amount, neither more nor less.
What is exactly the right amount? To calculate this you must study
probability theory. Even if you cannot do the math, knowing that the
math exists tells you that the dance step is precise and has no room in
it for your whims.


 The eleventh virtue is scholarship. Study many sciences and absorb
their power as your own. Each field that you consume makes you larger.
If you swallow enough sciences the gaps between them will diminish and
your knowledge will become a unified whole. If you are gluttonous you
will become vaster than mountains. It is especially important to eat
math and science which impinge upon rationality: evolutionary
psychology, heuristics and biases, social psychology, probability
theory, decision theory. But these cannot be the only fields you study.
The Art must have a purpose other than itself, or it collapses into
infinite recursion.


 Before these eleven virtues is a virtue which is nameless.

{
 Miyamoto Musashi wrote, in \textit{The Book of Five
Rings}:\footnote{Musashi, \textit{Book of Five Rings}.\comment{4}}}

\begin{quote}
{
 The primary thing when you take a sword in your hands is your
intention to cut the enemy, whatever the means. Whenever you parry,
hit, spring, strike or touch the enemy's cutting sword,
you must cut the enemy in the same movement. It is essential to attain
this. If you think only of hitting, springing, striking or touching the
enemy, you will not be able actually to cut him. More than anything,
you must be thinking of carrying your movement through to cutting him.}
\end{quote}


 Every step of your reasoning must cut through to the correct
answer in the same movement. More than anything, you must think of
carrying your map through to reflecting the territory.


 If you fail to achieve a correct answer, it is futile to protest
that you acted with propriety.


 How can you improve your conception of rationality? Not by saying
to yourself, ``It is my duty to be
rational.'' By this you only enshrine your mistaken
conception. Perhaps your conception of rationality is that it is
rational to believe the words of the Great Teacher, and the Great
Teacher says, ``The sky is green,''
and you look up at the sky and see blue. If you think,
``It may look like the sky is blue, but rationality is
to believe the words of the Great Teacher,'' you lose
a chance to discover your mistake.


 Do not ask whether it is ``the
Way'' to do this or that. Ask whether the sky is blue
or green. If you speak overmuch of the Way you will not attain it.


 You may try to name the highest principle with names such as
``the map that reflects the
territory'' or ``experience of
success and failure'' or ``Bayesian
decision theory.'' But perhaps you describe
incorrectly the nameless virtue. How will you discover your mistake?
Not by comparing your description to itself, but by comparing it to
that which you did not name.


 If for many years you practice the techniques and submit yourself
to strict constraints, it may be that you will glimpse the center. Then
you will see how all techniques are one technique, and you will move
correctly without feeling constrained. Musashi wrote:
``When you appreciate the power of nature, knowing the
rhythm of any situation, you will be able to hit the enemy naturally
and strike naturally. All this is the Way of the
Void.''


 These then are twelve virtues of rationality:


 Curiosity, relinquishment, lightness, evenness, argument,
empiricism, simplicity, humility, perfectionism, precision,
scholarship, and the void.

\myendsectiontext


\bigskip




