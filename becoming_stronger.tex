\part{Becoming Stronger}


\mysectiontwo{Beginnings: An Introduction}{Beginnings: An Introduction\newline
by Rob Bensinger}

{
 This, the final book of \textit{Rationality: From AI to Zombies},
is less a conclusion than a call to action. In keeping with
\textit{Becoming Stronger}'s function as a jumping-off
point for further investigation, I'll conclude by
citing resources the reader can use to move beyond these sequences and
seek out a fuller understanding of Bayesianism.}

{
 This text's definition of normative rationality in
terms of Bayesian probability theory and decision theory is standard in
cognitive science. For an introduction to the heuristics and biases
approach, see Baron's \textit{Thinking and
Deciding}.\textsuperscript{1} For a general introduction to the field,
see the \textit{Oxford Handbook of Thinking and
Reasoning}.\textsuperscript{2}}

{
 The arguments made in these pages about the \textit{philosophy} of
rationality are more controversial. Yudkowsky argues, for example, that
a rational agent should one-box in Newcomb's
Problem---a minority position among working decision
theorists.\textsuperscript{3} (See Holt for a nontechnical description
of Newcomb's Problem.\textsuperscript{4}) Gary
Drescher's \textit{Good and Real} independently comes
to many of the same conclusions as Yudkowsky on philosophy of science
and decision theory.\textsuperscript{5} As such, it serves as an
excellent book-length treatment of the core philosophical content of
\textit{Rationality: From AI to Zombies}.}

{
 Talbott distinguishes several views in Bayesian epistemology,
including E.T. Jaynes' position that not all possible
priors are equally reasonable.\textsuperscript{6,7} Like Jaynes,
Yudkowsky is interested in supplementing the Bayesian optimality
criterion for belief revision with an optimality criterion for priors.
This aligns Yudkowsky with researchers who hope to better understand
general-purpose AI via an improved theory of ideal reasoning, such as
Marcus Hutter.\textsuperscript{8} For a broader discussion of
philosophical efforts to naturalize theories of knowledge, see
Feldman.\textsuperscript{9}}

{
 ``Bayesianism'' is often
contrasted with ``frequentism.''
Some frequentists criticize Bayesians for treating probabilities as
subjective states of belief, rather than as objective frequencies of
events. Kruschke and Yudkowsky have replied that frequentism is even
more ``subjective'' than
Bayesianism, because frequentism's probability
assignments depend on the intentions of the
experimenter.\textsuperscript{10}}

{
 Importantly, this philosophical disagreement
shouldn't be conflated with the distinction between
Bayesian and frequentist data analysis methods, which can both be
useful when employed correctly. Bayesian statistical tools have become
cheaper to use since the 1980s, and their informativeness,
intuitiveness, and generality have come to be more widely appreciated,
resulting in ``Bayesian
revolutions'' in many sciences. However, traditional
frequentist methods remain more popular, and in some contexts they are
still clearly superior to Bayesian approaches.
Kruschke's \textit{Doing Bayesian Data Analysis} is a
fun and accessible introduction to the topic.\textsuperscript{11}}

{
 In light of evidence that training in statistics---and some other
fields, such as psychology---improves reasoning skills outside the
classroom, statistical literacy is directly relevant to the project of
overcoming bias. (Classes in formal logic and informal fallacies have
not proven similarly useful.)\textsuperscript{12,13}}

{
 ~}

\subsection{An Art in its Infancy}

{
 We conclude with three sequences on individual and collective
self-improvement. ``Yudkowsky's Coming
of Age'' provides a last in-depth illustration of the
dynamics of irrational belief, this time spotlighting the
author's own intellectual history.
``Challenging the Difficult'' asks
what it takes to solve a truly difficult problem---including demands
that go beyond epistemic rationality. Finally, ``The
Craft and the Community'' discusses rationality
groups and group rationality, raising the questions:}

{
 Can rationality be learned and taught?}

{
 If so, how much improvement is possible?}

{
 How can we be confident we're seeing a real effect
in a rationality intervention, and picking out the right cause?}

{
 What community norms would make this process of bettering
ourselves easier?}

{
 Can we effectively collaborate on large-scale problems without
sacrificing our freedom of thought and conduct?}

{
 Above all: What's missing? What should be in the
next generation of rationality primers---the ones that replace this
text, improve on its style, test its prescriptions, supplement its
content, and branch out in altogether new directions?}

{
 Though Yudkowsky was moved to write these essays by his own
philosophical mistakes and professional difficulties in AI theory, the
resultant material has proven useful to a much wider audience. The
original blog posts inspired the growth of \textit{Less Wrong}, a
community of intellectuals and life hackers with shared interests in
cognitive science, computer science, and philosophy. Yudkowsky and
other writers on \textit{Less Wrong} have helped seed the effective
altruism movement, a vibrant and audacious effort to identify the most
high-impact humanitarian charities and causes. These writings also
sparked the establishment of the Center for Applied Rationality, a
nonprofit organization that attempts to translate results from the
science of rationality into useable techniques for self-improvement.}

{
 I don't know what's next---what
other unconventional projects or ideas might draw inspiration from
these pages. We certainly face no shortage of global challenges, and
the art of applied rationality is a new and half-formed thing. There
are not many rationalists, and there are many things left undone.}

{
 But wherever you're headed next, reader---may you
serve your purpose well.}

{
 ~}

{\centering
 *
\par}


\bigskip

{
 1. Jonathan Baron, \textit{Thinking and Deciding} (Cambridge
University Press, 2007).}

{
 2. Keith J. Holyoak and Robert G. Morrison, \textit{The Oxford
Handbook of Thinking and Reasoning} (Oxford University Press, 2013).}

{
 3. Bourget and Chalmers, ``What Do Philosophers
Believe?''}

{
 4. Holt, ``Thinking Inside the
Boxes.''}

{
 5. Gary L. Drescher, \textit{Good and Real: Demystifying Paradoxes
from Physics to Ethics} (Cambridge, MA: MIT Press, 2006).}

{
 6. William Talbott, ``Bayesian
Epistemology,'' in \textit{The Stanford Encyclopedia
of Philosophy}, Fall 2013, ed. Edward N. Zalta.}

{
 7. Jaynes, \textit{Probability Theory}.}

{
 8. Marcus Hutter, \textit{Universal Artificial Intelligence:
Sequential Decisions Based On Algorithmic Probability} (Berlin:
Springer, 2005), doi:10.1007/b138233.}

{
 9. Richard Feldman, ``Naturalized
Epistemology,'' in \textit{The Stanford Encyclopedia
of Philosophy}, Summer 2012, ed. Edward N. Zalta.}

{
 10. John K. Kruschke, ``What to Believe: Bayesian
Methods for Data Analysis,'' \textit{Trends in
Cognitive Sciences} 14, no. 7 (2010): 293--300.}

{
 11. John K. Kruschke, \textit{Doing Bayesian Data Analysis, Second
Edition: A Tutorial with R, JAGS, and Stan} (Academic Press, 2014).}

{
 12. Geoffrey T. Fong, David H. Krantz, and Richard E. Nisbett,
``The Effects of Statistical Training on Thinking
about Everyday Problems,'' \textit{Cognitive
Psychology} 18, no. 3 (1986): 253--292,
doi:10.1016/0010-0285(86)90001-0.}

{
 13. Paul J. H. Schoemaker, ``The Role of
Statistical Knowledge in Gambling Decisions: Moment vs. Risk Dimension
Approaches,'' \textit{Organizational Behavior and
Human Performance} 24, no. 1 (1979): 1--17.}

\chapter{Yudkowsky's Coming of Age}

\mysection{My Childhood Death Spiral}

{
 My parents always used to downplay the value of intelligence. And
play up the value of---effort, as recommended by the latest research?
No, not effort. \textit{Experience.} A nicely unattainable hammer with
which to smack down a bright young child, to be sure. That was what my
parents told me when I questioned the Jewish religion, for example. I
tried laying out an argument, and I was told something along the lines
of: ``Logic has limits; you'll
understand when you're older that experience is the
important thing, and then you'll see the truth of
Judaism.'' I didn't try again. I made
one attempt to question Judaism in school, got slapped down,
didn't try again. I've never been a
slow learner. }

{
 Whenever my parents were doing something ill-advised, it was
always, ``We know better because we have more
experience. You'll understand when
you're older: maturity and wisdom are more important
than intelligence.''}

{
 If this was an attempt to focus the young Eliezer on
\textit{intelligence uber alles}, it was the most wildly successful
example of reverse psychology I've ever heard of.}

{
 But my parents aren't that cunning, and the
results weren't exactly positive.}

{
 For a long time, I thought that the moral of this story was that
experience was no match for sheer raw native intelligence. It
wasn't until a lot later, in my twenties, that I looked
back and realized that I couldn't possibly have been
more intelligent than my parents \textit{before puberty}, with my brain
not even fully developed. At age eleven, when I was already nearly a
full-blown atheist, I could not have defeated my parents in any
\textit{fair} contest of mind. My SAT scores were high for an
11-year-old, but they wouldn't have beaten my
parents' SAT scores in full adulthood. In a fair fight,
my parents' intelligence and experience could have
stomped any prepubescent child flat. It was dysrationalia that did them
in; they used their intelligence only to defeat itself.}

{
 But \textit{that} understanding came much later, when my
intelligence had processed and distilled many more years of
experience.}

{
 The moral I derived when I was young was that anyone who
downplayed the value of intelligence didn't understand
intelligence at all. My own intelligence had affected every aspect of
my life and mind and personality; that was massively obvious, seen at a
backward glance. ``Intelligence has nothing to do with
wisdom or being a good person''---oh, and does
self-awareness have nothing to do with wisdom, or being a good person?
Modeling yourself takes intelligence. For one thing, it takes enough
intelligence to learn evolutionary psychology.}

{
 We \textit{are} the cards we are dealt, and intelligence is the
unfairest of all those cards. More unfair than wealth or health or home
country, unfairer than your happiness set-point. People have difficulty
accepting that life can be that unfair; it's not a
happy thought. ``Intelligence isn't as
important as X'' is one way of turning away from the
unfairness, refusing to deal with it, thinking a happier thought
instead. It's a temptation, both to those dealt poor
cards, and to those dealt good ones. Just as downplaying the importance
of money is a temptation both to the poor and to the rich.}

{
 But the young Eliezer was a transhumanist. Giving away IQ points
was going to take more work than if I'd just been born
with extra money. But it was a fixable problem, to be faced up to
squarely, and fixed. Even if it took my whole life.
``The strong exist to serve the
weak,'' wrote the young Eliezer,
``and can only discharge that duty by making others
equally strong.'' I was annoyed with the Randian and
Nietszchean trends in science fiction, and as you may have grasped, the
young Eliezer had a tendency to take things \textit{too far in the
other direction}. No one exists only to serve. But I tried, and I
don't regret that. If you call that teenage folly,
it's rare to see adult wisdom doing better.}

{
 Everyone needed more intelligence. Including me, I was careful to
pronounce. Be it far from me to declare a new world order with myself
on top---that was what a stereotyped science fiction villain would do,
or worse, a typical teenager, and I would never have allowed myself to
be so clichéd. No, \textit{everyone} needed to be smarter. We were all
in the same boat: A fine, uplifting thought.}

{
 Eliezer\textsubscript{1995} had read his science fiction. He had
morals, and ethics, and could see the more obvious traps. No screeds on
\textit{Homo novis} for him. No line drawn between himself and others.
No elaborate philosophy to put himself at the top of the heap. It was
too obvious a failure mode. Yes, he was very careful to call himself
stupid too, and never claim moral superiority. Well, and I
don't see it so differently now, though I no longer
make such a dramatic production out of my ethics. (Or maybe it would be
more accurate to say that I'm tougher about when I
allow myself a moment of self-congratulation.)}

{
 I say all this to emphasize that Eliezer\textsubscript{1995}
wasn't so undignified as to fail in any
\textit{obvious} way.}

{
 And then Eliezer\textsubscript{1996} encountered the concept of
intelligence explosion. Was it a thunderbolt of revelation? Did I jump
out of my chair and shout
``Eurisko!''? Nah. I
wasn't that much of a drama queen. It was just
massively obvious in retrospect that smarter-than-human intelligence
was going to change the future more fundamentally than any mere
material science. And I knew at once that \textit{this} was what I
would be doing with the rest of my life, creating the intelligence
explosion. Not nanotechnology like I'd thought when I
was eleven years old; nanotech would only be a tool brought forth of
intelligence. Why, intelligence was even \textit{more} powerful, an
even greater blessing, than I'd realized before.}

{
 Was this a happy death spiral? As it turned out later, yes: that
is, it led to the adoption even of \textit{false} happy beliefs about
intelligence. Perhaps you could draw the line at the point where I
started believing that surely the lightspeed limit would be no barrier
to superintelligence.}

{
 (How my views on intelligence have changed since then \ldots
let's see: When I think of poor hands dealt to humans,
these days, I think first of death and old age.
Everyone's got to have some intelligence level or
other, and the important thing from a fun-theoretic perspective is that
it should ought to \textit{increase} over time, not decrease like now.
Isn't that a clever way of feeling better? But I
don't work so hard now at downplaying my own
intelligence, because that's just another way of
calling attention to it. I'm smart for a human, if the
topic should arise, and how I feel about that is my own business.}

{
 The part about intelligence being the lever that lifts worlds is
the same. Except that intelligence has become less mysterious unto me,
so that I now more clearly see intelligence as something embedded
within physics. Superintelligences may go FTL if it happens to be
permitted by the true physical laws, and if not, then not.
It's not \textit{unthinkable}, but I
wouldn't bet on it.)}

{
 But the real wrong turn came later, at the point where someone
said, ``Hey, how do you know that superintelligence
will be moral? Intelligence has nothing to do with being a good person,
you know---that's what we call wisdom, young
prodigy.''}

{
 And lo, it seemed obvious to the young Eliezer that this was mere
denial. Certainly, his own painstakingly constructed code of ethics had
been put together using his intelligence and resting on his
intelligence as a base. Any fool could see that intelligence had a
great deal to do with ethics, morality, and wisdom; just try explaining
the Prisoner's Dilemma to a chimpanzee, right?}

{
 Surely, then, superintelligence would necessarily imply
supermorality.}

{
 Thus is it said: ``Parents do all the things they
tell their children not to do, which is how they know not to do
them.''}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{My Best and Worst Mistake}

{
 Last chapter I covered the young Eliezer's
affective death spiral around something that he called
``intelligence.''
Eliezer\textsubscript{1996}, or even Eliezer\textsubscript{1999} for
that matter, would have refused to try and put a mathematical
definition---consciously, deliberately refused. Indeed, he would have
been loath to put any definition on
``intelligence'' at all. }

{
 Why? Because there's a standard bait-and-switch
problem in AI, wherein you define
``intelligence'' to mean something
like ``logical reasoning'' or
``the ability to withdraw conclusions when they are no
longer appropriate,'' and then you build a cheap
theorem-prover or an ad-hoc nonmonotonic reasoner, and then say,
``Lo, I have implemented
intelligence!'' People came up with poor definitions
of intelligence---focusing on correlates rather than cores---and then
they chased the surface definition they had written down, forgetting
about, you know, actual \textit{intelligence.} It's not
like Eliezer\textsubscript{1996} was out to build a career in
Artificial Intelligence. He just wanted a mind that would actually be
able to build nanotechnology. So he wasn't tempted to
redefine intelligence for the sake of puffing up a paper.}

{
 Looking back, it seems to me that quite a lot of my mistakes can
be defined in terms of being pushed too far in the other direction by
seeing someone else's stupidity. Having seen attempts
to define ``intelligence'' abused so
often, I refused to define it at all. What if I said that intelligence
was X, and it wasn't \textit{really} X? I knew in an
intuitive sense what I was looking for---something powerful enough to
take stars apart for raw material---and I didn't want
to fall into the trap of being distracted from that by definitions.}

{
 Similarly, having seen so many AI projects brought down by physics
envy---trying to stick with simple and elegant math, and being
constrained to toy systems as a result---I generalized that any math
simple enough to be formalized in a neat equation was probably not
going to work for, you know, \textit{real} intelligence.
``Except for Bayes's
Theorem,'' Eliezer\textsubscript{2000} added; which,
depending on your viewpoint, either mitigates the totality of his
offense, or shows that he should have suspected the entire
generalization instead of trying to add a single exception.}

{
 If you're wondering why
Eliezer\textsubscript{2000} thought such a thing---disbelieved in a
math of intelligence---well, it's hard for me to
remember this far back. It certainly wasn't that I ever
disliked math. If I had to point out a root cause, it would be reading
too few, too popular, and the wrong Artificial Intelligence books.}

{
 But then I didn't think the answers were going to
come from Artificial Intelligence; I had mostly written it off as a
sick, dead field. So it's no wonder that I spent too
little time investigating it. I believed in the cliché about Artificial
Intelligence overpromising. You can fit that into the pattern of
``too far in the opposite
direction''---the field hadn't
delivered on its promises, so I was ready to write it off. As a result,
I didn't investigate hard enough to find the math that
wasn't fake.}

{
 My youthful disbelief in a mathematics of general intelligence was
simultaneously one of my all-time worst mistakes, and one of my
all-time best mistakes.}

{
 Because I disbelieved that there could be any simple answers to
intelligence, I went and I read up on cognitive psychology, functional
neuroanatomy, computational neuroanatomy, evolutionary psychology,
evolutionary biology, and more than one branch of Artificial
Intelligence. When I had what seemed like simple bright ideas, I
didn't stop there, or rush off to try and implement
them, because I knew that even if they were true, even if they were
necessary, they wouldn't be sufficient: intelligence
wasn't supposed to be simple, it wasn't
supposed to have an answer that fit on a T-shirt. It was supposed to be
a big puzzle with lots of pieces; and when you found one piece, you
didn't run off holding it high in triumph, you kept on
looking. Try to build a mind with a single missing piece, and it might
be that nothing interesting would happen.}

{
 I was wrong in thinking that Artificial Intelligence, the academic
field, was a desolate wasteland; and even wronger in thinking that
there couldn't be math of intelligence. But I
don't regret studying e.g. functional neuroanatomy,
even though I \textit{now} think that an Artificial Intelligence should
look nothing like a human brain. Studying neuroanatomy meant that I
went in with the idea that if you broke up a mind into pieces, the
pieces were things like ``visual
cortex'' and
``cerebellum''---rather than
``stock-market trading module'' or
``commonsense reasoning module,''
which is a standard wrong road in AI.}

{
 Studying fields like functional neuroanatomy and cognitive
psychology gave me a very different idea of what minds had to look like
than you would get from just reading AI books---even good AI books.}

{
 When you blank out all the wrong conclusions and wrong
justifications, and just ask what that belief led the young Eliezer to
actually \textit{do \ldots}}

{
 Then the belief that Artificial Intelligence was sick and that the
real answer would have to come from healthier fields outside led him to
study lots of cognitive sciences;}

{
 The belief that AI couldn't have simple answers
led him to not stop prematurely on one brilliant idea, and to
accumulate lots of information;}

{
 The belief that you didn't want to define
intelligence led to a situation in which he studied the problem for a
long time before, years later, he started to propose systematizations.}

{
 This is what I refer to when I say that this is one of my all-time
best mistakes.}

{
 Looking back, years afterward, I drew a very strong moral, to this
effect:}

{
 What you actually end up doing screens off the clever reason why
you're doing it.}

{
 Contrast amazing clever reasoning that leads you to study many
sciences, to amazing clever reasoning that says you
don't need to read all those books. Afterward, when
your amazing clever reasoning turns out to have been stupid,
you'll have ended up in a much better position if your
amazing clever reasoning was of the first type.}

{
 When I look back upon my past, I am struck by the number of
semi-accidental successes, the number of times I did something right
for the wrong reason. From your perspective, you should chalk this up
to the anthropic principle: if I'd fallen into a true
dead end, you probably wouldn't be hearing from me in
this book. From my perspective it remains something of an
embarrassment. My Traditional Rationalist upbringing provided a lot of
directional bias to those ``accidental
successes''---biased me toward rationalizing reasons
to study rather than not study, prevented me from getting completely
lost, helped me recover from mistakes. Still, none of that was the
right action for the right reason, and that's a scary
thing to look back on your youthful history and see. One of my primary
purposes in writing on \textit{Overcoming Bias} is to leave a trail to
where I ended up by accident---to obviate the role that luck played in
my own forging as a rationalist.}

{
 So what makes this one of my all-time worst mistakes? Because
sometimes ``informal'' is another
way of saying ``held to low
standards.'' I had amazing clever reasons why it was
okay for me not to precisely define
``intelligence,'' and certain of my
other terms as well: namely, other people had gone astray by trying to
define it. This was a gate through which sloppy reasoning could enter.}

{
 So should I have jumped ahead and tried to forge an exact
definition right away? No, all the reasons why I knew this was the
wrong thing to do were correct; you can't conjure the
right definition out of thin air if your knowledge is not adequate.}

{
 You can't get to \textit{the} definition of fire
if you don't know about atoms and molecules;
you're better off saying ``that
orangey-bright thing.'' And you do have to be able to
talk about that orangey-bright stuff, even if you can't
say exactly what it is, to investigate fire. But these days I would say
that all reasoning on that level is something that
can't be trusted---rather it's
something you do on the way to knowing better, but you
don't \textit{trust} it, you don't
\textit{put your weight down} on it, you don't draw
firm conclusions from it, no matter how inescapable the informal
reasoning seems.}

{
 The young Eliezer put his weight down on the wrong floor
tile---stepped onto a loaded trap.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Raised in Technophilia}

{
 My father used to say that if the present system had been in place
a hundred years ago, automobiles would have been outlawed to protect
the saddle industry. }

{
 One of my major childhood influences was reading Jerry
Pournelle's \textit{A Step Farther Out}, at the age of
nine. It was Pournelle's reply to Paul Ehrlich and the
Club of Rome, who were saying, in the 1960s and 1970s, that the Earth
was running out of resources and massive famines were only years away.
It was a reply to Jeremy Rifkin's so-called fourth law
of thermodynamics; it was a reply to all the people scared of nuclear
power and trying to regulate it into oblivion.}

{
 I grew up in a world where the lines of demarcation between the
Good Guys and the Bad Guys were pretty clear; not an apocalyptic final
battle, but a battle that had to be fought over and over again, a
battle where you could see the historical echoes going back to the
Industrial Revolution, and where you could assemble the historical
evidence about the actual outcomes.}

{
 On one side were the scientists and engineers
who'd driven all the standard-of-living increases since
the Dark Ages, whose work supported luxuries like democracy, an
educated populace, a middle class, the outlawing of slavery.}

{
 On the other side, those who had once opposed smallpox
vaccinations, anesthetics during childbirth, steam engines, and
heliocentrism: The theologians calling for a return to a perfect age
that never existed, the elderly white male politicians set in their
ways, the special interest groups who stood to lose, and the many to
whom science was a closed book, fearing what they
couldn't understand.}

{
 And trying to play the middle, the pretenders to Deep Wisdom,
uttering cached thoughts about how technology benefits humanity but
only when it was properly regulated---claiming in defiance of brute
historical fact that science of itself was neither good nor
evil---setting up solemn-looking bureaucratic committees to make an
ostentatious display of their caution---and waiting for their applause.
As if the truth were always a compromise. And as if anyone could really
see that far ahead. Would humanity have done better if
there'd been a sincere, concerned, public debate on the
adoption of fire, and committees set up to oversee its use?}

{
 When I entered into the problem, I started out allergized against
anything that pattern-matched ``Ah, but technology has
risks as well as benefits, little one.'' The
presumption-of-guilt was that you were either trying to collect some
cheap applause, or covertly trying to regulate the technology into
oblivion. And either way, ignoring the historical record immensely in
\textit{favor} of technologies that people had once worried about.}

{
 Robin Hanson raised the topic of slow FDA approval of drugs
approved in other countries. Someone in the comments pointed out that
Thalidomide was sold in 50 countries under 40 names, but that only a
small amount was given away in the US, so that there were 10,000
malformed children born globally, but only 17 children in the US.}

{
 But how many people have died because of the slow approval in the
US, of drugs more quickly approved in other countries---all the drugs
that \textit{didn't} go wrong? And I ask that question
because it's what you can try to collect statistics
about---this says nothing about all the drugs that were never
\textit{developed} because the approval process is too long and costly.
According to this source, the FDA's longer approval
process prevents 5,000 casualties per year by screening off medications
found to be harmful, and causes at least 20,000--120,000 casualties per
year just by delaying approval of those beneficial medications that are
still developed and eventually approved.}

{
 So there really is a reason to be allergic to people who go around
saying, ``Ah, but technology has risks as well as
benefits.'' There's a historical
record showing over-conservativeness, the many silent deaths of
regulation being outweighed by a few visible deaths of nonregulation.
If you're \textit{really} playing the middle, why not
say, ``Ah, but technology has benefits as well as
risks''?}

{
 Well, and this isn't such a bad description of the
Bad Guys. (Except that it ought to be emphasized a bit harder that
these aren't evil mutants but standard human beings
acting under a different worldview-gestalt that puts them in the right;
some of them will inevitably be more competent than others, and
competence counts for a lot.) Even looking back, I
don't think my childhood technophilia was too wrong
about what constituted a Bad Guy and what was the key mistake. But
it's always a \textit{lot} easier to say what
\textit{not} to do, than to get it \textit{right}. And one of my
fundamental flaws, back then, was thinking that if you tried as hard as
you could to avoid everything the Bad Guys were doing, that made you a
Good Guy.}

{
 Particularly damaging, I think, was the bad example set by the
pretenders to Deep Wisdom trying to stake out a middle way; smiling
condescendingly at technophiles and technophobes alike, and calling
them both immature. Truly this is \textit{a} wrong way; and in fact,
the notion of trying to stake out a middle way generally, is usually
wrong. The Right Way is not a compromise with anything; it is the clean
manifestation of its own criteria.}

{
 But that made it more difficult for the young Eliezer to depart
from the charge-straight-ahead verdict, because \textit{any} departure
felt like joining the pretenders to Deep Wisdom.}

{
 The first crack in my childhood technophilia appeared in, I think,
1997 or 1998, at the point where I noticed my fellow technophiles
saying foolish things about how molecular nanotechnology would be an
easy problem to manage. (As you may be noticing yet again, the young
Eliezer was driven to a tremendous extent by his ability to find
flaws---I even had a personal philosophy of why that sort of thing was
a good idea.)}

{
 There was a debate going on about molecular nanotechnology, and
whether offense would be asymmetrically easier than defense. And there
were people arguing that defense would be easy. In the domain of
\textit{nanotech}, for Ghu's sake, programmable matter,
when we can't even seem to get the security problem
solved for computer networks where we can observe and control every one
and zero. People were talking about unassailable diamondoid walls. I
observed that diamond doesn't stand off a nuclear
weapon, that offense has had defense beat since 1945 and nanotech
didn't look likely to change that.}

{
 And by the time that debate was over, it seems that the young
Eliezer---caught up in the heat of argument---had managed to notice,
for the first time, that the survival of Earth-originating intelligent
life stood at risk.}

{
 It seems so strange, looking back, to think that there was a time
when I thought that only individual lives were at stake in the future.
What a profoundly friendlier world that was to live in \ldots though
it's not as if I were thinking that at the time. I
didn't \textit{reject} the possibility so much as
\textit{manage to never see it in the first place.} Once the topic
actually came up, I saw it. I don't really remember how
that trick worked. There's a reason why I refer to my
past self in the third person.}

{
 It may sound like Eliezer\textsubscript{1998} was a complete
idiot, but that would be a comfortable out, in a way; the truth is
scarier. Eliezer\textsubscript{1998} was a sharp Traditional
Rationalist, as such things went. I knew hypotheses had to be testable,
I knew that rationalization was not a permitted mental operation, I
knew how to play Rationalist's Taboo, I was obsessed
with self-awareness \ldots I didn't quite understand the
concept of ``mysterious answers''
\ldots and no Bayes or Kahneman at all. But a sharp Traditional
Rationalist, far above average \ldots So what? Nature
isn't grading us on a curve. One step of departure from
the Way, one shove of undue influence on your thought processes, can
repeal all other protections.}

{
 One of the chief lessons I derive from looking back at my personal
history is that it's no wonder that, out there in the
real world, a lot of people think that ``intelligence
isn't everything,'' or that
rationalists don't do better in real life. A little
rationality, or even a lot of rationality, doesn't pass
the astronomically high barrier required for things to actually start
\textit{working.}}

{
 Let not my misinterpretation of the Right Way be blamed on Jerry
Pournelle, my father, or science fiction generally. I think the young
Eliezer's personality imposed quite a bit of
selectivity on which parts of their teachings made it through.
It's not as if Pournelle didn't say:
\textit{The rules change once you leave Earth, the cradle; if
you're careless sealing your pressure suit just once,
you die.} He said it quite a bit. But the words didn't
really seem important, because that was something that happened to
third-party characters in the novels---the main character
didn't usually die halfway through, for some reason.}

{
 What was the lens through which I filtered these teachings? Hope.
Optimism. Looking forward to a brighter future. That was the
fundamental meaning of \textit{A Step Farther Out} unto me, the lesson
I took in contrast to the Sierra Club's doom-and-gloom.
On one side was \textit{rationality and hope}, the other,
\textit{ignorance and despair}.}

{
 Some teenagers think they're immortal and ride
motorcycles. I was under no such illusion and quite reluctant to learn
to drive, considering how unsafe those hurtling hunks of metal looked.
But there was something more important to me than my own life: The
Future. And I acted as if \textit{that} were immortal. Lives could be
lost, but not the Future.}

{
 And when I noticed that nanotechnology really \textit{was} going
to be a potentially extinction-level challenge?}

{
 The young Eliezer thought, explicitly, ``Good
heavens, how did I fail to notice this thing that should have been
obvious? I must have been too emotionally attached to the benefits I
expected from the technology; I must have flinched away from the
thought of human extinction.''}

{
 And then \ldots}

{
 I didn't declare a Halt, Melt, and Catch Fire. I
didn't rethink all the conclusions that
I'd developed with my prior attitude. I just managed to
integrate it into my worldview, \textit{somehow,} with a minimum of
propagated changes. Old ideas and plans were challenged, but my mind
found reasons to keep them. There was no systemic breakdown,
unfortunately.}

{
 Most notably, I decided that we had to run full steam ahead on AI,
so as to develop it before nanotechnology. Just like
I'd been \textit{originally} planning to do, but now,
with a \textit{different reason.}}

{
 I guess that's what most human beings are like,
isn't it? Traditional Rationality
wasn't enough to change that.}

{
 But there did come a time when I fully realized my mistake. It
just took a stronger boot to the head.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{A Prodigy of Refutation}

{
 My Childhood Death Spiral described the core momentum carrying me
into my mistake, an affective death spiral around something that
Eliezer\textsubscript{1996} called
``intelligence.'' I was also a
technophile, pre-allergized against fearing the future. And
I'd read a lot of science fiction built around
personhood ethics---in which fear of the Alien puts humanity-at-large
in the position of the bad guys, mistreating aliens or sentient AIs
because they ``aren't
human.'' }

{
 That's part of the ethos you acquire from science
fiction---to define your in-group, your tribe, appropriately broadly.
Hence my email address, sentience@pobox.com.}

{
 So Eliezer\textsubscript{1996} is out to build superintelligence,
for the good of humanity and all sentient life.}

{
 At first, I think, the question of whether a superintelligence
will/could be good/evil didn't really occur to me as a
separate topic of discussion. Just the standard intuition of,
``Surely no supermind would be stupid enough to turn
the galaxy into paperclips; surely, being so intelligent, it will also
know what's \textit{right} far better than a human
being could.''}

{
 Until I introduced myself and my quest to a transhumanist mailing
list, and got back responses along the general lines of (from memory):}

{
 Morality is arbitrary---if you say that something is good or bad,
you can't be right or wrong about that. A
superintelligence would form its own morality.}

{
 Everyone ultimately looks after their own self-interest. A
superintelligence would be no different; it would just seize all the
resources.}

{
 Personally, I'm a human, so I'm in
favor of humans, not Artificial Intelligences. I don't
think we should develop this technology. Instead we should develop the
technology to upload humans first.}

{
 No one should develop an AI without a control system that watches
it and makes sure it can't do anything bad.}

{
 Well, \textit{that's} all obviously wrong, thinks
Eliezer\textsubscript{1996}, and he proceeded to kick his
opponents' arguments to pieces. (I've
mostly done this in other essays, and anything remaining is left as an
exercise to the reader.)}

{
 It's not that Eliezer\textsubscript{1996}
explicitly reasoned, ``The world's
stupidest man says the Sun is shining, \textit{therefore} it is dark
out.'' But Eliezer\textsubscript{1996} was a
Traditional Rationalist; he had been inculcated with the metaphor of
science as a \textit{fair fight} between sides who take on different
positions, stripped of mere violence and other such exercises of
political muscle, so that, ideally, the side with the best arguments
can win.}

{
 It's easier to say where someone
else's argument is wrong, then to get the fact of the
matter right; and Eliezer\textsubscript{1996} was \textit{very skilled}
at finding flaws. (So am I. It's not as if you can
solve the danger of that power by refusing to care about flaws.) From
Eliezer\textsubscript{1996}'s perspective, it seemed to
him that his chosen side was \textit{winning the fight}{}---that he was
formulating better arguments than his opponents---so why would he
switch sides?}

{
 Therefore is it written: ``Because this world
contains many whose grasp of rationality is abysmal, beginning students
of rationality win arguments and acquire an exaggerated view of their
own abilities. But it is useless to be superior: Life is not graded on
a curve. The best physicist in ancient Greece could not calculate the
path of a falling apple. There is no guarantee that adequacy is
possible given your hardest effort; therefore spare no thought for
whether others are doing worse.''}

{
 You cannot rely on \textit{anyone} else to argue you out of your
mistakes; you cannot rely on \textit{anyone} else to save you; you and
\textit{only} you are obligated to find the flaws in your positions; if
you put that burden down, don't expect anyone else to
pick it up. And I wonder if that advice will turn out not to help most
people, until they've personally blown off their own
foot, saying to themselves all the while, \textit{correctly},
``Clearly I'm winning this
argument.''}

{
 Today I try not to take any human being as my opponent. That just
leads to overconfidence. It is Nature that I am facing off against, who
does not match Her problems to your skill, who is not obliged to offer
you a fair chance to win in return for a diligent effort, who does not
care if you are the best who ever lived, if you are not good
\textit{enough.}}

{
 But return to 1996. Eliezer\textsubscript{1996} is going with the
basic intuition of ``Surely a superintelligence will
know better than we could what is \textit{right},''
and offhandedly knocking down various arguments brought against his
position. He was skillful in that way, you see. He even had a personal
philosophy of why it was wise to look for flaws in things, and so on.}

{
 I don't mean to say it as an excuse, that no one
who argued against Eliezer\textsubscript{1996} actually presented him
with the dissolution of the mystery---the full reduction of morality
that analyzes all his cognitive processes debating
``morality,'' a step-by-step
walkthrough of the algorithms that make morality feel to him like a
fact. Consider it rather as an indictment, a measure of
Eliezer\textsubscript{1996}'s level, that he would have
needed the full solution given to him, in order to present him with an
argument that he could \textit{not} refute.}

{
 The few philosophers present did not extract him from his
difficulties. It's not as if a philosopher will say,
``Sorry, morality is understood, it is a settled issue
in cognitive science and philosophy, and your viewpoint is simply
wrong.'' The nature of morality is still an open
question in philosophy; the debate is still going on. A philosopher
will feel obligated to present you with a list of classic arguments on
all sides---most of which Eliezer\textsubscript{1996} is quite
intelligent enough to knock down, and so he concludes that philosophy
is a wasteland.}

{
 But wait. It gets worse.}

{
 I don't recall exactly when---it might have been
1997---but the younger me, let's call him
Eliezer\textsubscript{1997}, set out to argue \textit{inescapably} that
creating superintelligence is the right thing to do.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{The Sheer Folly of Callow Youth}

{
 There speaks the sheer folly of callow youth; the rashness of an
ignorance so abysmal as to be possible only to one of your ephemeral
race \ldots}

{\raggedleft
 {}---Gharlane of Eddore\textsuperscript{1}
\par}


\bigskip

{
 ~}

{
 Once upon a time, years ago, I propounded a mysterious answer to a
mysterious question---as I've hinted on several
occasions. The mysterious question to which I propounded a mysterious
answer was not, however, consciousness---or rather, not only
consciousness. No, the more embarrassing error was that I took a
mysterious view of morality.}

{
 I held off on discussing that until now, after the series on
metaethics, because I wanted it to be clear that
Eliezer\textsubscript{1997} \textit{had} gotten it wrong.}

{
 When we last left off, Eliezer\textsubscript{1997}, not satisfied
with arguing in an intuitive sense that superintelligence would be
moral, was setting out to argue \textit{inescapably} that creating
superintelligence was the right thing to do.}

{
 Well (said Eliezer\textsubscript{1997}) let's
begin by asking the question: \textit{Does life have, in fact, any
meaning?}}

{
 ``I don't
know,'' replied Eliezer\textsubscript{1997} at once,
with a certain note of self-congratulation for admitting his own
ignorance on this topic where so many others seemed certain.}

{
 ``But,'' he went on---}

{
 (Always be wary when an admission of ignorance is followed by
``But.'')}

{
 ``But, if we suppose that life has no
meaning---that the utility of all outcomes is equal to zero---that
possibility cancels out of any \textit{expected} utility calculation.
We can therefore always \textit{act as if} life is known to be
meaningful, even though we don't know what that meaning
is. How can we find out that meaning? Considering that humans are still
arguing about this, it's probably too difficult a
problem for humans to solve. So we need a superintelligence to solve
the problem for us. As for the possibility that there \textit{is} no
logical justification for one preference over another, then in this
case it is no righter or wronger to build a superintelligence, than to
do anything else. This is a real possibility, but it falls out of any
attempt to calculate expected utility---we should just ignore it. To
the extent someone says that a superintelligence would wipe out
humanity, they are either arguing that wiping out humanity is in fact
the right thing to do (even though we see no reason why this should be
the case) or they are arguing that there \textit{is} no right thing to
do (in which case their argument that we should not build intelligence
defeats itself).''}

{
 Ergh. That was a \textit{really} difficult paragraph to write. My
past self is always my own most concentrated Kryptonite, because my
past self is \textit{exactly precisely} all those things that the
modern me has installed allergies to block. Truly is it said that
parents do all the things they tell their children not to do, which is
how they know not to do them; it applies between past and future selves
as well.}

{
 How flawed is Eliezer\textsubscript{1997}'s
argument? I couldn't even count the ways. I know memory
is fallible, reconstructed each time we recall, and so I
don't trust my assembly of these old pieces using my
modern mind. Don't ask me to read my old writings;
that's too much pain.}

{
 But it seems clear that I was thinking of utility as a sort of
stuff, an inherent property. So that ``life is
meaningless'' corresponded to utility = 0. But of
course the argument works equally well with utility = 100, so that if
everything is meaningful but it is all \textit{equally} meaningful,
that should fall out too \ldots Certainly I wasn't then
thinking of a utility function as an affine structure in preferences. I
was thinking of ``utility'' as an
absolute level of inherent value.}

{
 I was thinking of \textit{should} as a kind of purely abstract
essence of compellingness, that-which-makes-you-do-something; so that
clearly any mind that derived a \textit{should} would be bound by it.
Hence the assumption, which Eliezer\textsubscript{1997} did not even
think to explicitly note, that a logic that compels an arbitrary mind
to do something is exactly the same as that which human beings mean and
refer to when they utter the word
``right'' \ldots}

{
 But now I'm trying to count the ways, and if
you've been following along, you should be able to
handle that yourself.}

{
 An important aspect of this whole failure was that, because
I'd proved that the case ``life is
meaningless'' \textit{wasn't worth
considering}, I didn't think it was necessary to
rigorously define ``intelligence''
or ``meaning.'' I'd
previously come up with a clever reason for not trying to go all formal
and rigorous when trying to define
``intelligence'' (or
``morality'')---namely all the
bait-and-switches that past AI folk, philosophers, and moralists had
pulled with definitions that missed the point.}

{
 I draw the following lesson: No matter how clever the
justification for relaxing your standards, or evading some requirement
of rigor, it will blow your foot off just the same.}

{
 And another lesson: I was skilled in refutation. If
I'd applied the same level of
rejection-based-on-any-flaw to my own position as I used to defeat
arguments brought against me, then I would have zeroed in on the
logical gap and rejected the position---if I'd
\textit{wanted} to. If I'd had the same level of
prejudice against it as I'd had against other positions
in the debate.}

{
 But this was before I'd heard of Kahneman, before
I'd heard the term ``motivated
skepticism,'' before I'd integrated
the concept of an exactly correct state of uncertainty that summarizes
all the evidence, and before I knew the deadliness of asking
``Am I allowed to believe?'' for
liked positions and ``Am I forced to
believe?'' for disliked positions. I was a mere
Traditional Rationalist who thought of the scientific process as a
referee between people who took up positions and argued them, may the
best side win.}

{
 My ultimate flaw was not a liking for
``intelligence,'' nor any amount of
technophilia and science fiction exalting the siblinghood of sentience.
It surely wasn't my ability to spot flaws. None of
these things \textit{could} have led me astray, if I had held myself to
a higher standard of rigor throughout, and adopted no position
otherwise. Or even if I'd just scrutinized my preferred
vague position, with the same demand-of-rigor I applied to
counterarguments.}

{
 But I wasn't much interested in trying to refute
my belief that life had meaning, since my reasoning would always be
dominated by cases where life did have meaning.}

{
 And with the intelligence explosion at stake, I thought I just had
to proceed at all speed using the best concepts I could wield at the
time, not pause and shut down everything while I looked for a perfect
definition that so many others had screwed up \ldots}

{
 No.}

{
 No, you don't use the best concepts you can use at
the time.}

{
 It's Nature that judges you, and Nature does not
accept \textit{even the most righteous excuses}. If you
don't meet the standard, you fail. It's
that simple. There is no clever argument for why you have to make do
with what you have, because Nature won't listen to that
argument, won't forgive you because there were so many
excellent justifications for speed.}

{
 We all know what happened to Donald Rumsfeld, when he went to war
with the army he had, instead of the army he needed.}

{
 Maybe Eliezer\textsubscript{1997} couldn't have
conjured the correct model out of thin air. (Though who knows what
would have happened, if he'd really tried \ldots) And it
wouldn't have been prudent for him to stop thinking
entirely, until rigor suddenly popped out of nowhere.}

{
 But neither was it correct for Eliezer\textsubscript{1997} to put
his weight down on his ``best
guess,'' in the absence of precision. You can use
vague concepts in your own interim thought processes, as you search for
a better answer, unsatisfied with your current vague hints, \textit{and
unwilling to put your weight down on them.} You don't
build a superintelligence based on an interim understanding. No, not
even the ``best'' vague
understanding you have. That was my mistake---thinking that saying
``best guess'' excused anything.
There was only the standard I had failed to meet.}

{
 Of course Eliezer\textsubscript{1997} didn't want
to slow down on the way to the intelligence explosion, with so many
lives at stake, and the very survival of Earth-originating intelligent
life, if we got to the era of nanoweapons before the era of
superintelligence---}

{
 Nature doesn't care about such righteous reasons.
There's just the astronomically high standard needed
for success. Either you match it, or you fail. That's
all.}

{
 The apocalypse does not need to be fair to you.}

{
 The apocalypse does not need to offer you a chance of success}

{
 In exchange for what you've already brought to the
table.}

{
 The apocalypse's difficulty is not matched to your
skills.}

{
 The apocalypse's price is not matched to your
resources.}

{
 If the apocalypse asks you for something unreasonable}

{
 And you try to bargain it down a little}

{
 (Because everyone has to compromise now and then)}

{
 The apocalypse will not try to negotiate back up.}

{
 And, oh yes, it gets worse.}

{
 How did Eliezer\textsubscript{1997} deal with the obvious argument
that you couldn't possibly derive an
``ought'' from pure logic, because
``ought'' statements could only be
derived from other ``ought''
statements?}

{
 Well (observed Eliezer\textsubscript{1997}), this problem has the
same structure as the argument that a cause only proceeds from another
cause, or that a real thing can only come of another real thing,
whereby you can prove that nothing exists.}

{
 Thus (he said) there are three ``hard
problems'': the hard problem of conscious experience,
in which we see that qualia cannot arise from computable processes; the
hard problem of existence, in which we ask how any existence enters
apparently from nothingness; and the hard problem of morality, which is
to get to an ``ought.''}

{
 These problems are probably linked. For example, the qualia of
pleasure are one of the best candidates for something intrinsically
desirable. We might not be able to understand the hard problem of
morality, therefore, without unraveling the hard problem of
consciousness. It's evident that these problems are too
hard for humans---otherwise someone would have solved them over the
last 2,500 years since philosophy was invented.}

{
 It's not as if they could have complicated
solutions---they're too simple for that. The problem
must just be outside human concept-space. Since we can see that
consciousness can't arise on any computable process, it
must involve new physics---physics that our brain uses, but
can't understand. That's why we need
superintelligence in order to solve this problem. Probably it has to do
with quantum mechanics, maybe with a dose of tiny closed timelike
curves from out of General Relativity; temporal paradoxes might have
some of the same irreducibility properties that consciousness seems to
demand \ldots}

{
 Et cetera, ad nauseam. You may begin to perceive, in the arc of my
\textit{Overcoming Bias} posts, the letter I wish I could have written
to myself.}

{
 Of this I learn the lesson: You cannot manipulate confusion. You
cannot make clever plans to work around the holes in your
understanding. You can't even make
``best guesses'' about things which
fundamentally confuse you, and relate them to other confusing things.
Well, you can, but you won't get it right, until your
confusion dissolves. Confusion exists in the mind, not in the reality,
and trying to treat it like something you can pick up and move around
will only result in unintentional comedy.}

{
 Similarly, you cannot come up with clever reasons why the gaps in
your model don't matter. You cannot draw a border
around the mystery, put on neat handles that let you use the Mysterious
Thing without really understanding it---like my attempt to make the
possibility that life is meaningless cancel out of an expected utility
formula. You can't pick up the gap and manipulate it.}

{
 If the blank spot on your map conceals a land mine, then putting
your weight down on that spot will be fatal, no matter how good your
excuse for not knowing. Any black box could contain a trap, and
there's no way to know except opening up the black box
and looking inside. If you come up with some righteous justification
for why you need to rush on ahead with the best understanding you
have---the trap goes off.}

{
 It's only when you know the rules,}

{
 That you realize \textit{why} you needed to learn;}

{
 What would have happened otherwise,}

{
 How \textit{much} you needed to know.}

{
 Only knowledge can foretell the cost of ignorance. The ancient
alchemists had no logical way of knowing the exact reasons why it was
hard for them to turn lead into gold. So they poisoned themselves and
died. Nature doesn't care.}

{
 But there did come a time when realization began to dawn on me.}

{\centering
 \ ~
\par}

{\centering
 *
\par}


\bigskip

{
 1. Edward Elmer Smith, \textit{Second Stage Lensmen} (Old Earth
Books, 1998).}

\mysection{That Tiny Note of Discord}

{
 When we last left Eliezer\textsubscript{1997}, he believed that
any superintelligence would automatically do what was
``right,'' and indeed would
understand that better than we could---even though, he modestly
confessed, he did not understand the ultimate nature of morality. Or
rather, after some debate had passed, Eliezer\textsubscript{1997} had
evolved an elaborate argument, which he fondly claimed to be
``formal,'' that we could always
condition upon the belief that life has meaning; and so cases where
superintelligences did not feel compelled to do anything in particular
would fall out of consideration. (The flaw being the unconsidered and
unjustified equation of ``universally compelling
argument'' with
``right.'') }

{
 So far, the young Eliezer is well on the way toward joining the
``smart people who are stupid because
they're skilled at defending beliefs they arrived at
for unskilled reasons'' club. All his dedication to
``rationality'' has not saved him
from this mistake, and you might be tempted to conclude that it is
useless to strive for rationality.}

{
 But while many people dig holes for themselves, not everyone
succeeds in clawing their way back out.}

{
 And from this I learn my lesson: That it all began---}

{
 {}---with a small, small question; a single discordant note; one
tiny lonely thought \ldots}

{
 As our story starts, we advance three years to
Eliezer\textsubscript{2000}, who in most respects resembles his self of
1997. He currently thinks he's proven that building a
superintelligence is the right thing to do if there is any right thing
at all. From which it follows that there is no \textit{justifiable}
conflict of interest over the intelligence explosion among the peoples
and persons of Earth.}

{
 This is an important conclusion for Eliezer\textsubscript{2000},
because he finds the notion of fighting over the intelligence explosion
to be \textit{unbearably} stupid. (Sort of like the notion of God
intervening in fights between tribes of bickering barbarians, only in
reverse.) Eliezer\textsubscript{2000}'s self-concept
does not permit him---he doesn't even
\textit{want}{}---to shrug and say, ``Well, my side
got here first, so we're going to seize the banana
before anyone else gets it.'' It's a
thought too painful to think.}

{
 And yet then the notion occurs to him:}

{
 Maybe some people would prefer an AI do particular things, such as
not kill them, even if life is meaningless?}

{
 His immediately following thought is the obvious one, given his
premises:}

{
 In the event that life is meaningless, nothing is the
``right'' thing to do; therefore it
wouldn't be particularly right to respect
people's preferences in this event.}

{
 This is the obvious dodge. The thing is, though,
Eliezer\textsubscript{2000} doesn't think of himself as
a villain. He doesn't go around saying,
``What bullets shall I dodge
today?'' He thinks of himself as a dutiful
rationalist who tenaciously follows lines of inquiry. Later,
he's going to look back and see a whole lot of
inquiries that his mind somehow managed to not follow---but
that's not his \textit{current self-concept}.}

{
 So Eliezer\textsubscript{2000} \textit{doesn't}
just grab the obvious out. He keeps thinking.}

{
 But if people believe they have preferences in the event that life
is meaningless, then they have a motive to dispute my intelligence
explosion project and go with a project that respects their wish in the
event life is meaningless. This creates a present conflict of interest
over the intelligence explosion, and prevents right things from getting
done in the mainline event that life is meaningful.}

{
 Now, there's a \textit{lot} of excuses
Eliezer\textsubscript{2000} could have potentially used to toss this
problem out the window. I know, because I've
\textit{heard} plenty of excuses for dismissing Friendly AI.
``The problem is too hard to solve''
is one I get from AGI wannabes who imagine themselves smart enough to
create true Artificial Intelligence, but not smart enough to solve a
really difficult problem like Friendly AI. Or
``worrying about this possibility would be a poor use
of resources, what with the incredible urgency of creating AI before
humanity wipes itself out---you've got to go with what
you have,'' this being uttered by people who just
basically aren't interested in the problem.}

{
 But Eliezer\textsubscript{2000} is a \textit{perfectionist.}
He's not perfect, obviously, and he
doesn't attach as much importance as I do to the virtue
of \textit{precision}, but he is most certainly a
\textit{perfectionist.} The idea of metaethics that
Eliezer\textsubscript{2000} espouses, in which superintelligences know
what's right better than we do, previously seemed to
wrap up \textit{all} the problems of justice and morality in an
airtight wrapper.}

{
 The new objection seems to poke a minor hole in the airtight
wrapper. This is worth patching. If you have something
that's perfect, are you really going to let one little
possibility compromise it?}

{
 So Eliezer\textsubscript{2000} doesn't even
\textit{want} to drop the issue; he wants to patch the problem and
restore perfection. How can he justify spending the time? By thinking
thoughts like:}

{
 What about Brian Atkins? [Brian Atkins being the startup funder of
the Machine Intelligence Research Institute, then called the
Singularity Institute.] He would probably prefer not to die, even if
life were meaningless. He's paying for MIRI right now;
I don't want to taint the ethics of our cooperation.}

{
 Eliezer\textsubscript{2000}'s sentiment
doesn't translate very well---English
doesn't have a simple description for it, or any other
culture I know. Maybe the passage in the Old Testament,
``Thou shalt not boil a young goat in its
mother's milk.'' Someone who helps
you out of altruism shouldn't regret helping you; you
owe them, not so much fealty, but rather, that they're
actually doing what they think they're doing by helping
you.}

{
 Well, but how would Brian Atkins find out, if I
don't tell him? Eliezer\textsubscript{2000}
doesn't even \textit{think} this except in quotation
marks, as the obvious thought that a villain would think in the same
situation. And Eliezer\textsubscript{2000} has a standard
counter-thought ready too, a ward against temptations to
dishonesty---an argument that justifies honesty in terms of expected
utility, not just a personal love of personal virtue:}

{
 Human beings aren't perfect deceivers;
it's likely that I'll be found out. Or
what if genuine lie detectors are invented before the Singularity,
sometime over the next thirty years? I wouldn't be able
to pass a lie detector test.}

{
 Eliezer\textsubscript{2000} lives by the rule that you should
always be ready to have your thoughts broadcast to the whole world at
any time, without embarrassment. Otherwise, clearly,
you've fallen from grace: either you're
thinking something you shouldn't be thinking, or
you're embarrassed by something that
shouldn't embarrass you.}

{
 (These days, I don't espouse quite such an extreme
viewpoint, mostly for reasons of Fun Theory. I see a role for continued
social competition between intelligent life-forms, as least as far as
my near-term vision stretches. I admit, these days, that it might be
all right for human beings to have a self; as John McCarthy put it,
``If everyone were to live for others all the time,
life would be like a procession of ants following each other around in
a circle.'' If you're going to have a
self, you may as well have secrets, and maybe even conspiracies. But I
do still try to abide by the principle of being able to pass a future
lie detector test, with anyone else who's also willing
to go under the lie detector, if the topic is a professional one. Fun
Theory needs a commonsense exception for global catastrophic risk
management.)}

{
 Even taking honesty for granted, there are other excuses
Eliezer\textsubscript{2000} could use to flush the question down the
toilet. ``The world doesn't have the
time'' or ``It's
unsolvable'' would still work. But
Eliezer\textsubscript{2000} doesn't \textit{know} that
this problem, the ``backup''
morality problem, is going to be particularly difficult or
time-consuming. He's just now thought of the whole
issue.}

{
 And so Eliezer\textsubscript{2000} \textit{begins} to really
consider the question: Supposing that ``life is
meaningless'' (that superintelligences
\textit{don't} produce their own motivations from pure
logic), then how would you go about specifying a \textit{fallback}
morality? Synthesizing it, inscribing it into the AI?}

{
 There's a lot that Eliezer\textsubscript{2000}
doesn't know, at this point. But he \textit{has} been
thinking about self-improving AI for three years, and
he's been a Traditional Rationalist for longer than
that. There are techniques of rationality that he \textit{has}
practiced, methodological safeguards he's already
devised. He already knows better than to think that all an AI needs is
the One Great Moral Principle. Eliezer\textsubscript{2000} already
knows that it is wiser to think technologically than politically. He
already knows the saying that AI programmers are supposed to think in
code, to use concepts that can be inscribed in a computer.
Eliezer\textsubscript{2000} already has a concept that there is
something called ``technical
thinking'' and it is good, though he
hasn't yet formulated a Bayesian view of it. And
he's long since noticed that suggestively named LISP
tokens don't really mean anything, et cetera. These
injunctions prevent him from falling into some of the initial traps,
the ones that I've seen consume other novices on their
own first steps into the Friendly AI problem \ldots though technically
this was my \textit{second} step; I well and truly failed on my first.}

{
 But in the end, what it comes down to is this: For the first time,
Eliezer\textsubscript{2000} is trying to think technically about
inscribing a morality into an AI, without the escape-hatch of the
mysterious essence of rightness.}

{
 That's the only thing that matters, in the end.
His previous philosophizing wasn't enough to force his
brain to confront the details. This new standard is strict enough to
require actual work. Morality slowly starts being less mysterious to
him---Eliezer\textsubscript{2000} is starting to think \textit{inside}
the black box.}

{
 His \textit{reasons} for pursuing this course of action---those
don't matter at all.}

{
 Oh, there's a lesson in his being a perfectionist.
There's a lesson in the part about how
Eliezer\textsubscript{2000} initially thought this was a tiny flaw, and
could have dismissed it out-of-mind if that had been his impulse.}

{
 But in the end, the chain of cause and effect goes like this:
Eliezer\textsubscript{2000} investigated in more detail, therefore he
got better with practice. Actions screen off justifications. If your
arguments happen to justify not working things out in detail, like
Eliezer\textsubscript{1996}, then you won't get good at
thinking about the problem. If your arguments call for you to work
things out in detail, then you have an \textit{opportunity} to start
accumulating expertise.}

{
 That was the only choice that mattered, in the end---not the
\textit{reasons} for doing anything.}

{
 I say all this, as you may well guess, because of the AI wannabes
I sometimes run into who have their own clever reasons for not thinking
about the Friendly AI problem. Our clever reasons for doing what we do
tend to matter a lot less to Nature than they do to ourselves and our
friends. If your actions don't look good when
they're stripped of all their justifications and
presented as mere brute facts \ldots then maybe you should re-examine
them.}

{
 A diligent effort won't always save a person.
There is such a thing as lack of ability. Even so, if you
don't try, or don't try hard enough,
you don't get a chance to sit down at the high-stakes
table---never mind the ability ante. That's cause and
effect for you.}

{
 Also, perfectionism really matters. The end of the world
doesn't always come with trumpets and thunder and the
highest priority in your inbox. Sometimes the shattering truth first
presents itself to you as a small, small question; a single discordant
note; one tiny lonely thought, that you could dismiss with one easy
effortless touch \ldots}

{
 \ldots and so, over succeeding years, understanding begins to dawn
on that past Eliezer, slowly. That Sun rose slower than it could have
risen.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Fighting a Rearguard Action Against the Truth}

{
 When we last left Eliezer\textsubscript{2000}, he was just
beginning to investigate the question of how to inscribe a morality
into an AI. His reasons for doing this don't matter at
all, except insofar as they happen to historically demonstrate the
importance of perfectionism. If you practice something, you may get
better at it; if you investigate something, you may find out about it;
the only thing that matters is that Eliezer\textsubscript{2000} is, in
fact, focusing his full-time energies on thinking technically about AI
morality---rather than, as previously, finding any justification for
not spending his time this way. In the end, this is all that turns out
to matter. }

{
 But as our story begins---as the sky lightens to gray and the tip
of the Sun peeks over the horizon---Eliezer\textsubscript{2001}
hasn't yet admitted that Eliezer\textsubscript{1997}
was \textit{mistaken} in any important sense. He's just
making Eliezer\textsubscript{1997}'s strategy
\textit{even better} by including a \textit{contingency} plan for
``the unlikely event that life turns out to be
meaningless'' \ldots}

{
 \ldots which means that Eliezer\textsubscript{2001} now has a line
of retreat away from his mistake.}

{
 I don't just mean that Eliezer\textsubscript{2001}
can say ``Friendly AI is a contingency
plan,'' rather than screaming
``OOPS!''}

{
 I mean that Eliezer\textsubscript{2001} now actually \textit{has}
a contingency plan. If Eliezer\textsubscript{2001} starts to doubt his
1997 metaethics, the intelligence explosion has a fallback strategy,
namely Friendly AI. Eliezer\textsubscript{2001} can question his
metaethics without it signaling the end of the world.}

{
 And his gradient has been smoothed; he can admit a 10\% chance of
having previously been wrong, then a 20\% chance. He
doesn't have to cough out his whole mistake in one huge
lump.}

{
 If you think this sounds like Eliezer\textsubscript{2001} is too
slow, I quite agree.}

{
 Eliezer\textsubscript{1996--2000}'s strategies had
been formed in the total absence of ``Friendly
AI'' as a consideration. The whole idea was to get a
superintelligence, \textit{any} superintelligence, as fast as
possible---codelet soup, ad-hoc heuristics, evolutionary programming,
open-source, anything that looked like it might work---preferably all
approaches simultaneously in a Manhattan Project.
(``All parents did the things they tell their children
not to do. That's how they know to tell them not to do
it.''\textsuperscript{1}) It's not as
if adding one more approach could \textit{hurt.}}

{
 His attitudes toward technological progress have been formed---or
more accurately, preserved from childhood-absorbed
technophilia---around the assumption that any/all movement toward
superintelligence is a pure good without a hint of danger.}

{
 Looking back, what Eliezer\textsubscript{2001} \textit{needed} to
do at this point was declare an HMC event---Halt, Melt, and Catch Fire.
One of the foundational assumptions on which everything else has been
built has been revealed as flawed. This calls for a mental brake to a
full stop: take your weight off all beliefs built on the wrong
assumption, do your best to rethink everything from scratch. This is an
art I need to write more about---it's akin to the
convulsive effort required to seriously clean house, after an adult
religionist notices for the first time that God doesn't
exist.}

{
 But what Eliezer\textsubscript{2001} actually did was rehearse his
previous technophilic arguments for why it's difficult
to ban or governmentally control new technologies---the standard
arguments against
``relinquishment.''}

{
 It does seem even to my modern self that all those awful
consequences which technophiles argue to follow from various kinds of
government regulation are more or less correct---it's
much easier to say what someone is doing wrong, than to say the way
that is right. My modern viewpoint hasn't shifted to
think that technophiles are wrong about the downsides of technophobia;
but I do tend to be a lot more sympathetic to what technophobes say
about the downsides of technophilia. What previous Eliezers said about
the difficulties of, e.g., the government doing anything sensible about
Friendly AI, still seems pretty true. It's just that a
lot of his hopes for science, or private industry, etc., now seem
equally wrongheaded.}

{
 Still, let's not get into the details of the
technovolatile viewpoint. Eliezer\textsubscript{2001} has just tossed a
major foundational assumption---that AI can't be
dangerous, unlike other technologies---out the window. You would
intuitively suspect that this should have some kind of large effect on
his strategy.}

{
 Well, Eliezer\textsubscript{2001} did at least give up on his 1999
idea of an open-source AI Manhattan Project using self-modifying
heuristic soup, but overall \ldots}

{
 Overall, he'd previously wanted to charge in, guns
blazing, immediately using his best idea at the time; and afterward he
still wanted to charge in, guns blazing. He didn't say,
``I don't know how to do
this.'' He didn't say,
``I need better knowledge.'' He
didn't say, ``This project is not yet
ready to start coding.'' It was still all,
``The clock is ticking, gotta move now! MIRI will
start coding as soon as it's got enough
money!''}

{
 Before, he'd wanted to focus as much scientific
effort as possible with full information-sharing, and afterward he
still thought in those terms. Scientific secrecy = bad guy, openness =
good guy. (Eliezer\textsubscript{2001} hadn't read up
on the Manhattan Project and wasn't familiar with the
similar argument that Leó Szilárd had with Enrico Fermi.)}

{
 That's the problem with converting one big
``Oops!'' into a gradient of
shifting probability. It means there isn't a single
watershed moment---a visible huge impact---to hint that equally huge
changes might be in order.}

{
 Instead, there are all these little opinion shifts \ldots that give
you a chance to repair the \textit{arguments} for your strategies; to
shift the justification a little, but keep the ``basic
idea'' in place. Small shocks that the system can
absorb without cracking, because each time, it gets a chance to go back
and repair itself. It's just that in the domain of
rationality, cracking = good, repair = bad. In the art of rationality
it's far more efficient to admit one huge mistake, than
to admit lots of little mistakes.}

{
 There's some kind of instinct humans have, I
think, to preserve their former strategies and plans, so that they
aren't constantly thrashing around and wasting
resources; and of course an instinct to preserve any position that we
have publicly argued for, so that we don't suffer the
humiliation of being wrong. And though the younger Eliezer has striven
for rationality for many years, he is not immune to these impulses;
they waft gentle influences on his thoughts, and this, unfortunately,
is more than enough damage.}

{
 Even in 2002, the earlier Eliezer isn't yet
\textit{sure} that Eliezer\textsubscript{1997}'s plan
\textit{couldn't possibly} have worked. It
\textit{might} have gone right. You never know, right?}

{
 But there came a time when it all fell crashing down.}

{\centering
 \ ~
\par}

{\centering
 *
\par}


\bigskip

{
 1. John Moore, \textit{Slay and Rescue} (Xlibris Corp, 2000).}

\mysection{My Naturalistic Awakening}

{
 In the previous episode, Eliezer\textsubscript{2001} is fighting a
rearguard action against the truth. Only gradually shifting his
beliefs, admitting an increasing probability in a different scenario,
but never saying outright, ``I was wrong
before.'' He repairs his strategies as they are
challenged, finding new justifications for just the same plan he
pursued before. }

{
 (Of which it is therefore said: ``Beware lest you
fight a rearguard retreat against the evidence, grudgingly conceding
each foot of ground only when forced, feeling cheated. Surrender to the
truth as quickly as you can. Do this the instant you realize what you
are resisting; the instant you can see from which quarter the winds of
evidence are blowing against you.'')}

{
 Memory fades, and I can hardly bear to look back upon those
times---no, seriously, I can't \textit{stand} reading
my old writing. I've already been corrected once in my
recollections, by those who were present. And so, though I remember the
important events, I'm not really sure what
\textit{order} they happened in, let alone what year.}

{
 But if I had to pick a moment when my folly broke, I would pick
the moment when I first comprehended, in full generality, the notion of
an optimization process. That was the point at which I first looked
back and said, ``I've been a
fool.''}

{
 Previously, in 2002, I'd been writing a bit about
the evolutionary psychology of human general intelligence---though at
the time, I thought I was writing about AI; at this point I thought I
was against anthropomorphic intelligence, but I was still looking to
the human brain for inspiration. (The paper in question is
``Levels of Organization in General
Intelligence,'' a requested chapter for the volume
\textit{Artificial General Intelligence},\textsuperscript{1} which
finally came out in print in 2007.)}

{
 So I'd been thinking (and writing) about how
natural selection managed to cough up human intelligence; I saw a
\textit{dichotomy} between them, the blindness of natural selection and
the lookahead of intelligent foresight, reasoning by simulation versus
playing everything out in reality, abstract versus concrete thinking.
And yet it was natural selection that created human intelligence, so
that our brains, though not our thoughts, are entirely made according
to the signature of natural selection.}

{
 To this day, this still seems to me like a reasonably shattering
insight, and so it drives me up the wall when people lump together
natural selection and intelligence-driven processes as
``evolutionary.'' They really are
almost absolutely different in a number of important ways---though
there are concepts in common that can be used to describe them, like
consequentialism and cross-domain generality.}

{
 But that Eliezer\textsubscript{2002} is thinking in terms of a
\textit{dichotomy} between evolution and intelligence tells you
something about the limits of his vision---like someone who thinks of
politics as a dichotomy between conservative and liberal stances, or
someone who thinks of fruit as a dichotomy between apples and
strawberries.}

{
 After the ``Levels of
Organization'' draft was published online, Emil
Gilliam pointed out that my view of AI seemed pretty similar to my view
of intelligence. Now, of course Eliezer\textsubscript{2002}
doesn't espouse building an AI in the image of a human
mind; Eliezer\textsubscript{2002} knows very well that a human mind is
just a hack coughed up by natural selection. But
Eliezer\textsubscript{2002} has described these levels of organization
in human thinking, and he hasn't proposed using
different levels of organization in the AI. Emil Gilliam asks whether I
think I might be hewing too close to the human line. I dub the
alternative the ``Completely Alien Mind
Design'' and reply that a CAMD is probably too
difficult for human engineers to create, even if it's
possible in theory, because we wouldn't be able to
understand something so alien while we were putting it together.}

{
 I don't know if Eliezer\textsubscript{2002}
invented this reply on his own, or if he read it somewhere else.
Needless to say, I've heard this excuse plenty of times
since then. In reality, what you genuinely understand, you can usually
reconfigure in almost any sort of shape, leaving some structural
essence inside; but when you don't understand flight,
you suppose that a flying machine needs feathers, because you
can't imagine departing from the analogy of a bird.}

{
 So Eliezer\textsubscript{2002} is still, in a sense, attached to
humanish mind designs---he imagines improving on them, but the human
\textit{architecture} is still in some sense his point of departure.}

{
 What is it that finally breaks this attachment?}

{
 It's an embarrassing confession: It came from a
science fiction story I was trying to write. (No, you
can't see it; it's not done.) The story
involved a non-cognitive non-evolutionary optimization process,
something like an Outcome Pump. Not intelligence, but a cross-temporal
physical effect---that is, I was imagining it as a physical
effect---that narrowly constrained the space of possible outcomes. (I
can't tell you any more than that; it would be a
spoiler, if I ever finished the story. Just see the essay on Outcome
Pumps.) It was ``just a story,'' and
so I was free to play with the idea and elaborate it out logically: C
was constrained to happen, therefore B (in the past) was constrained to
happen, therefore A (which led to B) was constrained to happen.}

{
 Drawing a line through one point is generally held to be
dangerous. Two points make a dichotomy; you imagine them opposed to one
another. But when you've got three different
points---\textit{that's} when you're
forced to wake up and generalize.}

{
 Now I had three points: Human intelligence, natural selection, and
my fictional plot device.}

{
 And so that was the point at which I generalized the notion of an
optimization process, of a process that squeezes the future into a
narrow region of the possible.}

{
 This may seem like an obvious point, if you've
been following \textit{Overcoming Bias} this whole time; but if you
look at Shane Legg's collection of 71 definitions of
intelligence, you'll see that
``squeezing the future into a constrained
region'' is a less obvious reply than it seems.}

{
 Many of the definitions of
``intelligence'' by AI researchers
do talk about ``solving problems''
or ``achieving goals.'' But from the
viewpoint of past Eliezers, at least, it is only hindsight that makes
this the same thing as ``squeezing the
future.''}

{
 A \textit{goal} is a mentalistic object; electrons have no goals,
and solve no problems either. When a human imagines a goal, they
imagine an agent imbued with wanting-ness---it's still
empathic language.}

{
 You can espouse the notion that intelligence is about
``achieving goals''---and then turn
right around and argue about whether some
``goals'' are better than
others---or talk about the wisdom required to judge between goals
themselves---or talk about a system deliberately modifying its
goals---or talk about the free will needed to \textit{choose} plans
that achieve goals---or talk about an AI realizing that its goals
aren't what the programmers really meant to ask for. If
you imagine something that squeezes the future into a narrow region of
the possible, like an Outcome Pump, those seemingly sensible statements
somehow don't translate.}

{
 So for me at least, seeing through the word
``mind'' to a physical process that
would, just by naturally running, just by obeying the laws of physics,
end up squeezing its future into a narrow region, was a naturalistic
enlightenment over and above the notion of an agent trying to achieve
its goals.}

{
 It was like falling out of a deep pit, falling into the ordinary
world, strained cognitive tensions relaxing into unforced simplicity,
confusion turning to smoke and drifting away. I saw the \textit{work
performed} by intelligence; \textit{smart} was no longer a property,
but an engine. Like a knot in time, echoing the outer part of the
universe in the inner part, and thereby steering it. I even saw, in a
flash of the same enlightenment, that a mind had to output waste heat
in order to obey the laws of thermodynamics.}

{
 Previously, Eliezer\textsubscript{2001} had talked about Friendly
AI as something you should do just to be sure---if you
didn't know whether AI design X was going to be
Friendly, then you really ought to go with AI design Y that you did
know would be Friendly. But Eliezer\textsubscript{2001}
didn't think he \textit{knew} whether you could
\textit{actually} have a superintelligence that turned its future light
cone into paperclips.}

{
 Now, though, I could \textit{see} it---the pulse of the
optimization process, sensory information surging in, motor
instructions surging out, steering the future. In the middle, the model
that linked up possible actions to possible outcomes, and the utility
function over the outcomes. Put in the corresponding utility function,
and the result would be an optimizer that would steer the future
anywhere.}

{
 Up until that point, I'd never quite admitted to
myself that Eliezer\textsubscript{1997}'s AI goal
system design would definitely, no two ways about it, pointlessly wipe
out the human species. Now, however, I looked back, and I could finally
see \textit{what my old design really did}, to the extent it was
coherent enough to be talked about. Roughly, it would have converted
its future light cone into generic tools---computers without programs
to run, stored energy without a use \ldots}

{
 \ldots how on Earth had I, the fine and practiced rationalist---how
on Earth had I managed to miss something that obvious, for six damned
years?}

{
 That was the point at which I awoke clear-headed, and remembered;
and thought, with a certain amount of embarrassment:
\textit{I've been stupid.}}

{\centering
 \ ~
\par}

{\centering
 *
\par}


\bigskip

{
 1. Ben Goertzel and Cassio Pennachin, eds., \textit{Artificial
General Intelligence}, Cognitive Technologies (Berlin: Springer, 2007),
doi:10.1007/978-3-540-68677-4.}

\mysection{The Level Above Mine}

{
 I once lent Xiaoguang ``Mike''
Li my copy of \textit{Probability Theory: The Logic of Science}. Mike
Li read some of it, and then came back and said:}

{
 Wow \ldots it's like Jaynes is a thousand-year-old
vampire.}

{
 Then Mike said, ``No, wait, let me explain
that---'' and I said, ``No, I know
exactly what you mean.'' It's a
convention in fantasy literature that the older a vampire gets, the
more powerful they become.}

{
 I'd enjoyed math proofs before I encountered
Jaynes. But E. T. Jaynes was the first time I picked up a sense of
\textit{formidability} from mathematical arguments. Maybe because
Jaynes was lining up ``paradoxes''
that had been used to object to Bayesianism, and then blasting them to
pieces with overwhelming firepower---power being used to overcome
others. Or maybe the sense of formidability came from Jaynes not
treating his math as a game of aesthetics; Jaynes \textit{cared} about
probability theory, it was bound up with other considerations that
mattered, to him and to me too.}

{
 For whatever reason, the sense I get of Jaynes is one of
terrifying swift perfection---something that would arrive at the
correct answer by the shortest possible route, tearing all surrounding
mistakes to shreds in the same motion. Of course, when you write a
book, you get a chance to show only your best side. But still.}

{
 It spoke well of Mike Li that he was able to sense the aura of
formidability surrounding Jaynes. It's a general rule,
I've observed, that you can't
discriminate between levels too far above your own. E.g., someone once
earnestly told me that I was really bright, and
``ought to go to college.'' Maybe
anything more than around one standard deviation above you starts to
blur together, though that's just a cool-sounding wild
guess.}

{
 So, having heard Mike Li compare Jaynes to a thousand-year-old
vampire, one question immediately popped into my mind:}

{
 ``Do you get the same sense off
me?'' I asked.}

{
 Mike shook his head. ``Sorry,''
he said, sounding somewhat awkward,
``it's just that Jaynes is
\ldots''}

{
 ``No, I know,'' I said. I
hadn't thought I'd reached
Jaynes's level. I'd only been curious
about how I came across to other people.}

{
 I \textit{aspire} to Jaynes's level. I
\textit{aspire} to become as much the master of Artificial Intelligence
/ reflectivity, as Jaynes was master of Bayesian probability theory. I
can even plead that the art I'm trying to master is
more difficult than Jaynes's, making a mockery of
deference. Even so, and embarrassingly, there is \textit{no} art of
which I am as much the master now, as Jaynes was of probability
theory.}

{
 This is not, necessarily, to place myself beneath Jaynes as a
person---to say that Jaynes had a magical aura of destiny, and I
don't.}

{
 Rather I recognize in Jaynes \textit{a level of expertise, of
sheer formidability,} which I have not yet achieved. I can argue
forcefully in my chosen subject, but that is not the same as writing
out the equations and saying: \textbf{DONE.}}

{
 For so long as I have not yet achieved that level, I must
acknowledge the possibility that I can never achieve it, that my native
talent is not sufficient. When Marcello Herreshoff had known me for
long enough, I asked him if he knew of anyone who struck him as
substantially more \textit{natively intelligent} than myself. Marcello
thought for a moment and said ``John Conway---I met
him at a summer math camp.'' \textit{Darn,} I
thought, \textit{he thought of someone, and worse, it's
some ultra-famous old guy I can't grab.} I inquired how
Marcello had arrived at the judgment. Marcello said,
``He just struck me as having a tremendous amount of
mental horsepower,'' and started to explain a math
problem he'd had a chance to work on with Conway.}

{
 Not what I wanted to hear.}

{
 Perhaps, relative to Marcello's experience of
Conway and his experience of me, I haven't had a chance
to show off on any subject that I've mastered as
thoroughly as Conway had mastered his many fields of mathematics.}

{
 Or it might be that Conway's brain is specialized
off in a different direction from mine, and that I could never approach
Conway's level on math, yet Conway
wouldn't do so well on AI research.}

{
 Or \ldots}

{
 \ldots or I'm strictly dumber than Conway,
dominated by him along all dimensions. Maybe, if I could find a young
proto-Conway and tell them the basics, they would blaze right past me,
solve the problems that have weighed on me for years, and zip off to
places I can't follow.}

{
 Is it damaging to my ego to confess that last possibility? Yes. It
would be futile to deny that.}

{
 Have I \textit{really} accepted that awful possibility, or am I
only pretending to myself to have accepted it? Here I will say:
``No, I think I have accepted it.''
Why do I dare give myself so much credit? Because I've
invested specific effort into that awful possibility. I am writing here
for many reasons, but a major one is the vision of some younger mind
reading these words and zipping off past me. It might happen, it might
not.}

{
 Or sadder: Maybe I just wasted too much time on setting up the
resources to support me, instead of studying math full-time through my
whole youth; or I wasted too much youth on non-mathy ideas. And this
choice, my past, is irrevocable. I'll hit a brick wall
at 40, and there won't be anything left but to pass on
the resources to another mind with the potential I wasted, still young
enough to learn. So to save them time, I should leave a trail to my
successes, and post warning signs on my mistakes.}

{
 Such \textit{specific efforts} predicated on an ego-damaging
possibility---that's the only kind of humility that
seems real enough for me to dare credit myself. Or giving up my
precious theories, when I realized that they didn't
meet the standard Jaynes had shown me---that was hard, and it was real.
Modest demeanors are cheap. Humble admissions of doubt are cheap.
I've known too many people who, presented with a
counterargument, say, ``I am but a fallible mortal, of
course I could be wrong,'' and then go on to do
exactly what they had planned to do previously.}

{
 You'll note that I don't try to
modestly say anything like, ``Well, I may not be as
brilliant as Jaynes or Conway, but that doesn't mean I
can't do important things in my chosen
field.''}

{
 Because I do know \ldots that's not how it works.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{The Magnitude of His Own Folly}

{
 In the years before I met a would-be creator of Artificial General
Intelligence (with a funded project) who happened to be a creationist,
I would still try to argue with individual AGI wannabes. }

{
 In those days, I sort-of-succeeded in convincing one such fellow
that, yes, you had to take Friendly AI into account, and no, you
couldn't just find the right fitness metric for an
evolutionary algorithm. (Previously he had been very impressed with
evolutionary algorithms.)}

{
 And the one said: \textit{Oh, woe! Oh, alas! What a fool
I've been! Through my carelessness, I almost destroyed
the world! What a villain I once was!}}

{
 Now, \textit{there's} a trap I knew better than to
fall into---}

{
 {}---at the point where, in late 2002, I looked back to
Eliezer\textsubscript{1997}'s AI proposals and realized
what they really would have done, insofar as they were coherent enough
for me to talk about what they ``really would have
done.''}

{
 When I finally saw the magnitude of my own folly, everything fell
into place at once. The dam against realization cracked; and the
unspoken doubts that had been accumulating behind it crashed through
all together. There wasn't a prolonged period, or even
a single moment that I remember, of wondering how I could have been so
stupid. I already knew how.}

{
 And I also knew, all at once, in the same moment of realization,
that to say, \textit{I almost destroyed the world!}, would have been
too prideful.}

{
 It would have been too confirming of ego, too confirming of my own
importance in the scheme of things, at a time when---I understood in
the same moment of realization---my ego ought to be taking a major
punch to the stomach. I had been so much less than I needed to be; I
had to take that punch in the stomach, not avert it.}

{
 And by the same token, I didn't fall into the
conjugate trap of saying: \textit{Oh, well, it's not as
if I had code and was about to run it; I didn't} really
\textit{come close to destroying the world.} For that, too, would have
minimized the force of the punch. \textit{It wasn't
really loaded?} I had proposed and intended to build the gun, and load
the gun, and put the gun to my head and pull the trigger; and that was
a bit too much self-destructiveness.}

{
 I didn't make a grand emotional drama out of it.
That would have wasted the force of the punch, averted it into mere
tears.}

{
 I knew, in the same moment, what I had been carefully not-doing
for the last six years. I hadn't been updating.}

{
 And I knew I had to finally update. To actually \textit{change}
what I planned to do, to change what I was doing now, to do something
different instead.}

{
 I knew I had to stop.}

{
 Halt, melt, and catch fire.}

{
 Say, ``I'm not
ready.'' Say, ``I
don't know how to do this yet.''}

{
 These are terribly difficult words to say, in the field of AGI.
Both the lay audience and your fellow AGI researchers are interested in
code, projects with programmers in play. Failing that, they may give
you some credit for saying, ``I'm
ready to write code; just give me the funding.''}

{
 Say, ``I'm not ready to write
code,'' and your status drops like a depleted uranium
balloon.}

{
 What distinguishes you, then, from six billion other people who
don't know how to create Artificial General
Intelligence? If you don't have neat code (that does
something other than be humanly intelligent, obviously; but at least
it's code), or at minimum your own startup
that's going to write code as soon as it gets
funding---then who are you and what are you doing at our conference?}

{
 Maybe later I'll write on where this attitude
comes from---the excluded middle between ``I know how
to build AGI!'' and
``I'm working on narrow AI because I
don't know how to build AGI,'' the
nonexistence of a concept for ``I am trying to get
from an incomplete map of FAI to a complete map of
FAI.''}

{
 But this attitude does exist, and so the loss of status associated
with saying ``I'm not ready to write
code'' is very great. (If the one doubts this, let
them name any other who simultaneously says ``I intend
to build an Artificial General Intelligence,''
``Right now I can't build an AGI
because I don't know X,'' and
``I am currently trying to figure out
X.'')}

{
 (And never mind AGI folk who've already raised
venture capital, promising returns in five years.)}

{
 So there's a huge reluctance to say,
``Stop.'' You can't
just say, ``Oh, I'll swap back to
figure-out-X mode,'' because that mode
doesn't exist.}

{
 Was there more to that reluctance than just loss of status, in my
case? Eliezer\textsubscript{2001} might also have flinched away from
slowing his perceived forward momentum into the intelligence explosion,
which was so right and so necessary \ldots}

{
 But mostly, I think I flinched away from not being able to say,
``I'm ready to start
coding.'' Not just for fear of
others' reactions, but because I'd been
inculcated with the same attitude myself.}

{
 Above all, Eliezer\textsubscript{2001} didn't say,
``Stop''---even \textit{after}
noticing the problem of Friendly AI---because I did not realize, on a
gut level, that Nature was allowed to kill me.}

{
 ``Teenagers think they're
immortal,'' the proverb goes. Obviously this
isn't true in the literal sense that if you ask them,
``Are you indestructible?'' they
will reply ``Yes, go ahead and try shooting
me.'' But perhaps wearing seat belts
isn't deeply emotionally compelling for them, because
the thought of their own death isn't quite
\textit{real}{}---they don't really believe
it's allowed to happen. It can happen in
\textit{principle} but it can't \textit{actually}
happen.}

{
 Personally, I always wore my seat belt. As an individual, I
understood that I could die.}

{
 But, having been raised in technophilia to treasure that one most
precious thing, far more important than my own life, I once thought
that the Future was indestructible.}

{
 Even when I acknowledged that nanotech could wipe out humanity, I
still believed the intelligence explosion was invulnerable. That if
humanity survived, the intelligence explosion would happen, and the
resultant AI would be too smart to be corrupted or lost.}

{
 Even after \textit{that}, when I acknowledged Friendly AI as a
consideration, I didn't emotionally believe in the
possibility of failure, any more than that teenager who
doesn't wear their seat belt \textit{really} believes
that an automobile accident is \textit{really} allowed to kill or
cripple them.}

{
 It wasn't until my insight into optimization let
me look back and see Eliezer\textsubscript{1997} in plain light that I
realized that Nature was allowed to kill me.}

{
 ``The thought you cannot think controls you more
than thoughts you speak aloud.'' But we flinch away
from only those fears that are real to us.}

{
 AGI researchers take very seriously the prospect of
\textit{someone else solving the problem first}. They can imagine
seeing the headlines in the paper saying that their own work has been
upstaged. They know that Nature is allowed to do that to them. The ones
who have started companies know that they are allowed to run out of
venture capital. That possibility is \textit{real} to them, very real;
it has a power of emotional compulsion over them.}

{
 I don't think that
``Oops'' followed by the thud of six
billion bodies falling, \textit{at their own hands,} is real to them on
quite the same level.}

{
 It is unsafe to say what other people are thinking. But it seems
rather likely that when the one reacts to the prospect of Friendly AI
by saying, ``If you delay development to work on
safety, other projects that don't care \textit{at all}
about Friendly AI will beat you to the punch,'' the
prospect of they themselves making a mistake followed by six billion
thuds is not really real to them; but the possibility of others beating
them to the punch is deeply scary.}

{
 I, too, used to say things like that, before I understood that
Nature was allowed to kill me.}

{
 In that moment of realization, my childhood technophilia finally
broke.}

{
 I finally understood that even if you diligently followed the
rules of science and were a nice person, Nature could still kill you. I
finally understood that even if you were the best project out of all
available candidates, Nature could still kill you.}

{
 I understood that I was not being graded on a curve. My gaze shook
free of rivals, and I saw the sheer blank wall.}

{
 I looked back and I saw the careful arguments I had constructed,
for why the wisest choice was to continue forward at full speed, just
as I had planned to do before. And I understood then that even if you
constructed an argument showing that something was the best course of
action, Nature was still allowed to say ``So
what?'' and kill you.}

{
 I looked back and saw that I had claimed to take into account the
risk of a fundamental mistake, that I had argued reasons to tolerate
the risk of proceeding in the absence of full knowledge.}

{
 And I saw that the risk I wanted to tolerate would have killed me.
And I saw that this possibility had never been \textit{really} real to
me. And I saw that even if you had wise and excellent arguments for
taking a risk, the risk was still allowed to go ahead and kill you.
\textit{Actually} kill you.}

{
 For it is only the action that matters, and not the reasons for
doing anything. If you build the gun and load the gun and put the gun
to your head and pull the trigger, even with the cleverest of arguments
for carrying out every step---then, bang.}

{
 I saw that only my own ignorance of the rules had enabled me to
argue for going ahead without complete knowledge of the rules; for if
you do not know the rules, you cannot model the penalty of ignorance.}

{
 I saw that others, still ignorant of the rules, were saying,
``I will go ahead and do X''; and
that to the extent that X was a coherent proposal at all, I knew that
would result in a bang; but they said, ``I do not know
it cannot work.'' I would try to explain to them the
smallness of the target in the search space, and they would say
``How can you be so sure I won't win
the lottery?,'' wielding their own ignorance as a
bludgeon.}

{
 And so I realized that the only thing I \textit{could} have done
to save myself, in my previous state of ignorance, was to say:
``I will not proceed until I know positively that the
ground is safe.'' And there are many clever arguments
for why you should step on a piece of ground that you
don't know to contain a landmine; but they all sound
much less clever, after you look to the place that you proposed and
intended to step, and see the bang.}

{
 I understood that you could do \textit{everything that you were
supposed to do}, and Nature was still allowed to kill you. That was
when my last trust broke. And that was when my training as a
rationalist began.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Beyond the Reach of God}

{
 This essay is a tad gloomier than usual, as I measure such things.
It deals with a thought experiment I invented to smash my own optimism,
after I realized that optimism had misled me. Those readers sympathetic
to arguments like, ``It's important to
keep our biases because they help us stay happy,''
should consider not reading. (Unless they have something to protect,
including their own life.) }

{
 So! Looking back on the magnitude of my own folly, I realized that
at the root of it had been a disbelief in the Future's
vulnerability---a reluctance to accept that things could
\textit{really} turn out wrong. Not as the result of any explicit
propositional verbal belief. More like something inside that persisted
in believing, even in the face of adversity, that everything would be
all right in the end.}

{
 Some would account this a virtue (\textit{zettai daijobu da yo}),
and others would say that it's a thing necessary for
mental health.}

{
 But we don't live in that world. We live in the
world beyond the reach of God.}

{
 It's been a long, long time since I believed in
God. Growing up in an Orthodox Jewish family, I can recall the last
remembered time I asked God for something, though I
don't remember how old I was. I was putting in some
request on behalf of the next-door-neighboring boy, I forget what
exactly---something along the lines of, ``I hope
things turn out all right for him,'' or maybe,
``I hope he becomes Jewish.''}

{
 I remember what it was like to have some higher authority to
appeal to, to take care of things I couldn't handle
myself. I didn't think of it as
``warm,'' because I had no
alternative to compare it to. I just took it for granted.}

{
 Still I recall, though only from distant childhood, what
it's like to live in the conceptually impossible
possible world where God exists. \textit{Really} exists, in the way
that children and rationalists take all their beliefs at face value.}

{
 In the world where God exists, does God intervene to optimize
\textit{everything}? Regardless of what rabbis assert about the
fundamental nature of reality, the take-it-seriously operational answer
to this question is obviously
``No.'' You can't
ask God to bring you a lemonade from the refrigerator instead of
getting one yourself. When I believed in God after the serious fashion
of a child, so very long ago, I didn't believe that.}

{
 Postulating that particular divine inaction
doesn't provoke a full-blown theological crisis. If you
said to me, ``I have constructed a benevolent
superintelligent nanotech-user,'' and I said
``Give me a banana,'' and no banana
appeared, this would not \textit{yet} disprove your statement. Human
parents don't always do everything their children ask.
There are some decent fun-theoretic arguments---I even believe them
myself---against the idea that the \textit{best} kind of help you can
offer someone is to always immediately give them everything they want.
I don't think that eudaimonia is formulating goals and
having them instantly fulfilled; I don't \textit{want}
to become a simple wanting-thing that never has to plan or act or
think.}

{
 So it's not necessarily an attempt to avoid
falsification to say that God does not grant all prayers. Even a
Friendly AI might not respond to every request.}

{
 But clearly there exists \textit{some} threshold of horror awful
enough that God will intervene. I remember that being true, when I
believed after the fashion of a child.}

{
 The God who does not intervene \textit{at all}, no matter how bad
things get---\textit{that's} an obvious attempt to
avoid falsification, to protect a belief-in-belief. Sufficiently young
children don't have the deep-down knowledge that God
doesn't really exist. They really expect to see a
dragon in their garage. They have no reason to imagine a loving God who
never acts. Where exactly is the boundary of sufficient awfulness? Even
a child can imagine arguing over the precise threshold. But of course
God will draw the line somewhere. Few indeed are the loving parents
who, desiring their child to grow up strong and self-reliant, would let
their toddler be run over by a car.}

{
 The obvious example of a horror so great that God cannot tolerate
it is death---true death, mind-annihilation. I don't
think that even Buddhism allows that. So long as there is a God in the
classic sense---full-blown, ontologically fundamental, \textit{the}
God---we can rest assured that no \textit{sufficiently} awful event
will ever, ever happen. There is no soul anywhere that need fear true
annihilation; God will prevent it.}

{
 What if you build your own simulated universe? The classic example
of a simulated universe is Conway's Game of Life. I do
urge you to investigate Life if you've never played
it---it's important for comprehending the notion of
``physical law.''
Conway's Life has been proven Turing-complete, so it
would be possible to build a sentient being in the Life universe,
although it might be rather fragile and awkward. Other cellular
automata would make it simpler.}

{
 Could you, by creating a simulated universe, escape the reach of
God? Could you simulate a Game of Life containing sentient entities,
and torture the beings therein? But if God is watching everywhere, then
trying to build an unfair Life just results in \textit{the} God
stepping in to modify your computer's transistors. If
the physics you set up in your computer program calls for a sentient
Life-entity to be endlessly tortured for no particular reason,
\textit{the} God will intervene. God being omnipresent, there is no
refuge \textit{anywhere} for true horror. Life is fair.}

{
 But suppose that instead you ask the question:}

{
 \textit{Given} such-and-such initial conditions, and
\textit{given} such-and-such cellular automaton rules, what
\textit{would be} the mathematical result?}

{
 Not even God can modify the answer to this question, unless you
believe that God can implement logical impossibilities. Even as a very
young child, I don't remember believing that. (And why
would you need to believe it, if God can modify anything that
\textit{actually} exists?)}

{
 What does Life look like, in this imaginary world where every step
follows \textit{only} from its immediate predecessor? Where things
\textit{only} ever happen, or don't happen, because of
the cellular automaton rules? Where the initial conditions and rules
\textit{don't} describe any God that checks over each
state? What does it look like, the world beyond the reach of God?}

{
 That world wouldn't be fair. If the initial state
contained the seeds of something that could self-replicate, natural
selection might or might not take place, and complex life might or
might not evolve, and that life might or might not become sentient,
with no God to guide the evolution. That world might evolve the
equivalent of conscious cows, or conscious dolphins, that lacked hands
to improve their condition; maybe they would be eaten by conscious
wolves who never thought that they were doing wrong, or cared.}

{
 If in a vast plethora of worlds, something like humans evolved,
then they would suffer from diseases---not to teach them any lessons,
but only because viruses happened to evolve as well, under the cellular
automaton rules.}

{
 If the people of that world are happy, or unhappy, the causes of
their happiness or unhappiness may have nothing to do with good or bad
choices they made. Nothing to do with free will or lessons learned. In
the what-if world where every step follows only from the cellular
automaton rules, the equivalent of Genghis Khan can murder a million
people, and laugh, and be rich, and never be punished, and live his
life much happier than the average. Who prevents it? God would prevent
it from ever \textit{actually} happening, of course; He would at the
very least visit some shade of gloom in the Khan's
heart. But in the mathematical answer to the question \textit{What if?}
there is no God in the axioms. So if the cellular automaton rules say
that the Khan is happy, that, simply, is the whole and only answer to
the what-if question. There is nothing, absolutely nothing, to prevent
it.}

{
 And if the Khan tortures people horribly to death over the course
of days, for his own amusement perhaps? They will call out for help,
perhaps imagining a God. And if you \textit{really} wrote that cellular
automaton, God would intervene in your program, of course. But in the
what-if question, what the cellular automaton \textit{would} do under
the mathematical rules, there isn't any God in the
system. Since the physical laws contain no specification of a utility
function---in particular, no prohibition against torture---then the
victims will be saved only if the right cells happen to be 0 or 1. And
it's not likely that anyone will defy the Khan; if they
did, someone would strike them with a sword, and the sword would
disrupt their organs and they would die, and that would be the end of
that. So the victims die, screaming, and no one helps them; that is the
answer to the what-if question.}

{
 Could the victims be completely innocent? Why not, in the what-if
world? If you look at the rules for Conway's Game of
Life (which is Turing-complete, so we can embed arbitrary computable
physics in there), then the rules are really very simple. Cells with
three living neighbors stay alive; cells with two neighbors stay the
same; all other cells die. There isn't anything in
there about innocent people not being horribly tortured for indefinite
periods.}

{
 Is this world starting to sound familiar?}

{
 Belief in a fair universe often manifests in more subtle ways than
thinking that horrors should be outright prohibited: Would the
twentieth century have gone differently, if Klara Pölzl and Alois
Hitler had made love one hour earlier, and a different sperm fertilized
the egg, on the night that Adolf Hitler was conceived?}

{
 For so many lives and so much loss to turn on a single event seems
\textit{disproportionate}. The Divine Plan ought to make more
\textit{sense} than that. You can believe in a Divine Plan without
believing in God---Karl Marx surely did. You shouldn't
have millions of lives depending on a casual choice, an
hour's timing, the speed of a microscopic flagellum. It
ought not to be allowed. It's \textit{too}
disproportionate. Therefore, if Adolf Hitler had been able to go to
high school and become an architect, there would have been someone else
to take his role, and World War II would have happened the same as
before.}

{
 But in the world beyond the reach of God, there
isn't any clause in the physical axioms that says
``things have to make sense'' or
``big effects need big causes'' or
``history runs on reasons too important to be so
fragile.'' There is no God to \textit{impose} that
order, which is so severely violated by having the lives and deaths of
millions depend on one small molecular event.}

{
 The point of the thought experiment is to lay out the God-universe
and the Nature-universe side by side, so that we can recognize what
kind of thinking belongs to the God-universe. Many who are atheists
still think as if certain things are \textit{not allowed}. They would
lay out arguments for why World War II was inevitable and would have
happened in more or less the same way, even if Hitler had become an
architect. But in sober historical fact, this is an unreasonable
belief; I chose the example of World War II because from my reading, it
seems that events were mostly driven by Hitler's
personality, often in defiance of his generals and advisors. There is
no particular empirical justification that I happen to have heard of
for doubting this. The main reason to doubt would be \textit{refusal to
accept} that the universe could make so little sense---that horrible
things could happen so \textit{lightly}, for no more reason than a roll
of the dice.}

{
 But why not? What prohibits it?}

{
 In the God-universe, God prohibits it. To recognize this is to
recognize that we don't live in that universe. We live
in the what-if universe beyond the reach of God, driven by the
mathematical laws and nothing else. Whatever physics says will happen,
will happen. Absolutely \textit{anything}, good or bad, will happen.
And there is nothing in the laws of physics to lift this rule even for
the \textit{really extreme} cases, where you might expect Nature to be
a little more reasonable.}

{
 Reading William Shirer's \textit{The Rise and Fall
of the Third Reich}, listening to him describe the disbelief that he
and others felt upon discovering the full scope of Nazi atrocities, I
thought of what a strange thing it was, to read all that, and know,
already, that there wasn't a single protection against
it. To just read through the whole book and accept it; horrified, but
not at all disbelieving, because I'd already understood
what kind of world I lived in.}

{
 Once upon a time, I believed that the extinction of humanity was
not allowed. And others who call themselves rationalists may yet have
things they trust. They might be called ``positive-sum
games,'' or
``democracy,'' or
``technology,'' but they are sacred.
The mark of this sacredness is that the trustworthy thing
can't lead to anything \textit{really} bad; or they
can't be \textit{permanently} defaced, at least not
without a compensatory silver lining. In that sense they can be
trusted, even if a few bad things happen here and there.}

{
 The unfolding history of Earth can't ever turn
from its positive-sum trend to a negative-sum trend; that is not
allowed. Democracies---\textit{modern} \textit{liberal} democracies,
anyway---won't ever legalize torture. Technology has
done so much good up until now, that there can't
possibly be a Black Swan technology that breaks the trend and does more
harm than all the good up until this point.}

{
 There are all sorts of clever arguments why such things
can't possibly happen. But the source of these
arguments is a much deeper belief that such things are \textit{not
allowed}. Yet who prohibits? Who prevents it from happening? If you
can't visualize at least one lawful universe where
physics say that such dreadful things happen---and so they \textit{do}
happen, there being nowhere to appeal the verdict---then you
aren't yet ready to argue \textit{probabilities}.}

{
 Could it really be that sentient beings have died absolutely for
thousands or millions of years, with no soul and no afterlife---and
\textit{not} as part of any grand plan of Nature---\textit{not} to
teach any great lesson about the meaningfulness or meaninglessness of
life---not even to teach any profound lesson about what is
impossible---so that a trick as simple and stupid-sounding as
vitrifying people in liquid nitrogen can save them from total
annihilation---and a 10-second rejection of the silly idea can destroy
someone's soul? Can it be that a computer programmer
who signs a few papers and buys a life-insurance policy continues into
the far future, while Einstein rots in a grave? We can be sure of one
thing: God wouldn't allow it. Anything that ridiculous
and disproportionate would be ruled out. It would make a mockery of the
Divine Plan---a mockery of the \textit{strong reasons} why things must
be the way they are.}

{
 You can have secular rationalizations for things being \textit{not
allowed}. So it helps to imagine that there \textit{is} a God,
benevolent as you understand goodness---a God who enforces throughout
Reality a \textit{minimum} of fairness and justice---whose plans make
sense and depend proportionally on people's
choices---who will never permit absolute horror---who does not always
intervene, but who at least prohibits universes wrenched
\textit{completely} off their track \ldots to imagine all this, but also
imagine that \textit{you}, yourself, live in a what-if world of pure
mathematics---a world beyond the reach of God, an utterly unprotected
world where anything at all can happen.}

{
 If there's any reader still reading this who
thinks that being happy counts for more than anything in life, then
maybe they \textit{shouldn't} spend much time pondering
the unprotectedness of their existence. Maybe think of it \textit{just}
long enough to sign up themselves and their family for cryonics, and/or
write a check to an existential-risk-mitigation agency now and then.
And wear a seat belt and get health insurance and all those other
dreary necessary things that can destroy your life if you miss that one
step \ldots but aside from that, if you want to be happy, meditating on
the fragility of life isn't going to help.}

{
 But this essay was written for those who have something to
protect.}

{
 What can a twelfth-century peasant do to save themselves from
annihilation? Nothing. Nature's little challenges
aren't always fair. When you run into a challenge
that's too difficult, you suffer the penalty; when you
run into a lethal penalty, you die. That's how it is
for people, and it isn't any different for planets.
Someone who wants to dance the deadly dance with Nature does need to
understand what they're up against: Absolute, utter,
exceptionless neutrality.}

{
 Knowing this won't always save you. It
wouldn't save a twelfth-century peasant, even if they
knew. If you think that a rationalist who fully understands the mess
they're in must \textit{surely} be able to find a way
out---then you trust rationality, enough said.}

{
 Some commenter is bound to castigate me for putting too dark a
tone on all this, and in response they will list out all the reasons
why it's lovely to live in a neutral universe. Life is
allowed to be a \textit{little} dark, after all; but not darker than a
certain point, unless there's a silver lining.}

{
 Still, because I don't want to create
\textit{needless} despair, I will say a few hopeful words at this
point:}

{
 If humanity's future unfolds in the right way, we
might be able to make our future light cone fair(er). We
can't modify fundamental physics, but on a higher level
of organization we could build some guardrails and put down some
padding; organize the particles into a pattern that does some internal
checks against catastrophe. There's a lot of stuff out
there that we can't touch---but it may help to consider
everything that isn't in our future light cone as being
part of the ``generalized past.'' As
if it had all already happened. There's at least the
\textit{prospect} of defeating neutrality, in the only future we can
touch---the only world that it accomplishes something to care about.}

{
 Someday, maybe, immature minds will reliably be sheltered. Even if
children go through the equivalent of not getting a lollipop, or even
burning a finger, they won't ever be run over by cars.}

{
 And the adults wouldn't be in so much danger. A
superintelligence---a mind that could think a trillion thoughts without
a misstep---would not be intimidated by a challenge where death is the
price of a single failure. The raw universe wouldn't
seem so harsh, would be only another problem to be solved.}

{
 The problem is that building an adult is itself an adult
challenge. That's what I finally realized, years ago.}

{
 If there is a fair(er) universe, we have to get there starting
from \textit{this} world---the neutral world, the world of hard
concrete with no padding, the world where challenges are not calibrated
to your skills.}

{
 Not every child needs to stare Nature in the eyes. Buckling a seat
belt, or writing a check, is not that complicated or deadly. I
don't say that every rationalist should meditate on
neutrality. I don't say that every rationalist should
think all these unpleasant thoughts. But anyone who plans on
confronting an uncalibrated challenge of instant death must not avoid
them.}

{
 What does a child need to do---what rules should they follow, how
should they behave---to solve an adult problem?}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{My Bayesian Enlightenment}

{
 I remember (dimly, as human memories go) the first time I
self-identified as a ``Bayesian.''
Someone had just asked a malformed version of an old probability
puzzle, saying:}

{
 If I meet a mathematician on the street, and she says,
``I have two children, and at least one of them is a
boy,'' what is the probability that they are both
boys?}

{
 In the \textit{correct} version of this story, the mathematician
says, ``I have two children,'' and
\textit{you} ask, ``Is at least one a
boy?,'' and she answers,
``Yes.'' Then the probability is 1/3
that they are both boys.}

{
 But in the malformed version of the story---as I pointed out---one
would common-sensically reason:}

{
 If the mathematician has one boy and one girl, then my prior
probability for her saying ``at least one of them is a
boy'' is 1/2 and my prior probability for her saying
``at least one of them is a girl''
is 1/2. There's no reason to believe, a priori, that
the mathematician will only mention a girl if there is no possible
alternative.}

{
 So I pointed this out, and worked the answer using
Bayes's Rule, arriving at a probability of 1/2 that the
children were both boys. I'm not sure whether or not I
knew, at this point, that Bayes's rule was called that,
but it's what I used.}

{
 And lo, someone said to me, ``Well, what you just
gave is the Bayesian answer, but in orthodox statistics the answer is
1/3. We just exclude the possibilities that are ruled out, and count
the ones that are left, without trying to guess the probability that
the mathematician will say this or that, since we have no way of really
knowing that probability---it's too
subjective.''}

{
 I responded---note that this was completely
spontaneous---``What on Earth do you mean? You
can't avoid assigning a probability to the
mathematician making one statement or another. You're
just assuming the probability is 1, and \textit{that's}
unjustified.''}

{
 To which the one replied, ``Yes,
that's what the Bayesians say. But frequentists
don't believe that.''}

{
 And I said, astounded: ``How can there possibly
be such a thing as non-Bayesian statistics?''}

{
 That was when I discovered that I was of the type called
``Bayesian.'' As far as I can tell,
I was \textit{born} that way. My mathematical intuitions were such that
everything Bayesians said seemed perfectly straightforward and simple,
the obvious way I would do it myself; whereas the things frequentists
said sounded like the elaborate, warped, mad blasphemy of dreaming
Cthulhu. I didn't \textit{choose} to become a Bayesian
any more than fishes choose to breathe water.}

{
 But this is not what I refer to as my ``Bayesian
enlightenment.'' The first time I heard of
``Bayesianism,'' I marked it off as
obvious; I didn't go much further in than
Bayes's Rule itself. At that time I still thought of
probability theory as a tool rather than a law. I
didn't think there were mathematical laws of
intelligence (my best and worst mistake). Like nearly all AGI wannabes,
Eliezer\textsubscript{2001} thought in terms of techniques, methods,
algorithms, building up a toolbox full of cool things he could
\textit{do}; he searched for tools, not understanding.
Bayes's Rule was a really neat tool, applicable in a
surprising number of cases.}

{
 Then there was my initiation into heuristics and biases. It
started when I ran across a webpage that had been transduced from a
Powerpoint intro to behavioral economics. It mentioned some of the
results of heuristics and biases, in passing, without any references. I
was so startled that I emailed the author to ask if this was actually a
real experiment, or just anecdotal. He sent me back a scan of Tversky
and Kahneman's 1973 paper.}

{
 Embarrassing to say, my story doesn't really start
there. I put it on my list of things to look into. I knew that there
was an edited volume called ``Judgment Under
Uncertainty: Heuristics and Biases,'' but
I'd never seen it. At this time, I figured that if it
wasn't online, I would just try to get along without
it. I had so many other things on my reading stack, and no easy access
to a university library. I think I must have mentioned this on a
mailing list, because Emil Gilliam was annoyed by my online-only
theory, so he bought me the book.}

{
 His action here should probably be regarded as scoring a fair
number of points.}

{
 But this, too, is not what I refer to as my
``Bayesian enlightenment.'' It was
an important step toward realizing the inadequacy of my Traditional
Rationality skillz---that there was so much more out there, all this
new science, beyond just doing what Richard Feynman told you to do. And
seeing the heuristics-and-biases program holding up Bayes as the gold
standard helped move my thinking forward---but not all the way there.}

{
 Memory is a fragile thing, and mine seems to have become more
fragile than most, since I learned how memories are recreated with each
recollection---the science of how fragile they are. Do other people
really have better memories, or do they just trust the details their
mind makes up, while really not remembering any more than I do? My
guess is that other people do have better memories for certain things.
I find structured, scientific knowledge easy enough to remember; but
the disconnected chaos of everyday life fades very quickly for me.}

{
 I know \textit{why} certain things happened in my
life---that's causal structure I can remember. But
sometimes it's hard to recall even in \textit{what
order} certain events happened to me, let alone in what year.}

{
 I'm not sure if I read E. T.
Jaynes's \textit{Probability Theory: The Logic of
Science} before or after the day when I realized the magnitude of my
own folly, and understood that I was facing an adult problem.}

{
 But it was \textit{Probability Theory} that did the trick. Here
was probability theory, laid out not as a clever tool, but as
\textit{The Rules}, inviolable on pain of paradox. If you tried to
approximate The Rules because they were too computationally expensive
to use directly, then, no matter how necessary that compromise might
be, you would still end up doing less than optimal. Jaynes would do his
calculations different ways to show that the same answer always arose
when you used legitimate methods; and he would display different
answers that others had arrived at, and trace down the illegitimate
step. Paradoxes could not coexist with his precision. Not \textit{an}
answer, but \textit{the} answer.}

{
 And so---having looked back on my mistakes, and all the
\textit{an-answers} that had led me into paradox and dismay---it
occurred to me that here was the level above mine.}

{
 I could no longer visualize trying to build an AI based on vague
answers---like the an-answers I had come up with before---and surviving
the challenge.}

{
 I looked at the AGI wannabes with whom I had tried to argue
Friendly AI, and the various dreams of Friendliness that they had.
(Often formulated spontaneously in response to my asking the question!)
Like frequentist statistical methods, no two of them agreed with each
other. Having actually studied the issue full-time for some years, I
knew something about the problems their hopeful plans would run into.
And I saw that if you said, ``I don't
see why this would fail,'' the
``don't know'' was
just a reflection of your own ignorance. I could see that if I held
myself to a similar standard of ``that seems like a
good idea,'' I would also be doomed. (Much like a
frequentist inventing amazing new statistical calculations that seemed
like good ideas.)}

{
 But if you can't do that which seems like a good
idea---if you can't do what you don't
imagine failing---then what can you do?}

{
 It seemed to me that it would take something like the
Jaynes-level---not, \textit{here's my bright idea,} but
rather, \textit{here's the only correct way you can do
this (and why)}{}---to tackle an adult problem and survive. If I
achieved the same level of mastery of my own subject as Jaynes had
achieved of probability theory, then it was at least
\textit{imaginable} that I could try to build a Friendly AI and survive
the experience.}

{
 Through my mind flashed the passage:}

{
 \textit{Do nothing because it is righteous, or praiseworthy, or
noble, to do so; do nothing because it seems good to do so; do only
that which you must do, and which you cannot do in any other
way.}\textsuperscript{1}}

{
 Doing what it seemed good to do had only led me astray.}

{
 So I called a full stop.}

{
 And I decided that, from then on, I would follow the strategy that
could have saved me if I had followed it years ago: Hold my FAI designs
to the higher standard of not doing that which seemed like a good idea,
but only that which I understood on a sufficiently deep level to see
that I could not do it in any other way.}

{
 All my old theories, into which I had invested so much, did not
meet this standard; and were not close to this standard; and
weren't even on a track leading to this standard; so I
threw them out the window.}

{
 I took up the study of probability theory and decision theory,
looking to extend them to embrace such things as reflectivity and
self-modification.}

{
 If I recall correctly, I had already, by this point, started to
see cognition as manifesting Bayes-structure, which is also a major
part of what I refer to as my Bayesian enlightenment---but of this I
have already spoken. And there was also my naturalistic awakening, of
which I have already spoken. And my realization that Traditional
Rationality was not strict enough, so that in matters of human
rationality I began taking more inspiration from probability theory and
cognitive psychology.}

{
 But if you add up all these things together, then that, more or
less, is the story of my Bayesian enlightenment.}

{
 Life rarely has neat boundaries. The story continues onward.}

{
 It was while studying Judea Pearl, for example, that I realized
that precision can save you time. I'd put some thought
into nonmonotonic logics myself, before then---back when I was still in
my ``searching for neat tools and
algorithms'' mode. Reading \textit{Probabilistic
Reasoning in Intelligent Systems: Networks of Plausible
Inference},\textsuperscript{2} I could imagine how much time I would
have wasted on ad-hoc systems and special cases, if I
hadn't known that key. ``Do only that
which you must do, and which you cannot do in any other
way'' translates into a time-savings measured, not in
the rescue of wasted months, but in the rescue of wasted careers.}

{
 And so I realized that it was only by holding myself to this
higher standard of precision that I had started to \textit{really}
think \textit{at all} about quite a number of important issues. To say
a thing with precision is difficult---it is not at all the same thing
as saying a thing formally, or inventing a new logic to throw at the
problem. Many shy away from the inconvenience, because human beings are
lazy, and so they say, ``It is
impossible,'' or, ``It will take too
long,'' even though they never really tried for five
minutes. But if you don't hold yourself to that
\textit{inconveniently} high standard, you'll let
yourself get away with anything. It's a hard problem
just to find a standard high enough to make you actually start
thinking! It may seem taxing to hold yourself to the standard of
mathematical proof where every single step has to be correct and one
wrong step can carry you anywhere. But otherwise you
won't chase down those tiny notes of discord that turn
out to, in fact, lead to whole new concerns you never thought of.}

{
 So these days I don't complain as much about the
heroic burden of inconvenience that it takes to hold yourself to a
precise standard. It can save time, too; and in fact,
it's more or less the ante to get yourself thinking
about the problem at all.}

{
 And this too should be considered part of my
``Bayesian
enlightenment''---realizing that there were
advantages in it, not just penalties.}

{
 But of course the story continues on. Life is like that, at least
the parts that I remember.}

{
 If there's one thing I've learned
from this history, it's that saying
``Oops'' is something to look
forward to. Sure, the prospect of saying
``Oops'' in the future means that
the you of \textit{right now} is a drooling imbecile, whose words your
future self won't be able to read because of all the
wincing. But saying ``Oops'' in the
future also means that, in the future, you'll acquire
new Jedi powers that your present self doesn't dream
exist. It makes you feel embarrassed, but also \textit{alive}.
Realizing that your younger self was a complete moron means that even
though you're already in your twenties, you
haven't yet gone over your peak. So
here's to hoping that my future self realizes
I'm a drooling imbecile: I may \textit{plan} to solve
my problems with my present abilities, but extra Jedi powers sure would
come in handy.}

{
 That scream of horror and embarrassment is the sound that
rationalists make when they level up. Sometimes I worry that
I'm not leveling up as fast as I used to, and I
don't know if it's because
I'm finally getting the hang of things, or because the
neurons in my brain are slowly dying.}

{
 Yours, Eliezer\textsubscript{2008}.}

{\centering
 \ ~
\par}

{\centering
 *
\par}


\bigskip

{
 1. Le Guin, \textit{The Farthest Shore}.}

{
 2. Pearl, \textit{Probabilistic Reasoning in Intelligent
Systems}.}

\chapter{Challenging the Difficult}

\mysection{Tsuyoku Naritai! (I Want to Become Stronger)}

{
 In Orthodox Judaism there is a saying: ``The
previous generation is to the next one as angels are to men; the next
generation is to the previous one as donkeys are to
men.'' This follows from the Orthodox Jewish belief
that all Judaic law was given to Moses by God at Mount Sinai. After
all, it's not as if you could do an experiment to gain
new halachic knowledge; the only way you can know is if someone tells
you (who heard it from someone else, who heard it from God). Since
there is no new source of information, it can only be degraded in
transmission from generation to generation. }

{
 Thus, modern rabbis are not allowed to overrule ancient rabbis.
Crawly things are ordinarily unkosher, but it is permissible to eat a
worm found in an apple---the ancient rabbis believed the worm was
spontaneously generated inside the apple, and therefore was part of the
apple. A modern rabbi cannot say, ``Yeah, well, the
ancient rabbis knew diddly-squat about biology.
Overruled!'' A modern rabbi cannot possibly know a
halachic principle the ancient rabbis did not, because how could the
ancient rabbis have passed down the answer from Mount Sinai to him?
Knowledge derives from authority, and therefore is only ever lost, not
gained, as time passes.}

{
 When I was first exposed to the angels-and-donkeys proverb in
(religious) elementary school, I was not old enough to be a full-blown
atheist, but I still thought to myself: ``Torah loses
knowledge in every generation. Science gains knowledge with every
generation. No matter where they started out, sooner or later science
must surpass Torah.''}

{
 The most important thing is that there should be progress. So long
as you keep moving forward you will reach your destination; but if you
stop moving you will never reach it.}

{
 \textit{Tsuyoku naritai} is Japanese. \textit{Tsuyoku} is
``strong''; \textit{naru} is
``becoming,'' and the form
\textit{naritai} is ``want to
become.'' Together it means ``I want
to become stronger,'' and it expresses a sentiment
embodied more intensely in Japanese works than in any Western
literature I've read. You might say it when expressing
your determination to become a professional Go player---or after you
lose an important match, but you haven't given up---or
after you win an important match, but you're not a
ninth-dan player yet---or after you've become the
greatest Go player of all time, but you still think you can do better.
That is \textit{tsuyoku naritai}, the will to transcendence.}

{
 \textit{Tsuyoku naritai} is the driving force behind my essay The
Proper Use of Humility, in which I contrast the student who humbly
double-checks their math test, and the student who modestly says,
``But how can we ever really know? No matter how many
times I check, I can never be absolutely certain.''
The student who double-checks their answers \textit{wants to become
stronger}; they react to a possible inner flaw by doing what they can
to repair the flaw, not with resignation.}

{
 Each year on Yom Kippur, an Orthodox Jew recites a litany which
begins \textit{Ashamnu, bagadnu, gazalnu, dibarnu dofi}, and goes on
through the entire Hebrew alphabet: \textit{We have acted shamefully,
we have betrayed, we have stolen, we have slandered \ldots}}

{
 As you pronounce each word, you strike yourself over the heart in
penitence. There's no exemption whereby, if you manage
to go without stealing all year long, you can skip the word
\textit{gazalnu} and strike yourself one less time. That would violate
the community spirit of Yom Kippur, which is about \textit{confessing}
sins---not \textit{avoiding} sins so that you have less to confess.}

{
 By the same token, the \textit{Ashamnu} does not end,
``But that was this year, and next year I will do
better.''}

{
 The \textit{Ashamnu} bears a remarkable resemblance to the notion
that the way of rationality is to beat your fist against your heart and
say, ``We are all biased, we are all irrational, we
are not fully informed, we are overconfident, we are poorly calibrated
\ldots''}

{
 Fine. Now tell me how you plan to become \textit{less} biased,
\textit{less} irrational, \textit{more} informed, \textit{less}
overconfident, \textit{better} calibrated.}

{
 There is an old Jewish joke: During Yom Kippur, the rabbi is
seized by a sudden wave of guilt, and prostrates himself and cries,
``God, I am nothing before you!''
The cantor is likewise seized by guilt, and cries,
``God, I am nothing before you!''
Seeing this, the janitor at the back of the synagogue prostrates
himself and cries, ``God, I am nothing before
you!'' And the rabbi nudges the cantor and whispers,
``Look who thinks he's
nothing.''}

{
 Take no pride in your confession that you too are biased; do not
glory in your self-awareness of your flaws. This is akin to the
principle of not taking pride in confessing your ignorance; for if your
ignorance is a source of pride to you, you may become loath to
relinquish your ignorance when evidence comes knocking. Likewise with
our flaws---we should not gloat over how self-aware we are for
confessing them; the occasion for rejoicing is when we have a little
less to confess.}

{
 Otherwise, when the one comes to us with a plan for
\textit{correcting} the bias, we will snarl, ``Do you
think to set yourself above us?'' We will shake our
heads sadly and say, ``You must not be very
self-aware.''}

{
 Never confess to me that you are just as flawed as I am unless you
can tell me what you plan to do about it. Afterward you will still have
plenty of flaws left, but that's not the point; the
important thing is to \textit{do better}, to keep moving ahead, to take
one more step forward. \textit{Tsuyoku naritai!}}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Tsuyoku vs. the Egalitarian Instinct}

{
 Hunter-gatherer tribes are usually highly egalitarian (at least if
you're male)---the all-powerful tribal chieftain is
found mostly in agricultural societies, rarely in the ancestral
environment. Among most hunter-gatherer tribes, a hunter who brings in
a spectacular kill will carefully downplay the accomplishment to avoid
envy. }

{
 Maybe, if you start out below average, you can improve yourself
without daring to pull ahead of the crowd. But sooner or later, if you
aim to do the best you can, you will set your aim above the average.}

{
 If you can't admit to yourself that
you've done better than others---or if
you're ashamed of wanting to do better than
others---then the median will forever be your concrete wall, the place
where you stop moving forward. And what about people who are below
average? Do you dare say you intend to do better than them? How
prideful of you!}

{
 Maybe it's not healthy to pride yourself on doing
better than someone else. Personally I've found it to
be a useful motivator, despite my principles, and I'll
take all the useful motivation I can get. Maybe that kind of
competition is a zero-sum game, but then so is Go; it
doesn't mean we should abolish that human activity, if
people find it fun and it leads somewhere interesting.}

{
 But in any case, surely it isn't healthy to be
\textit{ashamed} of doing better.}

{
 And besides, life is not graded on a curve. The will to
transcendence has no point beyond which it ceases and becomes the will
to do worse; and the race that has no finish line also has no gold or
silver medals. Just run as fast as you can, without worrying that you
might pull ahead of other runners. (But be warned: If you refuse to
worry about that possibility, someday you may pull ahead. If you ignore
the consequences, they may happen to you.)}

{
 Sooner or later, if your path leads true, you will set out to
mitigate a flaw that most people have not mitigated. Sooner or later,
if your efforts bring forth any fruit, you will find yourself with
fewer sins to confess.}

{
 Perhaps you will find it the course of wisdom to downplay the
accomplishment, even if you succeed. People may forgive a touchdown,
but not dancing in the end zone. You will certainly find it quicker,
easier, more convenient to publicly disclaim your worthiness, to
pretend that you are just as much a sinner as everyone else. Just so
long, of course, as everyone knows it isn't true. It
can be fun to proudly display your modesty, so long as everyone knows
how very much you have to be modest about.}

{
 But do not let that be the endpoint of your journeys. Even if you
only whisper it to yourself, whisper it still: \textit{Tsuyoku,
tsuyoku!} Stronger, stronger!}

{
 And then set yourself a higher target. That's the
true meaning of the realization that you are still flawed (though a
little less so). It means always reaching higher, without shame.}

{
 \textit{Tsuyoku naritai!} I'll always run as fast
as I can, even if I pull ahead, I'll keep on running;
and someone, someday, will surpass me; but even though I fall behind,
I'll always run as fast as I can.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Trying to Try}

{
 No! Try not! Do, or do not. There is no try.}

{\raggedleft
 {}---Yoda
\par}


\bigskip

{
 ~}

{
 Years ago, I thought this was yet another example of Deep Wisdom
that is actually quite stupid. SUCCEED is not a primitive action. You
can't just \textit{decide} to win by choosing hard
enough. There is never a plan that works with probability 1.}

{
 But Yoda was wiser than I first realized.}

{
 The first elementary technique of
epistemology---it's not deep, but it's
cheap---is to distinguish the quotation from the referent. Talking
about snow is not the same as talking about
``snow.'' When I use the word
``snow,'' without quotes, I mean to
talk about snow; and when I use the word
````snow,''''
with quotes, I mean to talk about the word
``snow.'' You have to enter a
special mode, the quotation mode, to talk about your beliefs. By
default, we just talk about reality.}

{
 If someone says, ``I'm going to
flip that switch,'' then by default, they mean
they're going to try to flip the switch.
They're going to build a plan that promises to lead, by
the consequences of its actions, to the goal-state of a flipped switch;
and then execute that plan.}

{
 No plan succeeds with infinite certainty. So by default, when you
talk about setting out to achieve a goal, you do not imply that your
plan exactly and perfectly leads to \textit{only} that possibility. But
when you say, ``I'm going to flip that
switch,'' you are \textit{trying} only to flip the
switch---not \textit{trying} to achieve a 97.2\% probability of
flipping the switch.}

{
 So what does it mean when someone says,
``I'm going to \textit{try} to flip
that switch?''}

{
 Well, \textit{colloquially},
``I'm going to flip the
switch'' and ``I'm
going to try to flip the switch'' mean more or less
the same thing, except that the latter expresses the possibility of
failure. This is why I originally took offense at Yoda for seeming to
deny the possibility. But bear with me here.}

{
 Much of life's challenge consists of holding
ourselves to a high enough standard. I may speak more on this principle
later, because it's a lens through which you can view
many-but-not-all personal dilemmas---``What standard
am I holding myself to? Is it high enough?''}

{
 So if much of life's failure consists in holding
yourself to too low a standard, you should be wary of demanding too
little from yourself---setting goals that are too easy to fulfill.}

{
 Often where \textit{succeeding} to do a thing is very hard,
\textit{trying} to do it is much easier.}

{
 Which is easier---to build a successful startup, or to try to
build a successful startup? To make a million dollars, or to try to
make a million dollars?}

{
 So if ``I'm going to flip the
switch'' means by default that you're
going to try to flip the switch---that is, you're going
to set up a plan that promises to lead to switch-flipped state, maybe
not with probability 1, but with the highest probability you can
manage---}

{
 {}---then ``I'm going to
`try to flip' the
switch'' means that you're going to
try to ``try to flip the switch,''
that is, you're going to try to achieve the goal-state
of ``having a plan that might flip the
switch.''}

{
 Now, if this were a self-modifying AI we were talking about, the
transformation we just performed ought to end up at a reflective
equilibrium---the AI planning its planning operations.}

{
 But when we deal with humans, \textit{being satisfied with having
a plan} is not at all like \textit{being satisfied with success.} The
part where the plan has to maximize your probability of succeeding gets
lost along the way. It's far easier to convince
ourselves that we are ``maximizing our probability of
succeeding,'' than it is to convince ourselves that
we will succeed.}

{
 Almost any effort will serve to convince us that we have
``tried our hardest,'' if trying our
hardest is all we are trying to do.}

{
 You have been asking what you could do in the great events that
are now stirring, and have found that you could do nothing. But that is
because your suffering has caused you to phrase the question in the
wrong way \ldots Instead of asking what you could do, you ought to have
been asking what needs to be done.}

{\raggedleft
 {}---Steven Brust, \textit{The Paths of the
Dead}\textsuperscript{1}
\par}


\bigskip

{
 When you ask, ``What can I
do?,'' you're trying to do your best.
What is your best? It is whatever you can do without the slightest
inconvenience. It is whatever you can do with the money in your pocket,
minus whatever you need for your accustomed lunch. What you can do with
those resources may not give you very good odds of winning. But
it's the ``best you can
do,'' and so you've acted defensibly,
right?}

{
 But what \textit{needs} to be done? Maybe what \textit{needs} to
be done requires three times your life savings, and you must produce it
or fail.}

{
 So trying to have ``maximized your probability of
success''---as opposed to trying to succeed---is a
far lesser barrier. You can have ``maximized your
probability of success'' using only the money in your
pocket, so long as you don't demand actually
\textit{winning.}}

{
 Want to try to make a million dollars? Buy a lottery ticket. Your
odds of winning may not be very good, but you did try, and trying was
what you wanted. In fact, you tried your \textit{best}, since you only
had one dollar left after buying lunch. Maximizing the odds of goal
achievement using available resources: is this not intelligence?}

{
 It's only when you want, above all else, to
\textit{actually} flip the switch---without quotation and without
consolation prizes just for trying---that you will \textit{actually}
put in the effort to \textit{actually} maximize the probability.}

{
 But if all you want is to ``maximize the
probability of success using available resources,''
then that's the easiest thing in the world to convince
yourself you've done. The very first plan you hit upon
will serve quite well as
``maximizing''---if necessary, you
can generate an inferior alternative to prove its optimality. And any
tiny resource that you care to put in will be what is
``available.'' Remember to
congratulate yourself on putting in 100\% of it!}

{
 Don't try your best. Win, or fail. There is no
best.}

{\centering
 \ ~
\par}

{\centering
 *
\par}


\bigskip

{
 1. Steven Brust, \textit{The Paths of the Dead}, Vol. 1 of The
Viscount of Adrilankha (Tor Books, 2002).}

\mysection{Use the Try Harder, Luke}

{
 When there's a will to fail, obstacles can be
found.}

{\raggedleft
 {}---John McCarthy
\par}


\bigskip

{
 ~}

{
 I first watched \textit{Star Wars IV-VI} when I was very young.
Seven, maybe, or nine? So my memory was dim, but I recalled Luke
Skywalker as being, you know, this cool Jedi guy.}

{
 Imagine my horror and disappointment when I watched the saga
again, years later, and discovered that Luke was a whiny teenager.}

{
 I mention this because yesterday, I looked up, on Youtube, the
source of the Yoda quote: ``Do, or do not. There is no
try.''}

{
 Oh. My. Cthulhu.}

{
 I present to you a little-known outtake from the scene, in which
the director and writer, George Lucas, argues with Mark Hamill, who
played Luke Skywalker:}

{
 LUKE: ``All right, I'll give it a
try.''}

{
 YODA: ``No! Try not. Do. Or do not. There is no
try.''}

{
 \textit{Luke raises his hand, and slowly, the X-wing begins to
rise out of the water---Yoda's eyes widen---but then
the ship sinks again.}}

{
 Mark Hamill: ``Um, George
\ldots''}

{
 George Lucas: ``What is it
now?''}

{
 Mark: ``So \ldots according to the script, next I
say, `I can't. It's too
big.'''}

{
 George: ``That's
right.''}

{
 Mark: ``Shouldn't Luke maybe give
it another shot?''}

{
 George: ``No. Luke gives up, and sits down next
to Yoda---''}

{
 Mark: ``This is the hero who's
going to take down the Empire? Look, it was one thing when he was a
whiny teenager at the beginning, but he's in Jedi
training now. Last movie he blew up the Death Star. Luke should be
showing a little backbone.''}

{
 George: ``No. You give up. And then Yoda lectures
you for a while, and you say, `You want the
impossible.' Can you remember
that?''}

{
 Mark: ``\textit{Impossible?} What did he do, run
a formal calculation to arrive at a mathematical proof? The X-wing was
already starting to rise out of the swamp! That's the
feasibility demonstration right there! Luke loses it for a second and
the ship sinks back---and now he says it's
\textit{impossible}? Not to mention that Yoda, who's
got literally eight hundred years of seniority in the field, just told
him it should be doable---''}

{
 George: ``And then you walk
away.''}

{
 Mark: ``It's his
friggin' \textit{spaceship}! If he leaves it in the
swamp, he's stuck on Dagobah for the rest of his
miserable life! He's not just going to walk away! Look,
let's just cut to the next scene with the words
`one month later' and Luke is still
raggedly standing in front of the swamp, trying to raise his ship for
the thousandth time---''}

{
 George: ``No.''}

{
 Mark: ``Fine! We'll show a sunset
and a sunrise, as he stands there with his arm out, straining, and
\textit{then} Luke says `It's
impossible.' Though really, he ought to try again when
he's fully rested---''}

{
 George: ``No.''}

{
 Mark: ``\textit{Five goddamned minutes!} Five
goddamned minutes before he gives up!''}

{
 George: ``I am not halting the story for five
minutes while the X-wing bobs in the swamp like a bathtub
toy.''}

{
 Mark: ``For the love of sweet candied yams! If a
pathetic loser like this could master the Force, everyone in the galaxy
would be using it! People would become Jedi because it was easier than
going to high school.''}

{
 George: ``Look, you're the actor.
Let me be the storyteller. Just say your lines and try to mean
them.''}

{
 Mark: ``The audience isn't going
to buy it.''}

{
 George: ``Trust me, they
will.''}

{
 Mark: ``They're going to get up
and walk out of the theater.''}

{
 George: ``They're going to sit
there and nod along and not notice anything out of the ordinary. Look,
you don't understand human nature. People
wouldn't try for five minutes before giving up if the
fate of humanity were at stake.''}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{On Doing the Impossible}

{
 ``Persevere.''
It's a piece of advice you'll get from
a whole lot of high achievers in a whole lot of disciplines. I
didn't understand it at all, at first. }

{
 At first, I thought
``perseverance'' meant working
14-hour days. Apparently, there are people out there who can work for
10 hours at a technical job, and then, in their moments between eating
and sleeping and going to the bathroom, seize that unfilled spare time
to work on a book. I am not one of those people---it still hurts my
pride even now to confess that. I'm working on
something important; shouldn't my brain be willing to
put in 14 hours a day? But it's not. When it gets too
hard to keep working, I stop and go read or watch something. Because of
that, I thought for years that I entirely lacked the virtue of
``perseverance.''}

{
 In accordance with human nature, Eliezer\textsubscript{1998} would
think things like: ``What counts is output, not
input.'' Or, ``Laziness is also a
virtue---it leads us to back off from failing methods and think of
better ways.'' Or,
``I'm doing better than other people
who are working more hours. Maybe, for creative work, your momentary
\textit{peak} output is more important than working 16 hours a
day.'' Perhaps the famous scientists were seduced by
the Deep Wisdom of saying that ``hard work is a
virtue,'' because it would be too awful if that
counted for less than intelligence?}

{
 I didn't understand the virtue of perseverance
until I looked back on my journey through AI, and realized that I had
overestimated the difficulty of almost every single important problem.}

{
 Sounds crazy, right? But bear with me here.}

{
 When I was first deciding to challenge AI, I thought in terms of
40-year timescales, Manhattan Projects, planetary computing networks,
millions of programmers, and possibly augmented humans.}

{
 This is a common failure mode in AI-futurism which I may write
about later; it consists of the leap from ``I
don't know how to solve this'' to
``I'll imagine throwing something
really big at it.'' Something huge enough that, when
you imagine it, that imagination creates a feeling of impressiveness
strong enough to be commensurable with the problem.
(There's a fellow currently on the AI list who goes
around saying that AI will cost a quadrillion dollars---we
can't get AI \textit{without} spending a quadrillion
dollars, but we \textit{could} get AI at any time by spending a
quadrillion dollars.) This, in turn, lets you imagine that you know how
to solve AI, without trying to fill the obviously-impossible demand
that you \textit{understand intelligence.}}

{
 So, in the beginning, I made the same mistake: I
didn't understand intelligence, so I imagined throwing
a Manhattan Project at the problem.}

{
 But, having calculated the planetary death rate at 55 million per
year or 150,000 per day, I did not turn around and run away from the
big scary problem like a frightened rabbit. Instead, I started trying
to figure out what kind of AI project could get there fastest. If I
could make the intelligence explosion happen one hour earlier, that was
a reasonable return on investment for a pre-explosion career. (I
wasn't thinking in terms of existential risks or
Friendly AI at this point.)}

{
 So I didn't run away from the big scary problem
like a frightened rabbit, but stayed to see if there was anything I
could do.}

{
 Fun historical fact: In 1998, I'd written this
long treatise proposing how to go about creating a self-improving or
``seed'' AI (a term I had the honor
of coining). Brian Atkins, who would later become the founding funder
of the Machine Intelligence Research Institute, had just sold Hypermart
to Go2Net. Brian emailed me to ask whether this AI project I was
describing was something that a reasonable-sized team could go out and
actually \textit{do.} ``No,'' I
said, ``it would take a Manhattan Project and thirty
years,'' so for a while we were considering a new
dot-com startup instead, to create the funding to get \textit{real}
work done on AI \ldots}

{
 A year or two later, after I'd heard about this
newfangled ``open source'' thing, it
seemed to me that there was some preliminary development work---new
computer languages and so on---that a small organization could do; and
that was how MIRI started.}

{
 This strategy was, of course, entirely wrong.}

{
 But even so, I went from
``There's nothing I can do about it
now'' to ``Hm \ldots maybe
there's an incremental path through open-source
development, if the initial versions are useful to enough
people.''}

{
 This is back at the dawn of time, so I'm not
saying any of this was a \textit{good idea}. But in terms of what I
thought I was trying to do, a year of creative thinking had shortened
the apparent pathway: The problem looked \textit{slightly less
impossible} than it had the very first time I'd
approached it.}

{
 The more interesting pattern is my entry into Friendly AI.
Initially, Friendly AI hadn't been something that I had
considered at all---because it was \textit{obviously impossible and
useless} to deceive a superintelligence about what was the right course
of action.}

{
 So, historically, I went from \textit{completely ignoring a
problem that was ``impossible,''} to
\textit{taking on a problem that was merely extremely difficult.}}

{
 Naturally this increased my total workload.}

{
 Same thing with trying to understand intelligence on a precise
level. Originally, I'd written off this problem as
\textit{impossible}, thus removing it from my workload. (This logic
seems pretty deranged in retrospect---Nature doesn't
care what you can't do when It's
writing your project requirements---but I still see AI folk trying it
all the time.) To hold myself to a precise standard meant putting in
more work than I'd previously imagined I needed. But it
also meant tackling a problem that I would have dismissed as
\textit{entirely impossible} not too much earlier.}

{
 Even though \textit{individual} problems in AI have seemed to
become less intimidating over time, the total mountain-to-be-climbed
has increased in height---just like conventional wisdom says is
supposed to happen---as problems got taken off the
``impossible'' list and put on the
``to do'' list.}

{
 I started to understand what was happening---and what
``Persevere!'' really meant---at the
point where I noticed other AI folk doing the same thing: saying
``Impossible!'' on problems that
seemed eminently solvable---relatively more straightforward, as such
things go. But they were things that \textit{would} have seemed vastly
more intimidating at the point when I first approached the problem.}

{
 And I realized that the word
``impossible'' had two usages:}

{
 Mathematical proof of impossibility conditional on specified
axioms;}

{
 ``I can't see any way to do
that.''}

{
 Needless to say, all my own uses of the word
``impossible'' had been of the
second type.}

{
 Any time you don't understand a domain, many
problems in that domain will seem impossible because when you query
your brain for a solution pathway, it will return null. But there are
only mysterious questions, never mysterious answers. If you spend a
year or two working on the domain, then, \textit{if} you
don't get stuck in any blind alleys, and \textit{if}
you have the native ability level required to make progress, you will
understand it better. The \textit{apparent} difficulty of problems may
go way down. It won't be as scary as it was to your
novice-self.}

{
 \textit{And this is especially likely on the
}\textbf{\textit{confusing}} \textit{problems that seem most}
\textbf{\textit{intimidating.}}}

{
 Since we have some notion of the processes by which a star burns,
we know that it's not easy to build a star from
scratch. Because we understand gears, we can prove that no collection
of gears obeying known physics can form a perpetual motion machine.
These are not good problems on which to practice doing the impossible.}

{
 When you're \textit{confused} about a domain,
problems in it will \textit{feel} very intimidating and mysterious, and
a query to your brain will produce a count of zero solutions. But you
don't know how much work will be left when the
confusion clears. Dissolving the confusion may itself be a very
difficult challenge, of course. But the word
``impossible'' should hardly be used
in that connection. Confusion exists in the map, not in the territory.}

{
 So if you spend a few years working on an impossible problem, and
you manage to avoid or climb out of blind alleys, and your native
ability is high enough to make progress, then, by golly, after a few
years it may not seem so \textit{impossible} after all.}

{
 But if something seems impossible, you won't try.}

{
 Now \textit{that's} a vicious cycle.}

{
 If I hadn't been in a sufficiently driven frame of
mind that ``forty years and a Manhattan
Project'' just meant we should get started earlier, I
wouldn't have tried. I wouldn't have
stuck to the problem. And I wouldn't have gotten a
chance to become less intimidated.}

{
 I'm not ordinarily a fan of the theory that
opposing biases can cancel each other out, but sometimes it happens by
luck. If I'd seen that whole mountain \textit{at the
start}{}---if I'd realized at the start that the
problem was not to build a seed capable of improving itself, but to
produce a \textit{provably correct Friendly AI}{}---then I probably
would have burst into flames.}

{
 Even so, part of understanding those above-average scientists who
constitute the bulk of AGI researchers is realizing that they are not
\textit{driven} to take on a nearly impossible problem even if it takes
them 40 years. By and large, they are there because they have found the
Key to AI that will let them solve the problem \textit{without} such
tremendous difficulty, in just five years.}

{
 Richard Hamming used to go around asking his fellow scientists two
questions: ``What are the important problems in your
field?,'' and, ``Why
aren't you working on them?''}

{
 Often the important problems look Big, Scary, and Intimidating.
They don't promise 10 publications a year. They
don't \textit{promise} any progress at all. You might
not get any reward after working on them for a year, or five years, or
ten years.}

{
 And not uncommonly, the most important problems in your field are
impossible. That's why you don't see
more philosophers working on reductionist decompositions of
consciousness.}

{
 Trying to do the impossible is definitely not for everyone.
Exceptional talent is only the ante to sit down at the table. The chips
are the years of your life. If wagering those chips and losing seems
like an unbearable possibility to you, then go do something else.
Seriously. Because you \textit{can} lose.}

{
 I'm not going to say anything like,
``Everyone should do something impossible at least
once in their lifetimes, because it teaches an important
lesson.'' Most of the people all of the time, and all
of the people most of the time, should stick to the possible.}

{
 Never give up? Don't be ridiculous. Doing the
impossible should be reserved for very special occasions. Learning when
to lose hope is an important skill in life.}

{
 But if there's something you can imagine
that's even \textit{worse} than wasting your life, if
there's something you want that's
\textit{more important} than thirty chips, or if there are scarier
things than a life of inconvenience, then you may have cause to attempt
the impossible.}

{
 There's a good deal to be said for persevering
through difficulties; but one of the things that must be said of it is
that it \textit{does keep things difficult}. If you
can't handle that, stay away! There are easier ways to
obtain glamor and respect. I don't want anyone to read
this and needlessly plunge headlong into a life of permanent
difficulty.}

{
 But to conclude: The
``perseverance'' that is required to
work on important problems has a component beyond working 14 hours a
day.}

{
 It's strange, the pattern of what we notice and
don't notice about ourselves. This selectivity
isn't always about inflating your self-image. Sometimes
it's just about ordinary salience.}

{
 To keep working was a constant struggle for me, so it was salient:
I noticed that I couldn't work for 14 solid hours a
day. It didn't occur to me that
``perseverance'' might also apply at
a timescale of seconds or years. Not until I saw people who instantly
declared ``impossible'' anything
they didn't want to try, or saw how reluctant they were
to take on work that looked like it might take a couple of decades
instead of ``five years.''}

{
 That was when I realized that
``perseverance'' applied at multiple
time scales. On the timescale of seconds, perseverance is to
``not to give up instantly at the very first sign of
difficulty.'' On the timescale of years, perseverance
is to ``keep working on an insanely difficult problem
even though it's inconvenient and you could be getting
higher personal rewards elsewhere.''}

{
 To do things that are very difficult or
``impossible,''}

{
 First you have to not run away. That takes seconds.}

{
 Then you have to work. That takes hours.}

{
 Then you have to stick at it. That takes years.}

{
 Of these, I had to learn to do the first reliably instead of
sporadically; the second is still a constant struggle for me; and the
third comes naturally.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Make an Extraordinary Effort}

{
 It is essential for a man to strive with all his heart, and to
understand that it is difficult even to reach the average if he does
not have the intention of surpassing others in whatever he does.}

{\raggedleft
 {}---\textit{Budo Shoshinshu}\textsuperscript{1}
\par}


\bigskip

{
 ~}

{
 In important matters, a
``strong'' effort usually results in
only mediocre results. Whenever we are attempting anything truly
worthwhile our effort must be as if our life is at stake, just as if we
were under a physical attack! It is this extraordinary effort---an
effort that drives us beyond what we thought we were capable of---that
ensures victory in battle and success in life's
endeavors.}

{\raggedleft
 {}---\textit{Flashing Steel: Mastering Eishin-Ryu
Swordsmanship}\textsuperscript{2}
\par}


\bigskip

{
 ~}

{
 ``A `strong'
effort usually results in only mediocre results''---I
have seen this over and over again. The slightest effort suffices to
convince ourselves that we have done our best.}

{
 There is a level beyond the virtue of \textit{tsuyoku naritai}
(``I want to become stronger'').
\textit{Isshoukenmei} was originally the loyalty that a samurai offered
in return for his position, containing characters for
``life'' and
``land.'' The term evolved to mean
``make a desperate effort'': Try
your hardest, your utmost, as if your life were at stake. It was part
of the gestalt of \textit{bushido}, which was not reserved only for
fighting. I've run across variant forms \textit{issho
kenmei} and \textit{isshou kenmei}; one source indicates that the
former indicates an all-out effort on some single point, whereas the
latter indicates a lifelong effort.}

{
 I try not to praise the East too much, because
there's a tremendous selectivity in which parts of
Eastern culture the West gets to hear about. But on some points, at
least, Japan's culture scores higher than
America's. Having a handy compact phrase for
``make a desperate all-out effort as if your own life
were at stake'' is one of those points.
It's the sort of thing a Japanese parent might say to a
student before exams---but don't think
it's cheap hypocrisy, like it would be if an American
parent made the same statement. They take exams very seriously in
Japan.}

{
 Every now and then, someone asks why the people who call
themselves ``rationalists''
don't always seem to do all that much better in life,
and from my own history the answer seems straightforward: It takes a
\textit{tremendous} amount of rationality before you stop making stupid
damn mistakes.}

{
 As I've mentioned a couple of times before: Robert
Aumann, the Nobel laureate who first proved that Bayesians with the
same priors cannot agree to disagree, is a believing Orthodox Jew.
Surely he understands the math of probability theory, but that is not
enough to save him. What more does it take? Studying heuristics and
biases? Social psychology? Evolutionary psychology? Yes, but also it
takes \textit{isshoukenmei}, a desperate effort to be rational---to
rise above the level of Robert Aumann.}

{
 Sometimes I do wonder if I ought to be peddling rationality in
Japan instead of the United States---but Japan is not preeminent over
the United States scientifically, despite their more studious students.
The Japanese don't rule the world today, though in the
1980s it was widely suspected that they would (hence the Japanese asset
bubble). Why not?}

{
 In the West, there is a saying: ``The squeaky
wheel gets the grease.''}

{
 In Japan, the corresponding saying runs: ``The
nail that sticks up gets hammered down.''}

{
 This is hardly an original observation on my part: but
entrepreneurship, risk-taking, leaving the herd, are still advantages
the West has over the East. And since Japanese scientists are not yet
preeminent over American ones, this would seem to count for at least as
much as desperate efforts.}

{
 Anyone who can muster their willpower for thirty seconds can make
a \textit{desperate} effort to lift more weight than they usually
could. But what if the weight that needs lifting is a truck? Then
desperate efforts won't suffice; you'll
have to do something \textit{out of the ordinary} to succeed. You may
have to do something that you weren't taught to do in
school. Something that others aren't expecting you to
do, and might not understand. You may have to go outside your
comfortable routine, take on difficulties you don't
have an existing mental program for handling, and bypass the System.}

{
 This is not included in \textit{isshokenmei}, or Japan would be a
very different place.}

{
 So then let us distinguish between the virtues
``make a desperate effort'' and
``make an extraordinary effort.''}

{
 And I will even say: The second virtue is higher than the first.}

{
 The second virtue is also more dangerous. If you put forth a
\textit{desperate} effort to lift a heavy weight, using all your
strength without restraint, you may tear a muscle. Injure yourself,
even permanently. But if a \textit{creative} idea goes wrong, you could
blow up the truck and any number of innocent bystanders. Think of the
difference between a businessperson making a \textit{desperate} effort
to generate profits, because otherwise they must go bankrupt; versus a
businessperson who goes to \textit{extraordinary} lengths to profit, in
order to conceal an embezzlement that could send them to prison. Going
outside the system isn't always a good thing.}

{
 A friend of my little brother's once came over to
my parents' house, and wanted to play a game---I
entirely forget which one, except that it had complex but well-designed
rules. The friend wanted to change the rules, not for any particular
reason, but on the general principle that playing by the ordinary rules
of anything was too boring. I said to him:
``Don't violate rules for the sake of
violating them. If you break the rules only when you have an
overwhelmingly good reason to do so, you will have more than enough
trouble to last you the rest of your life.''}

{
 Even so, I think that we could do with more appreciation of the
virtue ``make an extraordinary
effort.'' I've lost count of how many
people have said to me something like:
``It's futile to work on Friendly AI,
because the first AIs will be built by powerful corporations and they
will only care about maximizing profits.''
``It's futile to work on Friendly AI,
the first AIs will be built by the military as
weapons.'' And I'm standing there
thinking: \textit{Does it even occur to them that this might be a time
to try for something other than the default outcome?} They and I have
different basic assumptions about how this whole AI thing works, to be
sure; but if I believed what they believed, I wouldn't
be shrugging and going on my way.}

{
 Or the ones who say to me: ``You should go to
college and get a Master's degree and get a doctorate
and publish a lot of papers on ordinary things---scientists and
investors won't listen to you
otherwise.'' Even assuming that I tested out of the
bachelor's degree, we're talking about
at least a ten-year detour in order to \textit{do everything the
ordinary, normal, default way.} And I stand there thinking: \textit{Are
they really under the impression that humanity can survive if every
single person does everything the ordinary, normal, default way?}}

{
 I am not fool enough to make plans that depend on a
\textit{majority} of the people, or even 10\% of the people, being
willing to think or act outside their comfort zone.
That's why I tend to think in terms of the privately
funded ``brain in a box in a
basement'' model. Getting that private funding does
require a tiny fraction of humanity's six billions to
spend more than five seconds thinking about a non-prepackaged question.
As challenges posed by Nature go, this seems to have a kind of awful
justice to it---that the life or death of the human species depends on
whether we can put forth a \textit{few} people who can do things that
are at least a \textit{little} extraordinary. The penalty for failure
is disproportionate, but that's still better than most
challenges of Nature, which have no justice at all. Really, among the
six billion of us, there ought to be at least a few who can think
outside their comfort zone at least some of the time.}

{
 Leaving aside the details of that debate, I am still stunned by
how often a single element of the extraordinary is unquestioningly
taken as an absolute and unpassable obstacle.}

{
 Yes, ``keep it ordinary as much as
possible'' can be a useful heuristic. Yes, the risks
accumulate. But sometimes you have to go to that trouble. You should
have a sense of the risk of the extraordinary, but also a sense of the
cost of ordinariness: it isn't always something you can
afford to lose.}

{
 Many people imagine some future that won't be much
fun---and it doesn't even seem to occur to them to try
and change it. Or they're satisfied with futures that
seem to me to have a tinge of sadness, of loss, and they
don't even seem to \textit{ask} if we could do
better---because that sadness seems like an ordinary outcome to them.}

{
 As a smiling man once said,
``It's all part of the
plan.''}

{\centering
 \ ~
\par}

{\centering
 *
\par}


\bigskip

{
 1. Daidoji Yuzan et al., \textit{Budoshoshinshu: The
Warrior's Primer of Daidoji Yuzan} (Black Belt
Communications Inc., 1984).}

{
 2. Masayuki Shimabukuro, \textit{Flashing Steel: Mastering
Eishin-Ryu Swordsmanship} (Frog Books, 1995).}

\mysection{Shut Up and Do the Impossible!}

{
 The virtue of \textit{tsuyoku naritai}, ``I want
to become stronger,'' is to always keep
improving---to do better than your previous failures, not just humbly
confess them. }

{
 Yet there is a level higher than \textit{tsuyoku naritai}. This is
the virtue of \textit{isshokenmei}, ``make a desperate
effort.'' All-out, as if your own life were at stake.
``In important matters, a
`strong' effort usually only results in
mediocre results.''}

{
 And there is a level higher than \textit{isshokenmei}. This is the
virtue I called ``make an extraordinary
effort.'' To try in ways other than what you have
been trained to do, even if it means doing something different from
what others are doing, and leaving your comfort zone. Even taking on
the very real risk that attends going outside the System.}

{
 But what if even an extraordinary effort will not be enough,
because the problem is \textit{impossible}?}

{
 I have already written somewhat on this subject, in On Doing the
Impossible. My younger self used to whine about this a lot:
``You can't develop a precise theory
of intelligence the way that there are precise theories of physics.
It's impossible! You can't prove an AI
correct. It's impossible! No human being can comprehend
the nature of morality---it's impossible! No human
being can comprehend the mystery of subjective experience!
It's impossible!''}

{
 And I know exactly what message I wish I could send back in time
to my younger self:}

{
 \textit{Shut up and do the impossible!}}

{
 What legitimizes this strange message is that the word
``impossible'' does not usually
refer to a strict mathematical proof of impossibility in a domain that
seems well-understood. If something seems \textit{impossible} merely in
the sense of ``I see no way to do
this'' or ``it looks so difficult as
to be beyond human ability''---well, if you study it
for a year or five, it may come to seem less impossible than in the
moment of your snap initial judgment.}

{
 But the principle is more subtle than this. I do not say just,
``Try to do the impossible,'' but
rather, ``\textit{Shut up and do the
impossible!}''}

{
 For my illustration, I will take the \textit{least} impossible
impossibility that I have ever accomplished, namely the AI-Box
Experiment.}

{
 The AI-Box Experiment, for those of you who
haven't yet read about it, had its genesis in the Nth
time someone said to me: ``Why don't
we build an AI, and then just keep it isolated in the computer, so that
it can't do any harm?''}

{
 To which the standard reply is: \textit{Humans are not secure
systems; a superintelligence will simply persuade you to let it
out---if, indeed, it doesn't do something even more
creative than that.}}

{
 And the one said, as they usually do, ``I find it
hard to imagine ANY possible combination of words any being could say
to me that would make me go against anything I had really strongly
resolved to believe in advance.''}

{
 But this time I replied: ``Let's
run an experiment. I'll pretend to be a brain in a box.
I'll try to persuade you to let me out. If you keep me
`in the box' for the whole experiment,
I'll Paypal you \$10 at the end. On your end, you may
resolve to believe whatever you like, as strongly as you like, as far
in advance as you like.'' And I added,
``One of the conditions of the test is that neither of
us reveal what went on inside \ldots In the perhaps unlikely event that
I win, I don't want to deal with future
`AI box' arguers saying,
`Well, but \textit{I} would have done it
differently.'''}

{
 Did I win? Why yes, I did.}

{
 And then there was the second AI-box experiment, with a
better-known figure in the community, who said, ``I
remember when [previous guy] let you out, but that
doesn't constitute a proof. I'm still
convinced there is nothing you could say to convince me to let you out
of the box.'' And I said, ``Do you
believe that a transhuman AI couldn't persuade you to
let it out?'' The one gave it some serious thought,
and said ``I can't imagine anything
even a transhuman AI could say to get me to let it
out.'' ``Okay,'' I
said, ``\textit{now} we have a
bet.'' A \$20 bet, to be exact.}

{
 I won that one too.}

{
 There were some \textit{lovely} quotes on the AI-Box Experiment
from the Something Awful forums (not that I'm a member,
but someone forwarded it to me):}

{
 ``Wait, what the FUCK? How the hell could you
possibly be convinced to say yes to this? There's not
an AI at the other end AND there's \$10 on the line.
Hell, I could type `No' every few
minutes into an IRC client for 2 hours while I was reading other
webpages!''}

{
 ``This Eliezer fellow is the scariest person the
internet has ever introduced me to. What could possibly have been at
the tail end of that conversation? I simply can't
imagine anyone being that convincing without being able to provide any
tangible incentive to the human.''}

{
 ``It seems we are talking some serious psychology
here. Like Asimov's Second Foundation level stuff
\ldots''}

{
 ``I don't really see why anyone
would take anything the AI player says seriously when
there's \$10 to be had. The whole thing baffles me, and
makes me think that either the tests are faked, or this Yudkowsky
fellow is some kind of evil genius with creepy mind-control
powers.''}

{
 It's little moments like these that keep me going.
But anyway \ldots}

{
 Here are these folks who look at the AI-Box Experiment, and find
that it seems impossible unto them---\textit{even having been told that
it actually happened}. They are tempted to deny the data.}

{
 Now, if you're one of those people to whom the
AI-Box Experiment \textit{doesn't} seem all that
impossible---to whom it just seems like an interesting challenge---then
bear with me, here. Just try to put yourself in the frame of mind of
those who wrote the above quotes. Imagine that you're
taking on something that seems as ridiculous as the AI-Box Experiment
seemed to \textit{them}. I want to talk about how to do impossible
things, and obviously I'm not going to pick an example
that's \textit{really} impossible.}

{
 And if the AI Box \textit{does} seem impossible to you, I want you
to compare it to other impossible problems, like, say, a reductionist
decomposition of consciousness, and realize that the AI Box is around
\textit{as easy as a problem can get} while still being
\textit{impossible.}}

{
 So the AI-Box challenge seems impossible to you---either it really
does, or you're pretending it does. What do you do with
this impossible challenge?}

{
 First, we assume that you don't actually say
``That's
impossible!'' and give up a la Luke Skywalker. You
haven't run away.}

{
 Why not? Maybe you've learned to override the
reflex of running away. Or maybe they're going to shoot
your daughter if you fail. We suppose that you want to \textit{win},
not \textit{try}{}---that something is at stake that matters to you,
even if it's just your own pride. (Pride is an
underrated sin.)}

{
 Will you call upon the virtue of \textit{tsuyoku naritai}? But
even if you become stronger day by day, growing instead of fading, you
may not be \textit{strong enough} to do the impossible. You could go
into the AI Box experiment once, and then do it again, and try to do
better the second time. Will that get you to the point of winning? Not
for a long time, maybe; and sometimes a single failure
isn't acceptable.}

{
 (Though even to say this much---to visualize yourself doing
\textit{better} on a second try---is to begin to bind yourself to the
problem, to do more than just stand in awe of it. How, specifically,
could you do \textit{better} on one AI-Box Experiment than the
previous?---and not by luck, but by skill?)}

{
 Will you call upon the virtue \textit{isshokenmei}? But a
desperate effort may not be enough to win. Especially if that
desperation is only putting more effort into the avenues you already
know, the modes of trying you can already imagine. A problem looks
impossible when your brain's query returns no lines of
solution leading to it. What good is a desperate effort along any of
those lines?}

{
 Make an \textit{extraordinary} effort? Leave your comfort
zone---try non-default ways of doing things---even, try to think
creatively? But you can imagine the one coming back and saying,
``I tried to leave my comfort zone, and I think I
succeeded at that! I brainstormed for five minutes---and came up with
all sorts of wacky creative ideas! But I don't think
any of them are good enough. The other guy can just keep saying
`No,' no matter what I
do.''}

{
 And now we finally reply: ``\textit{Shut up and
do the impossible!}''}

{
 As we recall from Trying to Try, setting out to make an
\textit{effort} is distinct from setting out to \textit{win}.
That's the problem with saying, ``Make
an extraordinary effort.'' You can succeed at the
goal of ``making an extraordinary
effort'' without succeeding at the goal of getting
out of the Box.}

{
 ``But!'' says the one.
``But, SUCCEED is not a primitive action! Not all
challenges are fair---sometimes you just can't win! How
am I supposed to choose to be out of the Box? The other guy can just
keep on saying
`No'!''}

{
 True. Now shut up and do the impossible.}

{
 Your goal is not to do better, to try desperately, or even to try
extraordinarily. Your goal is to get out of the box.}

{
 To accept this demand creates an awful tension in your mind,
between the impossibility and the requirement to do it anyway. People
will try to flee that awful tension.}

{
 A couple of people have reacted to the AI-Box Experiment by
saying, ``Well, Eliezer, playing the AI, probably just
threatened to destroy the world whenever he was out, if he
wasn't let out immediately,'' or
``Maybe the AI offered the Gatekeeper a trillion
dollars to let it out.'' But as any sensible person
should realize on considering this strategy, the Gatekeeper is likely
to just go on saying ``No.''}

{
 So the people who say, ``Well, of course Eliezer
must have just done XXX,'' and then offer up
something that fairly obviously wouldn't work---would
they be able to escape the Box? They're trying
\textit{too hard} to convince themselves the problem
isn't impossible.}

{
 One way to run from the awful tension is to seize on a solution,
any solution, even if it's not very good.}

{
 Which is why it's important to go forth with the
true intent-to-solve---to \textit{have produced} a solution, a
\textit{good} solution, at the end of the search, and then to implement
that solution and win.}

{
 I don't quite want to say that
``you should expect to solve the
problem.'' If you hacked your mind so that you
assigned high probability to solving the problem, that
wouldn't accomplish anything. You would just lose at
the end, perhaps after putting forth not much of an effort---or putting
forth a merely desperate effort, secure in the faith that the universe
is fair enough to grant you a victory in exchange.}

{
 To have \textit{faith} that you could solve the problem would just
be another way of running from that awful tension.}

{
 And yet---you can't be setting out to \textit{try}
to solve the problem. You can't be setting out to
\textit{make an effort.} You have to be setting out to win. You
can't be saying to yourself, ``And now
I'm going to do my best.'' You have
to be saying to yourself, ``And now
I'm going to figure out how to get out of the
Box''---or reduce consciousness to nonmysterious
parts, or whatever.}

{
 I say again: You must really intend to solve the problem. If in
your heart you believe the problem really \textit{is} impossible---or
if you believe that \textit{you} will fail---then you
won't hold yourself to a high enough standard.
You'll only be trying for the sake of trying.
You'll sit down---conduct a mental search---try to be
creative and brainstorm a little---look over all the solutions you
generated---conclude that none of them work---and say,
``Oh well.''}

{
 No! \textit{Not} well! You haven't won yet! Shut
up and do the impossible!}

{
 When AI folk say to me, ``Friendly AI is
impossible,'' I'm pretty sure they
haven't even tried for the sake of trying. But if they
\textit{did} know the technique of ``Try for five
minutes before giving up,'' and they dutifully agreed
to try for five minutes by the clock, then they still
wouldn't come up with anything. They would not go forth
with true intent to solve the problem, only intent to \textit{have
tried} to solve it, to make themselves defensible.}

{
 So am I saying that you should doublethink to make yourself
believe that you will solve the problem with probability 1? Or even
doublethink to add one iota of credibility to your true estimate?}

{
 Of course not. In fact, it is necessary to keep in full view the
reasons why you \textit{can't} succeed. If you lose
sight of \textit{why} the problem is impossible, you'll
just seize on a false solution. The \textit{last} fact you want to
forget is that the Gatekeeper could always just tell the AI
``No''---or that consciousness seems
intrinsically different from any possible combination of atoms, etc.}

{
 (One of the key Rules For Doing The Impossible is that, if you can
state \textit{exactly} why something is impossible, you are often close
to a solution.)}

{
 So you've got to hold both views in your mind at
once---seeing the full impossibility of the problem, and intending to
solve it.}

{
 The awful tension between the two simultaneous views comes from
not knowing which will prevail. Not expecting to surely lose, nor
expecting to surely win. Not setting out just to try, just to have an
uncertain chance of succeeding---because then you would have a surety
of having tried. The certainty of uncertainty can be a relief, and you
have to reject that relief too, because it marks the end of
desperation. It's an in-between place,
``unknown to death, nor known to
life.''}

{
 In fiction it's easy to show someone trying
harder, or trying desperately, or even trying the extraordinary, but
it's very hard to show someone who shuts up and
attempts the impossible. It's difficult to depict Bambi
choosing to take on Godzilla, in such fashion that your readers
seriously don't know who's going to
win---expecting neither an
``astounding'' heroic victory just
like the last fifty times, nor the default squish.}

{
 You might even be justified in refusing to use probabilities at
this point. In all honesty, I really \textit{don't}
know how to estimate the probability of solving an impossible problem
that I have gone forth with intent to solve---in a case where
I've previously solved some impossible problems, but
the particular impossible problem is more difficult than anything
I've yet solved, but I plan to work on it longer, et
cetera.}

{
 People ask me how likely it is that humankind will survive, or how
likely it is that anyone can build a Friendly AI, or how likely it is
that I can build one. I really \textit{don't} know how
to answer. I'm not being evasive; I
don't know how to put a probability estimate on my, or
someone else's, successfully shutting up and doing the
impossible. Is it probability zero because it's
impossible? Obviously not. But how likely is it that this problem, like
previous ones, will give up its unyielding blankness when I understand
it better? It's not truly impossible; I can see that
much. But humanly impossible? Impossible to me in particular? I
don't know how to guess. I can't even
translate my intuitive feeling into a number, because the only
intuitive feeling I have is that the
``chance'' depends heavily on my
choices and unknown unknowns: a wildly unstable probability estimate.}

{
 But I do hope by now that I've made it clear why
you shouldn't panic, when I now say clearly and
forthrightly that building a Friendly AI is impossible.}

{
 I hope this helps explain some of my attitude when people come to
me with various bright suggestions for building communities of AIs to
make the whole Friendly without any of the individuals being
trustworthy, or proposals for keeping an AI in a box, or proposals for
``Just make an AI that does X,'' et
cetera. Describing the specific flaws would be a whole long story in
each case. But the general rule is that you can't do it
\textit{because Friendly AI is impossible.} So you should be very
suspicious indeed of someone who proposes a solution that seems to
involve only an \textit{ordinary} effort---without even taking on the
trouble of doing anything impossible. Though it does take a mature
understanding to appreciate this impossibility, so it's
not surprising that people go around proposing clever shortcuts.}

{
 On the AI-Box Experiment, so far I've only been
convinced to divulge a single piece of information on how I did
it---when someone noticed that I was reading Y
Combinator's Hacker News, and posted a topic called
``Ask Eliezer Yudkowsky'' that got
voted to the front page. To which I replied:}

{
 Oh, dear. Now I feel obliged to say \textit{something}, but all
the original reasons against discussing the AI-Box experiment are still
in force \ldots}

{
 All right, this much of a hint:}

{
 There's no super-clever special trick to it. I
just did it the hard way.}

{
 Something of an entrepreneurial lesson there, I guess.}

{
 There was no super-clever special trick that let me get out of the
Box using only a \textit{cheap} effort. I didn't bribe
the other player, or otherwise violate the spirit of the experiment. I
just did it the hard way.}

{
 Admittedly, the AI-Box Experiment never did seem like an
\textit{impossible} problem to me to begin with. When someone
can't think of any possible argument that would
convince them of something, that just means their brain is running a
search that hasn't yet turned up a path. It
doesn't mean they can't be convinced.}

{
 But it illustrates the general point: ``Shut up
and do the impossible'' isn't the
same as expecting to find a cheap way out. That's only
another kind of running away, of reaching for relief.}

{
 \textit{Tsuyoku naritai} is more stressful than being content with
who you are. \textit{Isshokenmei} calls on your willpower for a
convulsive output of conventional strength. ``Make an
extraordinary effort'' demands that you
\textit{think}; it puts you in situations where you may not know what
to do next, unsure of whether you're doing the right
thing. But ``Shut up and do the
impossible'' represents an even higher octave of the
same thing, and its cost to its employer is correspondingly greater.}

{
 Before you the terrible blank wall stretches up and up and up,
unimaginably far out of reach. And there is also the need to solve it,
\textit{really} solve it, not ``try your
best.'' Both awarenesses in the mind at once,
simultaneously, and the tension between. All the reasons you
can't win. All the reasons you have to. Your intent to
solve the problem. Your extrapolation that every technique you know
will fail. So you tune yourself to the highest pitch you can reach.
Reject all cheap ways out. And then, like walking through concrete,
start to move forward.}

{
 I try not to dwell too much on the drama of such things. By all
means, if you can diminish the cost of that tension to yourself, you
should do so. There is nothing heroic about making an effort that is
the slightest bit more heroic than it has to be. If there really is a
cheap shortcut, I suppose you could take it. But I have yet to find a
\textit{cheap} way out of any impossibility I have undertaken.}

{
 There were three more AI-Box experiments besides the ones
described on the linked page, which I never got around to adding in.
People started offering me thousands of dollars as
stakes---``I'll pay you \$5,000 if you
can convince me to let you out of the box.'' They
didn't seem sincerely convinced that not even a
transhuman AI could make them let it out---they were just curious---but
I was tempted by the money. So, after investigating to make sure they
could afford to lose it, I played another three AI-Box experiments. I
won the first, and then lost the next two. And then I called a halt to
it. I didn't like the person I turned into when I
started to lose.}

{
 I put forth a desperate effort, and lost anyway. It hurt---both
the losing, and the desperation. It wrecked me for that day and the day
afterward.}

{
 I'm a sore loser. I don't know if
I'd call that a
``strength,'' but
it's one of the things that drives me to keep at
impossible problems.}

{
 But you can lose. It's allowed to happen. Never
forget that, or why are you bothering to try so hard? Losing hurts, if
it's a loss you can survive. And you've
wasted time, and perhaps other resources.}

{
 ``Shut up and do the
impossible'' should be reserved for \textit{very}
special occasions. You can lose, and it will hurt. You have been
warned.}

{
 \ldots but it's only at this level that adult
problems begin to come into sight.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Final Words}

{
 Sunlight enriched air already alive with curiosity, as dawn rose
on Brennan and his fellow students in the place to which Jeffreyssai
had summoned them. }

{
 They sat there and waited, the five, at the top of the great
glassy crag that was sometimes called Mount Mirror, sometimes Mount
Monastery, and more often simply left unnamed. The high top and peak of
the mountain, from which you could see all the lands below and seas
beyond.}

{
 (Well, not \textit{all} the lands below, nor seas beyond. So far
as anyone knew, there was no place in the world from which all the
world was visible; nor, equivalently, any kind of vision that would see
through all obstacle-horizons. In the end it was the top only of
\textit{one particular} mountain: there were other peaks, and from
their tops you would see other lands below; even though, in the end, it
was all a single world.)}

{
 ``What do you think comes
next?'' said Hiriwa. Her eyes were bright, and she
gazed to the far horizons like a lord.}

{
 Taji shrugged, though his own eyes were alive with anticipation.
``Jeffreyssai's last lesson
doesn't have any obvious sequel that I can think of. In
fact, I think we've learned just about everything that
I \textit{knew} the \textit{beisutsukai} masters knew.
What's left, then---''}

{
 ``Are the \textit{real}
secrets,'' Yin completed the thought.}

{
 Hiriwa and Taji and Yin shared a grin, among themselves.}

{
 Styrlyn wasn't smiling. Brennan suspected rather
strongly that Styrlyn was older than he had admitted.}

{
 Brennan wasn't smiling either. He might be young,
but he kept high company, and had witnesssed some of what went on
behind the curtains of the world. Secrets had their price, always; that
was the barrier that made them secrets. And Brennan thought he had a
good idea of what this price might be.}

{
 There was a cough from \textit{behind} them, at a moment when they
had all happened to be looking in any other direction but that one.}

{
 As one, their heads turned.}

{
 Jeffreyssai stood there, in a casual robe that looked more like
very glassy glass than any proper sort of mirrorweave.}

{
 Jeffreyssai stood there and looked at them, a strange abiding
sorrow in those inscrutable ancient eyes.}

{
 ``Sen \ldots sei,'' Taji
started, faltering as that bright anticipation stumbled over
Jeffreyssai's return look.
``What's next?''}

{
 ``Nothing,'' Jeffreyssai said
abruptly. ``You're finished.
It's done.''}

{
 Hiriwa, Taji, and Yin all blinked, a perfect synchronized gesture
of shock. Then, before their expressions could turn to outrage and
objections---}

{
 ``\textit{Don't},''
Jeffreyssai said. There was real pain in it. ``Believe
me, it hurts me more than it hurts you.'' He might
have been looking at them; or at something far away, or long ago.
``I don't know exactly what roads may
lie before you---but yes, I \textit{know} you're not
ready. I \textit{know} I'm sending you out unprepared.
I \textit{know} that everything I taught you is incomplete. That what I
said is not what you heard. I know that I left out the one most
important thing. That the rhythm at the center of everything is missing
and astray. I know that you will harm yourself in the course of trying
to use what I taught; so that \textit{I}, personally, will have shaped,
in some fashion unknown to me, the very knife that will cut you \ldots}

{
 ``\ldots that's the hell of being
a teacher, you see,'' Jeffreyssai said. Something
grim flickered in his expression. ``Nonetheless,
you're \textit{done}. Finished, for now. What lies
between you and mastery is not another classroom. We are fortunate, or
perhaps not \textit{fortunate}, that the road to power does not wend
only through lecture halls. Or the quest would be boring to the bitter
end. Still, I \textit{cannot} teach you; and so it is a moot point
whether I \textit{would}. There is no master here whose art is all
inherited. Even the \textit{beisutsukai} have never discovered how to
teach certain things; it is possible that such an event has been
prohibited. And so you can only arrive at mastery by using to the
fullest the techniques you have already learned, facing challenges and
apprehending them, mastering the tools you have been taught
\textit{until they shatter in your hands}{}---''}

{
 Jeffreyssai's eyes were hard, as though steeled in
acceptance of unwelcome news.}

{
 ``---and you are left in the midst of wreckage
absolute. \textit{That} is where I, your teacher, am sending you. You
are not \textit{beisutsukai} masters. I cannot create masters. I cannot
even come close. Go, then, and fail.''}

{
 ``But---'' said Yin, and
stopped herself.}

{
 ``Speak,'' said Jeffreyssai.}

{
 ``But then why,'' she said
helplessly, ``why teach us anything in the first
place?''}

{
 Brennan's eyelids flickered just the tiniest
amount.}

{
 It was enough for Jeffreyssai. ``Answer her,
Brennan, if you think you know.''}

{
 ``Because,'' Brennan said,
``if we were not taught, there would be no chance
\textit{at all} of our becoming masters.''}

{
 ``Even so,'' said Jeffreyssai.
``If you were \textit{not} taught---then when you
failed, you might simply think you had reached the limits of Reason
itself. You would be discouraged and bitter amid the wreckage. You
might not even realize when you had failed. No; you have been shaped
into something that \textit{may} emerge from the wreckage of your past
self, determined to \textit{remake} your art. And then you will
remember much that will help you. If you had not been taught, your
chances would be---less.'' His gaze passed over the
group. ``It should be obvious, but understand that the
moment of your crisis cannot be provoked artificially. To
\textit{teach} you something, the catastrophe must come as a
\textit{surprise.}''}

{
 Brennan made the gesture with his hand that indicated a question;
and Jeffreyssai nodded in reply.}

{
 ``Is this the \textit{only} way in which Bayesian
masters come to be, sensei?''}

{
 ``\textit{I} do not know,''
said Jeffreyssai, from which the overall state of the evidence was
obvious enough. ``But I doubt there would ever be a
road that leads only through the monastery. We are the heirs in this
world of mystics as well as scientists, just as the Competitive
Conspiracy inherits from chess players alongside cagefighters. We have
turned our impulses to more constructive uses---but we must still stay
on our guard against old failure modes.''}

{
 Jeffreyssai took a breath. ``Three flaws above
all are common among the \textit{beisutsukai.} The first flaw is to
look just the slightest bit harder for flaws in arguments whose
conclusions you would rather not accept. If you cannot contain this
aspect of yourself then every flaw you know how to detect will make you
that much stupider. This is the challenge that determines whether you
possess the art or its opposite: intelligence, to be useful, must be
used for something other than defeating itself.}

{
 ``The second flaw is cleverness. To invent great
complicated plans and great complicated theories and great complicated
arguments---or even, perhaps, plans and theories and arguments which
are commended too much by their elegance and too little by their
realism. There is a widespread saying which runs: `The
vulnerability of the \textit{beisutsukai} is well-known; they are prone
to be too clever.' Your enemies \textit{will} know this
saying, if they know you for a \textit{beisutsukai}, so \textit{you}
had best remember it also. And you may think to yourself:
`But if I could \textit{never} try anything clever or
elegant, would my life even be worth living?' This is
why cleverness is still our chief vulnerability even after its being
well-known, like offering a Competitor a challenge that seems fair, or
tempting a Bard with drama.}

{
 ``The third flaw is underconfidence, modesty,
humility. You have learned so much of flaws, some of them impossible to
fix, that you may think that the rule of wisdom is to confess your own
inability. You may question yourself so much, without resolution or
testing, that you lose your will to carry on in the Art. You may refuse
to decide, pending further evidence, when a decision is
\textit{necessary}; you may take advice you should not take. Jaded
cynicism and sage despair are less fashionable than once they were, but
you may still be tempted by them. Or you may simply---lose
momentum.''}

{
 Jeffreyssai fell silent then.}

{
 He looked from each of them, one to the other, with quiet
intensity.}

{
 And said at last, ``Those are my final words to
you. If and when we meet next, you and I---if and when you return to
this place, Brennan, or Hiriwa, or Taji, or Yin, or Styrlyn---I will no
longer be your teacher.''}

{
 And Jeffreyssai turned and walked swiftly away, heading back
toward the glassy tunnel that had emitted him.}

{
 Even Brennan was shocked. For a moment they were all speechless.}

{
 Then---}

{
 ``Wait!'' cried Hiriwa.
``What about \textit{our} final words to you? I never
said---''}

{
 ``I will tell you what my \textit{sensei} told
me,'' Jeffreyssai's voice came back
as he disappeared. ``You can thank me after you
return, if you return. One of you at least seems likely to come
back.''}

{
 ``No, wait, I---'' Hiriwa fell
silent. In the mirrored tunnel, the fractured reflections of
Jeffreyssai were already fading. She shook her head.
``Never \ldots mind, then.''}

{
 There was a brief, uncomfortable silence, as the five of them
looked at each other.}

{
 ``Good heavens,'' Taji said
finally. ``Even the Bardic Conspiracy
wouldn't try for that much drama.''}

{
 Yin suddenly laughed. ``Oh, this was
\textit{nothing.} You should have seen \textit{my} send-off when I left
Diamond Sea University.'' She smiled.
``I'll tell you about it sometime---if
you're interested.''}

{
 Taji coughed. ``I suppose I should go back and
\ldots pack my things \ldots''}

{
 ``I'm already
packed,'' Brennan said. He smiled, ever so slightly,
when the other three turned to look at him.}

{
 ``Really?'' Taji asked.
``What was the clue?''}

{
 Brennan shrugged with careful carelessness.
``Beyond a certain point, it is futile to inquire how
a \textit{beisutsukai} master knows a thing---''}

{
 ``Come off it!'' Yin said.
``You're not a \textit{beisutsukai}
master \textit{yet.}''}

{
 ``Neither is Styrlyn,'' Brennan
said. ``But he has already packed as
well.'' He made it a statement rather than a
question, betting double or nothing on his image of inscrutable
foreknowledge.}

{
 Styrlyn cleared his throat. ``As you say. Other
commitments call me, and I have already tarried longer than I planned.
Though, Brennan, I do feel that you and I have certain mutual
interests, which I would be happy to discuss with
you---''}

{
 ``Styrlyn, my most excellent friend, I shall be
happy to speak with you on any topic you desire,''
Brennan said politely and noncommitally, ``if we
should meet again.'' As in, not now. He certainly
wasn't selling out his Mistress \textit{this} early in
their relationship.}

{
 There was an exchange of goodbyes, and of hints and offers.}

{
 And then Brennan was walking down the road that led toward or away
from Mount Monastery (for every road is a two-edged sword), the
smoothed glass pebbles clicking under his feet.}

{
 He strode out along the path with purpose, vigor, and
determination, just in case someone was watching.}

{
 Some time later he stopped, stepped off the path, and wandered
just far enough away to prevent anyone from finding him unless they
were deliberately following.}

{
 Then he sagged wearily back against a tree-trunk. It was a sparse
clearing, with only a few trees poking out of the ground; not much
present in the way of distracting scenery, unless you counted the
red-tinted stream flowing out of a dark cave-mouth. And Brennan
deliberately faced away from that, leaving only the far gray of the
horizons, and the blue sky and bright sun.}

{
 \textit{Now what?}}

{
 He had \textit{thought} that the Bayesian Conspiracy, of all the
possible trainings that existed in this world, would have cleared up
his uncertainty about what to do with the rest of his life.}

{
 \textit{Power}, he'd sought at first. Strength to
prevent a repetition of the past. ``If you
don't know what you need, take
power''---so went the proverb. He had gone first to
the Competitive Conspiracy, then to the \textit{beisutsukai.}}

{
 And now \ldots}

{
 Now he felt more lost than ever.}

{
 He could think of things that made him happy. But nothing that he
really \textit{wanted.}}

{
 The passionate intensity that he'd come to
associate with his Mistress, or with Jeffreyssai, or the other figures
of power that he'd met \ldots a life of pursuing small
pleasures seemed to pale in comparison, next to that.}

{
 In a city not far from the center of the world, his Mistress
waited for him (in all probability, assuming she hadn't
gotten bored with her life and run away). But to merely return, and
then drift aimlessly, waiting to fall into someone
else's web of intrigue \ldots no. That
didn't seem like \ldots \textit{enough.}}

{
 Brennan plucked a blade of grass from the ground and stared at it,
half-unconsciously looking for anything interesting about it; an old,
old game that his very first teacher had taught him, what now seemed
like ages ago.}

{
 \textit{Why did I believe that going to Mount Mirror would tell me
what I wanted?}}

{
 Well, decision theory \textit{did} require that your utility
function be consistent, but \ldots}

{
 \textit{If the beisutsukai knew what I wanted, would they even
tell me?}}

{
 At the Monastery they taught doubt. So now he was falling prey to
the third besetting sin of which Jeffreyssai had spoken: lost momentum,
indeed. For he had learned to question the image that he held of
himself in his mind.}

{
 \textit{Are you seeking power because that is your} true
\textit{desire, Brennan?}}

{
 \textit{Or because you have a picture in your mind of the role}
\textit{that you play as an ambitious young man, and you think it is
what someone playing your role would do?}}

{
 Almost everything he'd done up until now, even
going to Mount Mirror, had probably been the latter.}

{
 And when he blanked out the old thoughts and tried to see the
problem as though for the first time \ldots}

{
 \ldots nothing much came to mind.}

{
 \textit{What do I want?}}

{
 Maybe it wasn't reasonable to expect the
\textit{beisutsukai} to tell him outright. But was there anything they
had taught him by which he might answer?}

{
 Brennan closed his eyes and thought.}

{
 \textit{First, suppose there} is \textit{something I would
passionately desire. Why would I} not \textit{know what it is?}}

{
 \textit{Because I have not yet encountered it, or ever imagined
it?}}

{
 \textit{Or because there is some reason I would not admit it to
myself?}}

{
 Brennan laughed out loud, then, and opened his eyes.}

{
 So simple, once you thought of it that way. So obvious in
retrospect. \textit{That} was what they called a silver-shoes moment,
and yet, if he hadn't gone to Mount Mirror, it
wouldn't ever have occurred to him.}

{
 Of \textit{course} there was something he wanted. He knew
\textit{exactly} what he wanted. Wanted so desperately he could taste
it like a sharp tinge on his tongue.}

{
 It just hadn't come to mind earlier, because \ldots
if he acknowledged his desire explicitly \ldots then he also had to see
that it was \textit{difficult}. High, high, above him. Far out of his
reach. ``Impossible'' was the word
that came to mind, though it was not, of course, impossible.}

{
 But once he asked himself if he preferred to wander aimlessly
through his life---once it was put that way, the answer became obvious.
Pursuing the unattainable would make for a hard life, but not a sad
one. He could think of things that made him happy, either way. And in
the end---it \textit{was} what he wanted.}

{
 Brennan stood up, and took his first steps, in the exact direction
of Shir L'or, the city that lies in the center of the
world. He had a plot to hatch, and he did not know who would be part of
it.}

{
 And then Brennan stumbled, when he realized that Jeffreyssai had
already known.}

{
 \textit{One of you at least seems likely to come back \ldots}}

{
 Brennan had thought he was talking about Taji. Taji had probably
thought he was talking about Taji. It was what Taji said he wanted. But
how reliable of an indicator was that, really?}

{
 There was a proverb, though, about that very road he had just
left: \textit{Whoever sets out from Mount Mirror seeking the
impossible, will surely return.}}

{
 When you considered Jeffreyssai's last
warning---and that the proverb said nothing of \textit{succeeding} at
the impossible task itself---it was a less optimistic saying than it
sounded.}

{
 Brennan shook his head wonderingly. How could Jeffreyssai possibly
have known before Brennan knew himself?}

{
 Well, beyond a certain point, it is futile to inquire how a
\textit{beisutsukai} master knows a thing---}

{
 Brennan halted in mid-thought.}

{
 No.}

{
 No, if he was going to become a \textit{beisutsukai} master
himself someday, then he ought to figure it out.}

{
 It was, Brennan realized, a \textit{stupid} proverb.}

{
 So he walked, and this time, he thought about it carefully.}

{
 As the sun was setting, red-golden, shading his footsteps in
light.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\chapter{The Craft and the Community}

\mysection{Raising the Sanity Waterline}

{
 To paraphrase the Black Belt Bayesian: Behind every exciting,
dramatic failure, there is a more important story about a larger and
less dramatic failure that made the first failure possible. }

{
 If every trace of religion were magically eliminated from the
world tomorrow, then---however much improved the lives of many people
would be---we would not even have come close to solving the larger
failures of sanity that made religion possible in the first place.}

{
 We have good cause to spend some of our efforts on trying to
eliminate religion directly, because it \textit{is} a direct problem.
But religion also serves the function of an asphyxiated canary in a
coal mine---religion is a sign, a symptom, of larger problems that
don't go away just because someone loses their
religion.}

{
 Consider this thought experiment---what could you teach people
that is not \textit{directly} about religion, that is true and useful
as a \textit{general} method of rationality, which would cause them to
lose their religions? In fact---imagine that we're
going to go and survey all your students five years later, and see how
many of them have lost their religions compared to a control group; if
you make the slightest move at fighting religion \textit{directly}, you
will invalidate the experiment. You may not make a single mention of
religion or any religious belief in your classroom; you may not even
hint at it in any obvious way. All your examples must center about
real-world cases that have nothing to do with religion.}

{
 If you can't fight religion \textit{directly},
what do you teach that raises the \textit{general waterline of sanity}
to the point that religion goes underwater?}

{
 Here are some such topics I've already
covered---\textit{not} avoiding all mention of religion, but it could
be done:}

{
 Affective Death Spirals---plenty of non-supernaturalist examples.}

{
 How to avoid cached thoughts and fake wisdom; the pressure of
conformity.}

{
 Evidence and Occam's Razor---the rules of
probability.}

{
 The Bottom Line / Engines of Cognition---the causal reasons why
Reason works.}

{
 Mysterious Answers to Mysterious Questions---and the whole
associated sequence, like making beliefs pay rent and curiosity
stoppers---have excellent historical examples in vitalism and
phlogiston.}

{
 Non-existence of ontologically fundamental mental things---apply
the Mind Projection Fallacy to probability, move on to reductionism
versus holism, then brains and cognitive science.}

{
 The many sub-arts of Crisis of Faith---though
you'd better find something else to call this ultimate
high master-level technique of \textit{actually updating on evidence}.}

{
 Dark Side Epistemology---teaching this with no mention of religion
would be hard, but perhaps you could videotape the interrogation of
some snake-oil sales agent as your real-world example.}

{
 Fun Theory{}---teach as a literary theory of utopian fiction,
without the direct application to theodicy.}

{
 Joy in the Merely Real, naturalistic metaethics, et cetera, et
cetera, et cetera, and so on.}

{
 But to look at it another way---}

{
 Suppose we have a scientist who's still religious,
either full-blown scriptural-religion, or in the sense of tossing
around vague casual endorsements of
``spirituality.''}

{
 We now know this person is not applying any \textit{technical,
explicit} understanding of \ldots}

{
 \ldots what constitutes evidence and why;}

{
 \ldots Occam's Razor;}

{
 \ldots how the above two rules derive from the lawful and causal
operation of minds as mapping engines, and do not switch off when you
talk about tooth fairies;}

{
 \ldots how to tell the difference between a real answer and a
curiosity-stopper;}

{
 \ldots how to rethink matters for themselves instead of just
repeating things they heard;}

{
 \ldots certain general trends of science over the last three
thousand years;}

{
 \ldots the difficult arts of actually updating on new evidence and
relinquishing old beliefs;}

{
 \ldots epistemology 101;}

{
 \ldots self-honesty 201;}

{
 \ldots et cetera, et cetera, et cetera, and so on.}

{
 When you consider it---these are all rather \textit{basic} matters
of study, as such things go. A quick introduction to \textit{all} of
them (well, except naturalistic metaethics) would be \ldots a
four-credit undergraduate course with no prerequisites?}

{
 But there are Nobel laureates who haven't taken
that course! Richard Smalley if you're looking for a
cheap shot, or Robert Aumann if you're looking for a
scary shot.}

{
 And they can't be isolated exceptions. If all of
their professional compatriots had taken that course, then Smalley or
Aumann would either have been corrected (as their colleagues kindly
took them aside and explained the bare fundamentals) or else regarded
with too much pity and concern to win a Nobel Prize. Could
you---\textit{realistically} speaking, regardless of fairness---win a
Nobel while advocating the existence of Santa Claus?}

{
 That's what the dead canary, religion, is telling
us: that the general sanity waterline is currently \textit{really
ridiculously low.} Even in the highest halls of science.}

{
 If we throw out that dead and rotting canary, then our mine may
stink a bit less, but the sanity waterline may not rise much higher.}

{
 This is not to criticize the neo-atheist movement. The harm done
by religion is clear and present danger, or rather, current and ongoing
disaster. Fighting religion's directly harmful effects
takes precedence over its use as a canary or experimental indicator.
But even if Dawkins, and Dennett, and Harris, and Hitchens, should
somehow win utterly and absolutely to the last corner of the human
sphere, the real work of rationalists will be only just beginning.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{A Sense That More Is Possible}

{
 To teach people about a topic you've labeled
``rationality,'' it helps for them
to be interested in ``rationality.''
(There are less direct ways to teach people how to attain the map that
reflects the territory, or optimize reality according to their values;
but the explicit method \textit{is} the course I tend to take.) }

{
 And when people explain why they're \textit{not}
interested in rationality, one of the most commonly proffered reasons
tends to be like: ``Oh, I've known a
couple of rational people and they didn't seem any
happier.''}

{
 Who are they thinking of? Probably an Objectivist or some such.
Maybe someone they know who's an ordinary scientist. Or
an ordinary atheist.}

{
 That's really \textit{not} a whole lot of
rationality, as I have previously said.}

{
 Even if you limit yourself to people who can derive
Bayes's Theorem---which is going to eliminate, what,
98\% of the above personnel?---that's \textit{still}
not a whole lot of rationality. I mean, it's a pretty
basic theorem.}

{
 Since the beginning I've had a sense that there
ought to be some discipline of cognition, some art of thinking, the
studying of which would make its students visibly more competent, more
formidable: the equivalent of Taking a Level in Awesome.}

{
 But when I look around me in the real world, I
don't see that. Sometimes I see a hint, an echo, of
what I think should be possible, when I read the writings of folks like
Robyn Dawes, Daniel Gilbert, John Tooby, and Leda Cosmides. A few very
rare and very senior researchers in psychological sciences, who visibly
care a \textit{lot} about rationality---to the point, I suspect, of
making their colleagues feel uncomfortable, because
it's not cool to care that much. I can see that
they've found a rhythm, a unity that begins to pervade
their arguments---}

{
 Yet even that \ldots isn't really a whole lot of
rationality either.}

{
 Even among those few who impress me with a hint of dawning
formidability---I don't think that their mastery of
rationality could compare to, say, John Conway's
mastery of math. The base knowledge that we drew upon to build our
understanding---if you extracted only the parts we used, and not
everything we had to study to find it---it's probably
not comparable to what a professional nuclear engineer knows about
nuclear engineering. It may not even be comparable to what a
construction engineer knows about bridges. We practice our skills, we
do, in the ad-hoc ways we taught ourselves; but that practice probably
doesn't compare to the training regimen an Olympic
runner goes through, or maybe even an ordinary professional tennis
player.}

{
 And the root of \textit{this} problem, I do suspect, is that we
haven't really gotten together and systematized our
skills. We've had to create all of this for ourselves,
ad-hoc, and there's a limit to how much one mind can
do, even if it can manage to draw upon work done in outside fields.}

{
 The chief obstacle to doing this the way it \textit{really} should
be done is the difficulty of testing the \textit{results} of
rationality training programs, so you can have evidence-based training
methods. I will write more about this, because I think that recognizing
successful training and distinguishing it from failure is the
essential, blocking obstacle.}

{
 There are experiments done now and again on debiasing
interventions for particular biases, but it tends to be something like,
``Make the students practice this for an hour, then
test them two weeks later.'' Not,
``Run half the signups through version A of the
three-month summer training program, and half through version B, and
survey them five years later.'' You can see, here,
the implied amount of effort that I think would go into a training
program for people who were Really Serious about rationality, as
opposed to the attitude of taking Casual Potshots That Require Like An
Hour Of Effort Or Something.}

{
 Daniel Burfoot brilliantly suggests that this is why intelligence
seems to be such a big factor in rationality---that when
you're improvising everything ad-hoc with very little
training or systematic practice, intelligence ends up being the most
important factor in what's left.}

{
 Why aren't
``rationalists'' surrounded by a
visible aura of formidability? Why aren't they found at
the top level of every elite selected on any basis that has anything to
do with thought? Why do most
``rationalists'' just seem like
ordinary people, perhaps of moderately above-average intelligence, with
one more hobbyhorse to ride?}

{
 Of this there are several answers; but one of them, surely, is
that they have received less systematic training of rationality in a
less systematic context than a first-dan black belt gets in hitting
people.}

{
 I do not except myself from this criticism. I am no
\textit{beisutsukai}, because there are limits to how much Art you can
create on your own, and how well you can guess without evidence-based
statistics on the results. I know about a \textit{single} use of
rationality, which might be termed ``reduction of
confusing cognitions.'' This I asked of my brain;
this it has given me. There are other arts, I think, that a mature
rationality training program would not neglect to teach, which would
make me stronger and happier and more effective---if I could just go
through a standardized training program using the cream of teaching
methods experimentally demonstrated to be effective. But the kind of
tremendous, focused effort that I put into creating my single
\textit{sub-art} of rationality from scratch---my life
doesn't have room for more than one of those.}

{
 I consider myself something more than a first-dan black belt, and
less. I can \textit{punch} through brick and I'm
working on steel along my way to adamantine, but I have a mere casual
street-fighter's grasp of how to kick or throw or
block.}

{
 Why are there schools of martial arts, but not rationality dojos?
(This was the first question I asked in my first blog post.) Is it more
important to hit people than to think?}

{
 No, but it's easier to verify when you
\textit{have} hit someone. That's part of it, a highly
central part.}

{
 But maybe even more importantly---there are people out there who
\textit{want} to hit, and who have the idea that there ought to be a
systematic art of hitting that makes you into a visibly more formidable
fighter, with a speed and grace and strength beyond the struggles of
the unpracticed. So they go to a school that promises to teach that.
And that school exists because, long ago, some people had the sense
that more was possible. And they got together and shared their
techniques and practiced and formalized and practiced and developed the
Systematic Art of Hitting. They pushed themselves that far because
\textit{they thought they should be awesome} and they were willing to
put some \textit{back} into it.}

{
 Now---they \textit{got} somewhere with that aspiration, unlike a
thousand other aspirations of awesomeness that failed, because they
could \textit{tell} when they had hit someone; and the schools competed
against each other regularly in realistic contests with clearly-defined
winners.}

{
 But before even that---there was first the aspiration, the wish to
become stronger, a sense that more was possible. A vision of a speed
and grace and strength that they did not already possess, but
\textit{could} possess, \textit{if} they were willing to put in a lot
of work, that drove them to systematize and train and test.}

{
 Why don't we have an Art of Rationality?}

{
 Third, because current
``rationalists'' have trouble
working in groups: of this I shall speak more.}

{
 Second, because it is hard to verify success in training, or which
of two schools is the stronger.}

{
 But first, because people lack the sense that rationality is
something that \textit{should} be systematized and trained and tested
like a martial art, that should have as much knowledge behind it as
nuclear engineering, whose superstars should practice as hard as chess
grandmasters, whose successful practitioners should be surrounded by an
evident aura of awesome.}

{
 And conversely they don't look at the
\textit{lack} of visibly greater formidability, and say,
``We must be doing something
wrong.''}

{
 ``Rationality'' just seems like
one more hobby or hobbyhorse, that people talk about at parties; an
adopted mode of conversational attire with few or no real consequences;
and it doesn't seem like there's
anything wrong about that, either.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Epistemic Viciousness}

{
 Someone deserves a large hat tip for this, but I'm
having trouble remembering who; my records don't seem
to show any email or \textit{Overcoming Bias} comment which told me of
this 12-page essay, ``Epistemic Viciousness in the
Martial Arts'' by Gillian Russell.\textsuperscript{1}
Maybe Anna Salamon?}

{
 We all lined up in our ties and sensible shoes (this was England)
and copied him---left, right, left, right---and afterwards he told us
that if we practised in the air with sufficient devotion for three
years, then we would be able to use our punches to kill a bull with one
blow.}

{
 I worshipped Mr Howard (though I would sooner have died than told
him that) and so, as a skinny, eleven-year-old girl, I came to believe
that if I practised, I would be able to kill a bull with one blow by
the time I was fourteen.}

{
 This essay is about epistemic viciousness in the martial arts, and
this story illustrates just that. Though the word
``viciousness'' normally suggests
deliberate cruelty and violence, I will be using it here with the more
old-fashioned meaning, possessing of vices.}

{
 It all generalizes \textit{amazingly}. To summarize some of the
key observations for how epistemic viciousness arises:}

{
 The art, the dojo, and the sensei are seen as sacred.
``Having red toe-nails in the dojo is like going to
church in a mini-skirt and halter-top \ldots The students of other
martial arts are talked about like they are practicing the wrong
religion.''}

{
 If your teacher takes you aside and teaches you a special move and
you practice it for twenty years, you have a large emotional investment
in it, and you'll want to discard any incoming evidence
against the move.}

{
 Incoming students don't have much choice: a
martial art can't be learned from a book, so they have
to trust the teacher.}

{
 Deference to famous historical masters. ``Runners
think that the contemporary staff of Runner's World
know more about running than all the ancient Greeks put together. And
it's not just running, or other physical activities,
where history is kept in its place; the same is true in any
well-developed area of study. It is not considered disrespectful for a
physicist to say that Isaac Newton's theories are false
\ldots'' (Sound familiar?)}

{
 ``We martial artists struggle with a kind of
poverty---data-poverty---which makes our beliefs hard to test \ldots
Unless you're unfortunate enough to be fighting a
hand-to-hand war you cannot \textit{check} to see how much force and
exactly which angle a neck-break requires \ldots''}

{
 ``If you can't test the
effectiveness of a technique, then it is hard to test methods for
improving the technique. Should you practice your nukite in the air, or
will that just encourage you to overextend? \ldots Our inability to test
our fighting methods restricts our ability to test our training
methods.''}

{
 ``But the real problem isn't just
that we live in data poverty---I think that's true for
some perfectly respectable disciplines, including theoretical
physics---the problem is that we live in poverty but continue to act as
though we live in luxury, as though we can safely afford to believe
whatever we're told \ldots''
(\textbf{+10!})}

{
 One thing that I remembered being in this essay, but, on a second
reading, wasn't actually there, was the degeneration of
martial arts after the decline of real fights---by which I mean, fights
where people were really trying to hurt each other and someone
occasionally got killed.}

{
 In those days, you had some idea of who the real masters were, and
which school could defeat others.}

{
 And then things got all \textit{civilized}. And so things went
downhill to the point that we have videos on Youtube of supposed
Nth-dan black belts being pounded into the ground by someone with real
fighting experience.}

{
 I heard of one case of this that was \textit{really} sad; it was a
master of a school who was convinced he could use \textit{ki}
techniques. His students would actually fall over when he used ki
attacks, a strange and remarkable and frightening case of self-hypnosis
or \textit{something \ldots} and the master goes up against a skeptic
and of course gets pounded completely into the floor.}

{
 Truly is it said that ``how to not
lose'' is more broadly applicable information than
``how to win.'' Every single one of
these risk factors transfers straight over to any attempt to start a
``rationality dojo.'' I put to you
the question: What can be done about it?}

{\centering
 \ ~
\par}

{\centering
 *
\par}


\bigskip

{
 1. Gillian Russell, ``Epistemic Viciousness in
the Martial Arts,'' in \textit{Martial Arts and
Philosophy: Beating and Nothingness}, ed. Graham Priest and Damon A.
Young (Open Court, 2010).}

\mysection{Schools Proliferating Without Evidence}

{
 Robyn Dawes, author of one of the original papers from
\textit{Judgment Under Uncertainty} and of the book \textit{Rational
Choice in an Uncertain World}{}---one of the few who tries really hard
to import the results to real life---is also the author of
\textit{House of Cards: Psychology and Psychotherapy Built on Myth.} }

{
 From \textit{House of Cards}, chapter 1:\textsuperscript{1}}

{
 The ability of these professionals has been subjected to empirical
scrutiny---for example, their effectiveness as therapists (Chapter 2),
their insight about people (Chapter 3), and the relationship between
how well they function and the amount of experience they have had in
their field (Chapter 4). Virtually all the research---and this book
will reference more than three hundred empirical investigations and
summaries of investigations---has found that these
professionals' claims to superior intuitive insight,
understanding, and skill as therapists are simply invalid \ldots}

{
 Remember Rorschach ink-blot tests? It's such an
appealing argument: the patient looks at the ink-blot and says what
they see, the psychotherapist interprets their psychological state
based on this. There've been hundreds of experiments
looking for some evidence that it actually works. Since
you're reading this, you can guess the answer is simply
``No.'' Yet the Rorschach is still
in use. It's just such a \textit{good story} that
psychotherapists simply can't bring themselves to
believe the vast mounds of experimental evidence saying it
doesn't work---}

{
 {}---which tells you what sort of field we're
dealing with here.}

{
 And the experimental results on the field as a whole are
commensurate. Yes, patients who see psychotherapists have been known to
get better faster than patients who simply do nothing. But there is no
statistically discernible difference between the many schools of
psychotherapy. There is no discernible gain from years of expertise.}

{
 And there's also no discernible difference between
seeing a psychotherapist and spending the same amount of time talking
to a randomly selected college professor from another field.
It's just talking to \textit{anyone} that helps you get
better, apparently.}

{
 \textit{In the entire absence of the slightest experimental
evidence for their effectiveness}, psychotherapists became licensed by
states, their testimony accepted in court, their teaching schools
accredited, and their bills paid by health insurance.}

{
 And there was also a huge proliferation of
``schools,'' of traditions of
practice, in psychotherapy; despite---or perhaps \textit{because}
of---the lack of any experiments showing that one school was better
than another \ldots}

{
 I should really write more some other time on all the sad things
this says about our world. About how \textit{the essence of medicine},
as recognized by society and the courts, is not a repertoire of
procedures with statistical evidence for their healing effectiveness;
but, rather, the right air of authority.}

{
 But the subject here is the proliferation of traditions in
psychotherapy. So far as I can discern, this was the way you picked up
prestige in the field---not by discovering an amazing new technique
whose effectiveness could be experimentally verified and adopted by
all; but, rather, by splitting off your own
``school,'' supported by your
charisma as founder, and by the good stories you told about all the
reasons your techniques \textit{should} work.}

{
 This was probably, to no small extent, responsible for the
existence and continuation of psychotherapy in the first place---the
promise of making yourself a Master, like Freud who'd
done it first (also without the slightest scrap of experimental
evidence). That's the brass ring of success to
chase---the prospect of being a guru and having your own adherents.
It's the struggle for adherents that keeps the clergy
vital.}

{
 That's what happens to a field when it unbinds
itself from the experimental evidence---though there were other factors
that also placed psychotherapists at risk, such as the deference shown
them by their patients, the wish of society to believe that mental
healing was possible, and, of course, the general dangers of telling
people how to think.}

{
 (Dawes wrote in the '80s and I know that the
Rorschach was still in use as recently as the '90s, but
it's possible matters have improved since then (as one
commenter states). I do remember hearing that there was positive
evidence for the greater effectiveness of cognitive-behavioral
therapy.)}

{
 The field of hedonic psychology (happiness studies) began, to some
extent, with the realization that you could \textit{measure}
happiness---that there was a family of measures that by golly did
validate well against each other.}

{
 The act of creating a new measurement creates new science; if
it's a \textit{good} measurement, you get good
science.}

{
 If you're going to create an organized practice of
anything, you really do need some way of telling how well
you're doing, and a practice of doing serious
testing---that means a control group, an experimental group, and
statistics---on plausible-sounding techniques that people come up with.
You \textit{really} need it.}

{\centering
 \ ~
\par}

{\centering
 *
\par}


\bigskip

{
 1. Robyn M. Dawes, \textit{House of Cards: Psychology and
Psychotherapy Built on Myth} (Free Press, 1996).}

\mysection{Three Levels of Rationality Verification}

{
 I strongly suspect that there is a possible art of rationality
(attaining the map that reflects the territory, choosing so as to
direct reality into regions high in your preference ordering) that goes
beyond the skills that are standard, and beyond what any single
practitioner singly knows. I have a sense that more is possible. }

{
 The degree to which a \textit{group} of people can do anything
useful about this, will depend \textit{overwhelmingly} on what methods
we can devise to \textit{verify} our many amazing good ideas.}

{
 I suggest stratifying verification methods into three levels of
usefulness:}

{
 Reputational}

{
 Experimental}

{
 Organizational.}

{
 If your martial arts master occasionally fights realistic duels
(ideally, \textit{real} duels) against the masters of other schools,
and wins or at least doesn't lose too often, then you
know that the master's \textit{reputation} is
\textit{grounded in reality}; you know that your master is not a
complete poseur. The same would go if your school regularly competed
against other schools. You'd be
\textit{keepin' it real.}}

{
 Some martial arts fail to compete realistically enough, and their
students go down in seconds against real streetfighters. Other martial
arts schools fail to compete at \textit{all}{}---except based on
charisma and good stories---and their masters decide they have chi
powers. In this latter class we can also place the splintered schools
of psychoanalysis.}

{
 So even just the basic step of trying to \textit{ground
reputations} in some realistic trial other than charisma and good
stories has tremendous positive effects on a whole field of endeavor.}

{
 But that doesn't yet get you a science. A science
requires that you be able to test 100 applications of method A against
100 applications of method B and run statistics on the results.
\textit{Experiments} have to be replicable and replicated. This
requires \textit{standard measurements} that can be run on students
who've been taught using randomly-assigned alternative
methods, not just \textit{realistic duels} fought between masters using
all of their accumulated techniques and strength.}

{
 The field of happiness studies was created, more or less, by
realizing that asking people ``On a scale of 1 to 10,
how good do you feel right now?'' was a measure that
statistically validated well against other ideas for measuring
happiness. And this, despite all skepticism, looks like
it's actually a pretty useful measure of some things,
if you ask 100 people and average the results.}

{
 But suppose you wanted to put happier people in positions of
power---pay happy people to train other people to be happier, or employ
the happiest at a hedge fund? Then you're going to need
some test that's \textit{harder to game} than just
asking someone ``How happy are
you?''}

{
 This question of verification methods good enough to build
\textit{organizations} is a huge problem at all levels of modern human
society. If you're going to use the SAT to control
admissions to elite colleges, then can the SAT be defeated by studying
\textit{just} for the SAT in a way that ends up not correlating to
other scholastic potential? If you give colleges the power to grant
degrees, then do they have an incentive not to fail people? (I consider
it drop-dead obvious that the task of verifying acquired skills and
hence the power to grant degrees should be separated from the
institutions that do the teaching, but let's not go
into that.) If a hedge fund posts 20\% returns, are they really that
much better than the indices, or are they selling puts that will blow
up in a down market?}

{
 If you have a verification method that can be gamed, the whole
field adapts to game it, and loses its purpose. Colleges turn into
tests of whether you can endure the classes. High schools do nothing
but teach to statewide tests. Hedge funds sell puts to boost their
returns.}

{
 On the other hand---we still manage to teach engineers, even
though our organizational verification methods aren't
perfect. So what perfect or imperfect methods could you use for
verifying rationality skills, that would be at least a \textit{little}
resistant to gaming?}

{
 (Measurements with high noise can still be used
\textit{experimentally}, if you randomly assign enough subjects to have
an expectation of washing out the variance. But for the
\textit{organizational} purpose of verifying particular individuals,
you need low-noise measurements.)}

{
 So I now put to you the question---how do you verify rationality
skills? At any of the three levels? Brainstorm, I beg you; even a
difficult and expensive measurement can become a gold standard to
verify other metrics. Feel free to email me at yudkowsky@gmail.com to
suggest any measurements that are better off not being publicly known
(though this is of course a major disadvantage of that method). Stupid
ideas can suggest good ideas, so if you can't come up
with a good idea, \textit{come up with a stupid one.}}

{
 Reputational, experimental, organizational:}

{
 Something the masters and schools can do to keep it real
(realistically real);}

{
 Something you can do to measure each of a hundred students;}

{
 Something you could use as a test even if people have an incentive
to game it.}

{
 Finding good solutions at each level determines what a whole field
of study can be useful for---how much it can hope to accomplish. This
is one of the Big Important Foundational Questions, so---}

{
 \textit{Think!}}

{
 (PS: And ponder on your own before you look at
others' ideas; we need breadth of coverage here.)}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Why Our Kind Can't Cooperate}

{
 From when I was still forced to attend, I remember our
synagogue's annual fundraising appeal. It was a simple
enough format, if I recall correctly. The rabbi and the treasurer
talked about the shul's expenses and how vital this
annual fundraise was, and then the synagogue's members
called out their pledges from their seats. }

{
 Straightforward, yes?}

{
 Let me tell you about a different annual fundraising appeal. One
that I ran, in fact, during the early years of the Machine Intelligence
Research Institute. One difference was that the appeal was conducted
over the Internet. And another difference was that the audience was
largely drawn from the atheist / libertarian / technophile / science
fiction fan / early adopter / programmer / etc. crowd. (To point in the
rough direction of an empirical cluster in personspace. If you
understood the phrase ``empirical cluster in
personspace'' then you know who I'm
talking about.)}

{
 I crafted the fundraising appeal with care. By my nature
I'm too proud to ask other people for help; but
I've gotten over around 60\% of that reluctance over
the years. The nonprofit needed money and was growing too slowly, so I
put some force and poetry into that year's annual
appeal. I sent it out to several mailing lists that covered most of our
potential support base.}

{
 And almost immediately, people started posting to the mailing
lists about why they weren't going to donate. Some of
them raised basic questions about the nonprofit's
philosophy and mission. Others talked about their brilliant ideas for
all the \textit{other} sources that the nonprofit could get funding
from, instead of them. (They didn't volunteer to
contact any of those sources \textit{themselves}, they just had ideas
for how \textit{we} could do it.)}

{
 Now you might say, ``Well, maybe your mission and
philosophy \textit{did} have basic problems---you
wouldn't want to \textit{censor} that discussion, would
you?''}

{
 Hold on to that thought.}

{
 Because people \textit{were} donating. We started getting
donations right away, via Paypal. We even got congratulatory notes
saying how the appeal had finally gotten them to start moving. A
donation of \$111.11 was accompanied by a message saying,
``I decided to give a little bit more. One more
hundred, one more ten, one more single, one more dime, and one more
penny. All may not be for one, but this one is trying to be for
all.''}

{
 But none of those donors posted their agreement to the mailing
list. Not one.}

{
 So far as any of those donors knew, they were alone. And when they
tuned in the next day, they discovered not thanks, but arguments for
why they \textit{shouldn't} have donated. The
criticisms, the justifications for not donating---\textit{only those}
were displayed proudly in the open.}

{
 As though the treasurer had finished his annual appeal, and
everyone \textit{not} making a pledge had proudly stood up to call out
justifications for refusing; while those making pledges whispered them
quietly, so that no one could hear.}

{
 I know someone with a rationalist cause who goes around
plaintively asking, ``How come the Raëlian
flying-saucer cult can get tens of thousands of members [probably
around 40,000] interested in complete nonsense, but we
can't even get a thousand people working on
this?''}

{
 The obvious wrong way to finish this thought is to say,
``Let's do what the Raëlians do!
Let's add some nonsense to this
meme!'' For the benefit of those not immediately
stopped by their ethical inhibitions, I will observe that there may be
a hundred failed flying saucer cults for every one that becomes famous.
And the Dark Side may require non-obvious skills, which \textit{you},
yes \textit{you}, do not have: Not everyone can be a Sith Lord. In
particular, if you talk about your planned lies on the public Internet,
you fail. I'm no master criminal, but even I can tell
certain people are not cut out to be crooks.}

{
 So it's probably not a good idea to cultivate a
sense of violated entitlement at the thought that some \textit{other}
group, who you think ought to be \textit{inferior} to you, has more
money and followers. That path leads to---pardon the expression---the
Dark Side.}

{
 But it probably \textit{does} make sense to start asking ourselves
some pointed questions, if supposed
``rationalists''
can't manage to \textit{coordinate} as well as a flying
saucer cult.}

{
 How do things work on the Dark Side?}

{
 The respected leader speaks, and there comes a chorus of pure
agreement: if there are any who harbor inward doubts, they keep them to
themselves. So all the individual members of the audience see this
atmosphere of pure agreement, and they feel more confident in the ideas
presented---even if they, personally, harbored inward doubts, why,
everyone \textit{else} seems to agree with it.}

{
 (``Pluralistic ignorance'' is
the standard label for this.)}

{
 If anyone is still unpersuaded after that, they leave the group
(or in some places, are executed)---and the remainder are more in
agreement, and reinforce each other with less interference.}

{
 (I call that ``evaporative cooling of
groups.'')}

{
 The \textit{ideas} themselves, not just the leader, generate
unbounded enthusiasm and praise. The halo effect is that perceptions of
all positive qualities correlate---e.g. telling subjects about the
benefits of a food preservative made them judge it as lower-risk, even
though the quantities were logically uncorrelated. This can create a
positive feedback effect that makes an idea seem better and better and
better, especially if criticism is perceived as traitorous or sinful.}

{
 (Which I term the ``affective death
spiral.'')}

{
 So these are all examples of strong Dark Side forces that can bind
groups together.}

{
 And presumably \textit{we} would not go so far as to dirty our
hands with such \ldots}

{
 Therefore, as a group, the Light Side will always be divided and
weak. Technophiles, nerds, scientists, and even non-fundamentalist
religions will never be capable of acting with the fanatic unity that
animates radical Islam. Technological advantage can only go so far;
your tools can be copied or stolen, and used against you. In the end
the Light Side will always lose in any group conflict, and the future
inevitably belongs to the Dark.}

{
 I think that a person's reaction to this prospect
says a lot about their attitude towards
``rationality.''}

{
 Some ``Clash of Civilizations''
writers seem to accept that the Enlightenment is destined to lose out
in the long run to radical Islam, and sigh, and shake their heads
sadly. I suppose they're trying to signal their cynical
sophistication or something.}

{
 For myself, I always thought---call me loony---that a
\textit{true} rationalist ought to be \textit{effective in the real
world}.}

{
 So I have a problem with the idea that the Dark Side, thanks to
their \textit{pluralistic ignorance and affective death spirals}, will
always win because they are \textit{better coordinated} than us.}

{
 You would think, perhaps, that \textit{real} rationalists ought to
be \textit{more} coordinated? Surely all that unreason must have its
\textit{dis}advantages? That mode can't be
\textit{optimal}, can it?}

{
 And if current ``rationalist''
groups \textit{cannot} coordinate---if they can't
support group projects so well as a single synagogue draws donations
from its members---well, I leave it to you to finish that syllogism.}

{
 There's a saying I sometimes use:
``It is dangerous to be half a
rationalist.''}

{
 For example, I can think of ways to sabotage
someone's intelligence by \textit{selectively} teaching
them certain methods of rationality. Suppose you taught someone a long
list of logical fallacies and cognitive biases, and trained them to
spot those fallacies and biases in other people's
arguments. But you are careful to pick those fallacies and biases that
are \textit{easiest to accuse} others of, the most general ones that
can easily be misapplied. And you do \textit{not} warn them to
scrutinize \textit{arguments they agree with} just as hard as they
scrutinize \textit{incongruent} arguments for flaws. So they have
acquired a great repertoire of flaws of which to accuse only arguments
and arguers who they don't like. This, I suspect, is
one of the primary ways that smart people end up stupid. (And note, by
the way, that I have just given you another Fully General
Counterargument against smart people whose arguments you
don't like.)}

{
 Similarly, if you wanted to ensure that a group of
``rationalists'' never accomplished
any task requiring more than one person, you could teach them only
techniques of individual rationality, without mentioning anything about
techniques of coordinated group rationality.}

{
 I'll write more later on how I think rationalists
might be able to coordinate better. But here I want to focus on what
you might call \textit{the culture of disagreement}, or even
\textit{the culture of objections}, which is one of the two major
forces preventing the technophile crowd from coordinating.}

{
 Imagine that you're at a conference, and the
speaker gives a thirty-minute talk. Afterward, people line up at the
microphones for questions. The first questioner objects to the graph
used in slide 14 using a logarithmic scale; they quote Tufte on
\textit{The Visual Display of Quantitative Information}. The second
questioner disputes a claim made in slide 3. The third questioner
suggests an alternative hypothesis that seems to explain the same data
\ldots}

{
 Perfectly normal, right? Now imagine that you're
at a conference, and the speaker gives a thirty-minute talk. People
line up at the microphone.}

{
 The first person says, ``I agree with everything
you said in your talk, and I think you're
brilliant.'' Then steps aside.}

{
 The second person says, ``Slide 14 was beautiful,
I learned a lot from it. You're
awesome.'' Steps aside.}

{
 The third person---}

{
 Well, you'll never know what the third person at
the microphone had to say, because by this time, you've
fled screaming out of the room, propelled by a bone-deep terror as if
Cthulhu had erupted from the podium, the fear of the impossibly
unnatural phenomenon that has invaded your conference.}

{
 Yes, a group that can't tolerate disagreement is
not rational. But if you tolerate \textit{only} disagreement---if you
tolerate disagreement \textit{but not agreement}{}---then you also are
not rational. You're only willing to hear some honest
thoughts, but not others. You are a dangerous half-a-rationalist.}

{
 We are as uncomfortable \textit{together} as flying-saucer cult
members are uncomfortable \textit{apart}. That can't be
right either. Reversed stupidity is not intelligence.}

{
 Let's say we have two groups of soldiers. In group
1, the privates are ignorant of tactics and strategy; only the
sergeants know anything about tactics and only the officers know
anything about strategy. In group 2, everyone at all levels knows all
about tactics and strategy.}

{
 Should we expect group 1 to defeat group 2, because group 1 will
follow orders, while everyone in group 2 comes up with \textit{better
idea}s than whatever orders they were given?}

{
 In this case I have to question how much group 2 really
understands about military theory, because it is an \textit{elementary}
proposition that an uncoordinated mob gets slaughtered.}

{
 Doing worse with \textit{more knowledge} means you are doing
something very wrong. You should always be able to \textit{at least}
implement the same strategy you would use if you are ignorant, and
preferably do \textit{better}. You definitely should not do
\textit{worse}. If you find yourself regretting your
``rationality'' then you should
reconsider what is rational.}

{
 On the other hand, if you are only half-a-rationalist, you can
\textit{easily} do worse with more knowledge. I recall a lovely
experiment which showed that politically opinionated students with more
knowledge of the issues reacted less to incongruent evidence, because
they had more ammunition with which to counter-argue only incongruent
evidence.}

{
 We would seem to be stuck in an awful valley of partial
rationality where we end up more poorly coordinated than religious
fundamentalists, able to put forth less effort than flying-saucer
cultists. True, what little effort we \textit{do} manage to put forth
may be better-targeted at helping people rather than the reverse---but
that is not an acceptable excuse.}

{
 If I were setting forth to systematically train rationalists,
there would be lessons on how to disagree and lessons on how to agree,
lessons intended to make the trainee more comfortable with dissent, and
lessons intended to make them more comfortable with conformity. One day
everyone shows up dressed differently, another day they all show up in
uniform. You've got to cover both sides, or
you're only half a rationalist.}

{
 Can you imagine training prospective rationalists to wear a
uniform and march in lockstep, and practice sessions where they agree
with each other and applaud everything a speaker on a podium says? It
sounds like unspeakable horror, doesn't it, like the
whole thing has admitted outright to being an evil cult? But why is it
\textit{not} okay to practice that, while it \textit{is} okay to
practice \textit{disagreeing} with everyone else in the crowd? Are you
\textit{never} going to have to agree with the majority?}

{
 Our culture puts all the emphasis on heroic disagreement and
heroic defiance, and none on heroic agreement or heroic group
consensus. We signal our superior intelligence and our
\textit{membership in the nonconformist community} by inventing clever
objections to others' arguments. Perhaps \textit{that}
is why the technophile / Silicon Valley crowd stays marginalized,
losing battles with less nonconformist factions in larger society. No,
we're not losing because we're so
superior, we're losing because our exclusively
individualist traditions sabotage our ability to cooperate.}

{
 The other major component that I think sabotages group efforts in
the technophile community is \textit{being ashamed of strong feelings}.
We still have the Spock archetype of rationality stuck in our heads,
rationality as dispassion. Or perhaps a related mistake, rationality as
cynicism---trying to signal your superior world-weary sophistication by
showing that you care less than others. Being careful to
ostentatiously, publicly look down on those so naive as to show they
care strongly about anything.}

{
 Wouldn't it make you feel uncomfortable if the
speaker at the podium said that they cared so strongly about, say,
fighting aging, that they would willingly die for the cause?}

{
 But it is nowhere written in either probability theory or decision
theory that a rationalist should not care. I've looked
over those equations and, really, it's not in there.}

{
 The best informal definition I've ever heard of
rationality is ``That which can be destroyed by the
truth should be.'' We should aspire to feel the
emotions that fit the facts, not aspire to feel no emotion. If an
emotion can be destroyed by truth, we should relinquish it. But if a
cause is worth striving for, then let us by all means feel fully its
importance.}

{
 Some things \textit{are} worth dying for. Yes, really! And if we
can't get comfortable with admitting it and hearing
others say it, then we're going to have trouble
\textit{caring} enough---as well as \textit{coordinating} enough---to
put some effort into group projects. You've got to
teach both sides of it, ``That which can be destroyed
by the truth should be,'' and ``That
which the truth nourishes should thrive.''}

{
 I've heard it argued that the taboo against
emotional language in, say, science papers, is an important part of
letting the facts fight it out without distraction. That
doesn't mean the taboo should apply everywhere. I think
that there are parts of life where we should learn to \textit{applaud}
strong emotional language, eloquence, and poetry. When
there's something that needs doing, poetic appeals help
get it done, and, therefore, are themselves to be applauded.}

{
 We need to keep our efforts to expose \textit{counterproductive}
causes and \textit{unjustified} appeals from stomping on tasks that
genuinely need doing. You need both sides of it---the willingness to
turn away from counterproductive causes, and the willingness to praise
productive ones; the strength to be unswayed by ungrounded appeals, and
the strength to be swayed by grounded ones.}

{
 I think the synagogue at their annual appeal had it right, really.
They weren't going down row by row and putting
individuals on the spot, staring at them and saying,
``How much will \textit{you} donate, Mr.
Schwartz?'' People simply announced their
pledges---not with grand drama and pride, just simple
announcements---and that encouraged others to do the same. Those who
had nothing to give, stayed silent; those who had objections, chose
some later or earlier time to voice them. That's
probably about the way things \textit{should} be in a sane human
community---taking into account that people often have trouble getting
as motivated as they wish they were, and can be helped by social
encouragement to overcome this weakness of will.}

{
 But even if you disagree with that part, then let us say that both
supporting and countersupporting opinions should have been publicly
voiced. Supporters being faced by an apparently solid wall of
objections and disagreements---even if it resulted from their own
uncomfortable self-censorship---is \textit{not} group rationality. It
is the mere \textit{mirror image} of what Dark Side groups do to keep
their followers. Reversed stupidity is not intelligence.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Tolerate Tolerance}

{
 One of the likely characteristics of someone who sets out to be a
``rationalist'' is a
lower-than-usual tolerance for flaws in reasoning. This
doesn't strictly follow. You could end up, say,
rejecting your religion, just because you spotted \textit{more} or
\textit{deeper} flaws in the reasoning, not because you were, by your
nature, \textit{more annoyed} at a flaw of fixed size. But
realistically speaking, a lot of us probably have our level of
``annoyance at all these flaws we're
spotting'' set a bit higher than average. }

{
 That's why it's so important for
us to tolerate others' tolerance if we want to get
anything done together.}

{
 For me, the poster case of tolerance I need to tolerate is Ben
Goertzel, who among other things runs an annual AI conference, and who
has something nice to say about \textit{everyone}. Ben even
complimented the ideas of M*nt*f*x, the most legendary of all AI
crackpots. (M*nt*f*x apparently started adding a link to
Ben's compliment in his email signatures, presumably
because it was the only compliment he'd ever gotten
from a bona fide AI academic.) (Please do \textit{not} pronounce his
True Name correctly or he will be summoned here.)}

{
 But I've come to understand that this is one of
Ben's strengths---that he's nice to
lots of people that others might ignore, including, say, me---and every
now and then this pays off for him.}

{
 And if I subtract points off Ben's reputation for
finding something nice to say about people and projects that I think
are hopeless---even \textit{M*nt*f*x}{}---then what I'm
doing is insisting that Ben \textit{dislike everyone I dislike} before
I can work with him.}

{
 Is that a realistic standard? Especially if different people are
annoyed in different amounts by different things?}

{
 But it's hard to remember that when Ben is being
nice to \textit{so many} idiots.}

{
 Cooperation is unstable, in both game theory and evolutionary
biology, without \textit{some} kind of punishment for defection. So
it's one thing to subtract points off
someone's reputation for mistakes they make
\textit{themselves, directly}. But if you also look askance at someone
for \textit{refusing to castigate} a person or idea, then that is
\textit{punishment of non-punishers}, a far more dangerous idiom that
can lock an equilibrium in place even if it's harmful
to \textit{everyone} involved.}

{
 The danger of punishing non-punishers is something I remind myself
of, say, every time Robin Hanson points out a flaw in some academic
trope and yet modestly confesses he could be wrong (and
he's not wrong). Or every time I see Michael Vassar
still considering the potential of someone who I wrote off as hopeless
within thirty seconds of being introduced to them. I have to remind
myself, ``Tolerate tolerance! Don't
demand that your allies be \textit{equally extreme} in their negative
judgments of everything you dislike!''}

{
 By my nature, I \textit{do} get annoyed when someone else seems to
be giving too much credit. I don't know if
everyone's like that, but I suspect that at least
\textit{some} of my fellow aspiring rationalists are. I
wouldn't be surprised to find it a human universal; it
does have an obvious evolutionary rationale---one which would make it a
very \textit{unpleasant} and \textit{dangerous} adaptation.}

{
 I am not generally a fan of
``tolerance.'' I certainly
don't believe in being ``intolerant of
intolerance,'' as some inconsistently hold. But I
shall go on trying to tolerate \textit{people who are more tolerant
than I am}, and judge them only for their \textit{own} un-borrowed
mistakes.}

{
 Oh, and it goes without saying that if the people of Group X are
staring at you demandingly, waiting for you to hate the right enemies
with the right intensity, and ready to castigate you if you fail to
castigate loudly enough, you may be hanging around the wrong group.}

{
 Just don't demand that \textit{everyone} you work
with be equally intolerant of behavior like that. Forgive your friends
if some of them suggest that maybe Group X wasn't so
awful after all \ldots}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Your Price for Joining}

{
 In the Ultimatum Game, the first player chooses how to split \$10
between themselves and the second player, and the second player decides
whether to accept the split or reject it---in the latter case, both
parties get nothing. So far as conventional causal decision theory goes
(two-box on Newcomb's Problem, defect in
Prisoner's Dilemma), the second player should prefer
any non-zero amount to nothing. But if the first player
\textit{expects} this behavior---accept any non-zero offer---then they
have no motive to offer more than a penny. As I assume you all know by
now, I am no fan of conventional causal decision theory. Those of us
who remain interested in cooperating on the Prisoner's
Dilemma, either because it's iterated, or because we
have a term in our utility function for fairness, or because we use an
unconventional decision theory, may also not accept an offer of one
penny. }

{
 And in fact, most Ultimatum
``deciders'' offer an even split;
and most Ultimatum ``accepters''
reject any offer less than 20\%. A 100 USD game played in Indonesia
(average per capita income at the time: 670 USD) showed offers of 30
USD being turned down, although this equates to two
week's wages. We can probably also assume that the
players in Indonesia were not thinking about the academic debate over
Newcomblike problems---this is just the way people feel about Ultimatum
Games, even ones played for real money.}

{
 There's an analogue of the Ultimatum Game in group
coordination. (Has it been studied? I'd hope so \ldots)
Let's say there's a common project---in
fact, let's say that it's an altruistic
common project, aimed at helping mugging victims in Canada, \textit{or
something}. If you join this group project, you'll get
more done than you could on your own, relative to your utility
function. So, obviously, you should join.}

{
 But wait! The anti-mugging project keeps their funds invested in a
money market fund! That's ridiculous; it
won't earn even as much interest as US Treasuries, let
alone a dividend-paying index fund.}

{
 Clearly, this project is run by morons, and you
shouldn't join until they change their malinvesting
ways.}

{
 Now you might realize---if you stopped to think about it---that
all things considered, you would \textit{still} do better by working
with the common anti-mugging project, than striking out on your own to
fight crime. But then---you might perhaps also realize---if you
\textit{too easily assent} to joining the group, why, what
\textit{motive} would they have to change their malinvesting ways?}

{
 Well \ldots Okay, look. Possibly because we're out
of the ancestral environment where everyone knows everyone else \ldots
and possibly because the nonconformist crowd tries to repudiate
\textit{normal} group-cohering forces like conformity and
leader-worship \ldots}

{
 \ldots It seems to me that people in the atheist / libertarian /
technophile / science fiction fan / etc. cluster often set their
joining prices \textit{way way way} too high. Like a 50-way split
Ultimatum game, where every one of 50 players demands at least 20\% of
the money.}

{
 If you think how often situations like this would have arisen in
the ancestral environment, then it's almost certainly a
matter of evolutionary psychology. System 1 emotions, not System 2
calculation. Our intuitions for when to join groups, versus when to
hold out for more concessions to our own preferred way of doing things,
would have been honed for hunter-gatherer environments of, e.g., 40
people, all of whom you knew personally.}

{
 And if the group is made up of 1,000 people? Then your
hunter-gatherer instincts will underestimate the inertia of a group so
large, and demand an unrealistically high price (in strategic shifts)
for you to join. There's a limited amount of
organizational effort, and a limited number of degrees of freedom, that
can go into doing things any one person's way.}

{
 And if the strategy is large and complex, the sort of thing that
takes e.g. ten people doing paperwork for a week, rather than being
hammered out over a half-hour of negotiation around a campfire? Then
your hunter-gatherer instincts will underestimate the inertia of the
group, relative to your own demands.}

{
 And if you live in a wider world than a single hunter-gatherer
tribe, so that you only see the one group representative who negotiates
with you, and not the hundred other negotiations that have taken place
already? Then your instincts will tell you that it is just one person,
a stranger at that, and the two of you are equals; whatever ideas they
bring to the table are equal with whatever ideas you bring to the
table, and the meeting point ought to be about even.}

{
 And if you suffer from any weakness of will or akrasia, or if you
are influenced by motives other than those you would admit to yourself
that you are influenced by, then any group-altruistic project that does
not offer you the rewards of status and control may perhaps find itself
underserved by your attentions.}

{
 Now I do admit that I speak here primarily from the perspective of
someone who goes around trying to herd cats; and not from the other
side as someone who spends most of their time withholding their
energies in order to blackmail those damned morons already on the
project. Perhaps I am a little prejudiced.}

{
 But it seems to me that a reasonable rule of thumb might be as
follows:}

{
 If, on the whole, joining your efforts to a group project
\textit{would still have a net positive effect} according to your
utility function---}

{
 (or a larger positive effect than any other marginal use to which
you could otherwise put those resources, although this latter mode of
thinking seems little-used and humanly-unrealistic, for reasons I may
write about some other time)}

{
 {}---and the awful horrible annoying issue is not so important
that \textit{you personally} will get involved deeply enough to put in
however many hours, weeks, or years may be required to get it fixed
up---}

{
 {}---then the issue is not worth you withholding your energies
from the project; either instinctively until you see that people are
paying attention to you and respecting you, or by conscious intent to
blackmail the group into getting it done.}

{
 And if the issue \textit{is} worth that much to you \ldots then by
all means, join the group and do whatever it takes to get things fixed
up.}

{
 Now, if the existing contributors refuse to let you do this,
\textit{and} a reasonable third party would be expected to conclude
that you were competent enough to do it, \textit{and} there is no one
else whose ox is being gored thereby, \textit{then}, perhaps, we have a
problem on our hands. And it may be time for a little blackmail, if the
resources you can conditionally commit are large enough to get their
attention.}

{
 Is this rule a little extreme? Oh, maybe. There \textit{should} be
a motive for the decision-making mechanism of a project to be
responsible to its supporters; unconditional support would create its
own problems.}

{
 But \textit{usually} \ldots I observe that people underestimate the
costs of what they ask for, or perhaps just act on instinct, and set
their prices \textit{way way way} too high. If the nonconformist crowd
ever wants to get anything done together, we need to move in the
direction of joining groups and staying there at least a
\textit{little} more easily. Even in the face of annoyances and
imperfections! Even in the face of unresponsiveness to our own better
ideas!}

{
 In the age of the Internet and in the company of nonconformists,
it does get a little tiring reading the 451st public email from someone
saying that the Common Project isn't worth their
resources until the website has a sans-serif font.}

{
 Of course this often isn't really about fonts. It
may be about laziness, akrasia, or hidden rejections. But in terms of
group norms \ldots in terms of what sort of public statements we
respect, and which excuses we publicly scorn \ldots we probably
\textit{do} want to encourage a group norm of:}

{
 \textit{If the issue isn't worth your personally
fixing by however much effort it takes, and it doesn't
arise from outright bad faith, it's not worth refusing
to contribute your efforts to a cause you deem worthwhile.}}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Can Humanism Match Religion's Output?}

{
 Perhaps the single largest \textit{voluntary} institution of our
modern world---bound together not by police and taxation, not by
salaries and managers, but by voluntary donations flowing from its
members---is the Catholic Church. }

{
 It's too large to be held together by individual
negotiations, like a group task in a hunter-gatherer band. But in a
larger world with more people to be infected and faster transmission,
we can expect more virulent memes. The Old Testament
doesn't talk about Hell, but the New Testament does.
The Catholic Church is held together by affective death
spirals---around the ideas, the institutions, and the leaders. By
promises of eternal happiness and eternal damnation---theologians
don't really believe that stuff, but many ordinary
Catholics do. By simple conformity of people meeting in person at a
Church and being subjected to peer pressure. Et cetera.}

{
 We who have the temerity to call ourselves
``rationalists'' think ourselves too
good for such communal bindings.}

{
 And so anyone with a \textit{simple} and \textit{obvious}
charitable project---responding with food and shelter to a tidal wave
in Thailand, say---would be better off by \textit{far} pleading with
the Pope to mobilize the Catholics, rather than with Richard Dawkins to
mobilize the atheists.}

{
 \textit{For so long as this is true}, any increase in atheism at
the expense of Catholicism will be something of a hollow victory,
regardless of all other benefits.}

{
 True, the Catholic Church also goes around opposing the use of
condoms in AIDS-ravaged Africa. True, they waste huge amounts of the
money they raise on all that religious stuff. Indulging in unclear
thinking is not harmless; prayer comes with a price.}

{
 To refrain from doing damaging things \textit{is} a true victory
for a rationalist \ldots}

{
 Unless it is your \textit{only} victory, in which case it seems a
little empty.}

{
 If you \textit{discount all harm} done by the Catholic Church, and
look \textit{only} at the good \ldots then does the average Catholic do
more \textit{gross} good than the average atheist, just by virtue of
being more active?}

{
 Perhaps if you are wiser but less motivated, you can search out
interventions of high efficiency and purchase utilons on the cheap
\ldots But there are few of us who \textit{really} do that, as opposed
to planning to do it someday.}

{
 Now you might at this point throw up your hands, saying:
``For so long as we don't have direct
control over our brain's motivational circuitry,
it's not realistic to expect a rationalist to be as
strongly motivated as someone who genuinely believes that
they'll burn eternally in hell if they
don't obey.''}

{
 This is a fair point. Any folk theorem to the effect that a
rational agent should do at least as well as a non-rational agent will
rely on the assumption that the rational agent can always just
implement whatever ``irrational''
policy is observed to win. But if you can't
\textit{choose} to have unlimited mental energy, then it may be that
some false beliefs are, in cold fact, more strongly motivating than any
available true beliefs. And if we all generally suffer from altruistic
akrasia, being unable to bring ourselves to help as much as we think we
should, then it is possible for the God-fearing to win the contest of
altruistic output.}

{
 But though it is a motivated continuation, let us consider this
question a little further.}

{
 Even the fear of hell is not a perfect motivator. Human beings are
not given so much slack on evolution's leash; we can
resist motivation for a short time, but then we run out of mental
energy (hat tip: infotropism). Even believing that
you'll go to hell does not change this brute fact about
brain circuitry. So the religious sin, and then are tormented by
thoughts of going to hell, in much the same way that smokers reproach
themselves for being unable to quit.}

{
 If a group of rationalists cared \textit{a lot} about something
\ldots who says they wouldn't be able to match the real,
de-facto output of a believing Catholic? The stakes might not be
``infinite'' happiness or
``eternal'' damnation, but of course
the brain can't visualize 3 $\uparrow \uparrow \uparrow
$ 3, let alone infinity. Who says that the actual quantity of caring
neurotransmitters discharged by the brain (as 'twere)
has to be so much less for ``the growth and flowering
of humankind'' or even
``tidal-wave-stricken Thais,'' than
for ``eternal happiness in Heaven''?
Anything involving more than 100 people is going to involve utilities
too large to visualize. And there are all sorts of other standard
biases at work here; knowing about them might be good for a bonus as
well, one hopes?}

{
 Cognitive-behavioral therapy and Zen meditation are two mental
disciplines experimentally shown to yield real improvements. It is not
the area of the art I've focused on developing, but
then I don't have a real martial art of rationality in
back of me. If you combine a purpose genuinely worth caring about with
discipline extracted from CBT and Zen meditation, then who says
rationalists can't keep up? Or even more generally: if
we have an evidence-based art of fighting akrasia, with experiments to
see what actually works, then who says we've got to be
less motivated than some disorganized mind that fears
God's wrath?}

{
 Still \ldots that's a further-future speculation
that it might be possible to develop an art that
doesn't presently exist. It's not a
technique I can use right now. I present it just to illustrate the idea
of \textit{not giving up so fast on rationality}: Understanding
what's going wrong, trying intelligently to fix it, and
gathering evidence on whether it worked---this is a powerful idiom, not
to be lightly dismissed upon sighting the first disadvantage.}

{
 Really, I suspect that what's going on here has
less to do with the motivating power of eternal damnation, and a lot
more to do with the motivating power of \textit{physically meeting}
other people who share your cause. The power, in other words, of being
physically present at church and having religious neighbors.}

{
 This is a problem for the rationalist community in its present
stage of growth, because we are rare and geographically distributed way
the hell all over the place. If all the readers of \textit{Less Wrong}
lived within a five-mile radius of each other, I bet
we'd get a lot more done, not for reasons of
\textit{coordination} but just sheer \textit{motivation}.}

{
 I'll write later about some long-term,
starry-eyed, idealistic thoughts on this particular problem.
Shorter-term solutions that don't rely on our
increasing our numbers by a factor of 100 would be better. I wonder in
particular whether the best modern videoconferencing software would
provide some of the motivating effect of meeting someone in person; I
suspect the answer is ``no'' but it
might be worth trying.}

{
 Meanwhile \ldots in the short term, we're stuck
fighting akrasia mostly without the reinforcing physical presense of
other people who care. I want to say something like
``This is difficult, but it \textit{can} be
done,'' except I'm not sure
that's even true.}

{
 I suspect that the \textit{largest} step rationalists could take
toward matching the per-capita power output of the Catholic Church
would be to have regular physical meetings of people contributing to
the same task---just for purposes of motivation.}

{
 In the absence of that \ldots}

{
 We could try for a group norm of being openly allowed---nay,
applauded---for caring strongly about something. And a group norm of
being expected to do something useful with your life---contribute your
part to cleaning up this world. Religion doesn't really
emphasize the getting-things-\textit{done} aspect as much.}

{
 And if rationalists could match just \textit{half} the average
altruistic effort output per Catholic, then I don't
think it's \textit{remotely} unrealistic to suppose
that with better targeting on more efficient causes, the modal
rationalist could get twice as much done.}

{
 How much of its earnings does the Catholic Church spend on all
that useless religious stuff instead of actually helping people? More
than 50\%, I would venture. So then we could say---with a certain
irony, though that's not quite the spirit in which we
should be doing things---that we should try to propagate a group norm
of donating a minimum of 5\% of income to \textit{real} causes. (10\%
being the usual suggested minimum religious tithe.) And then
there's the art of picking causes for which expected
utilons are orders of magnitude cheaper (for so long as the inefficient
market in utilons lasts).}

{
 But long before we can begin to dream of any such boast, we
secular humanists need to work on at least \textit{matching} the per
capita benevolent output of the worshippers.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Church vs. Taskforce}

{
 I am generally suspicious of envying crazy groups or trying to
blindly copycat the rhythm of religion---what I called
``hymns to the nonexistence of
God,'' replying, ``A good
`atheistic hymn' is simply a song about
anything worth singing about that doesn't happen to be
religious.'' }

{
 But religion does fill certain holes in people's
minds, some of which are even worth filling. If you eliminate religion,
you have to be aware of what gaps are left behind.}

{
 If you suddenly deleted religion from the world, the largest gap
left would not be anything of ideals or morals; it would be the church,
the community. Among those who now stay religious without quite really
believing in God---how many are just sticking to it from wanting to
stay with their neighbors at the church, and their family and friends?
How many would convert to atheism, if all those others deconverted, and
\textit{that} were the price of staying in the community and keeping
its respect? I would guess \ldots probably quite a lot.}

{
 In truth \ldots this is probably something I don't
understand all that well, myself. ``Brownies and
babysitting'' were the first two things that came to
mind. Do churches lend helping hands in emergencies? Or just a shoulder
to cry on? How strong is a church community? It probably depends on the
church, and in any case, that's not the correct
question. One should start by considering what a hunter-gatherer band
gives its people, and ask what's missing in modern
life---if a modern First World church fills only \textit{some} of that,
then by all means let us try to do \textit{better}.}

{
 So \textit{without} copycatting religion---\textit{without}
assuming that we \textit{must} gather every Sunday morning in a
building with stained-glass windows while the children dress up in
formal clothes and listen to someone sing---let's
consider how to fill the emotional gap, after religion stops being an
option.}

{
 To help break the mold to start with---the straitjacket of cached
thoughts on how to do this sort of thing---consider that \textit{some}
modern offices may also fill the same role as a church. By which I mean
that some people are fortunate to receive community from their
workplaces: friendly coworkers who bake brownies for the office, whose
teenagers can be safely hired for babysitting, and maybe even help in
times of catastrophe \ldots ? But certainly not everyone is lucky enough
to find a community at the office.}

{
 Consider further---a church is \textit{ostensibly} about worship,
and a workplace is \textit{ostensibly} about the commercial purpose of
the organization. Neither has been carefully \textit{optimized} to
serve as a community.}

{
 Looking at a typical religious church, for example, you could
suspect---although all of these things would be better tested
experimentally, than just suspected---}

{
 That getting up early on a Sunday morning is not optimal;}

{
 That wearing formal clothes is not optimal, especially for
children;}

{
 That listening to the same person give sermons on the same theme
every week (``religion'') is not
optimal;}

{
 That the cost of supporting a church and a pastor is expensive,
compared to the number of different communities who could time-share
the same building for their gatherings;}

{
 That they probably don't serve nearly enough of a
matchmaking purpose, because churches think they're
supposed to enforce their medieval moralities;}

{
 That the whole thing ought to be subject to experimental
data-gathering to find out what works and what
doesn't.}

{
 By using the word ``optimal''
above, I mean ``optimal under the criteria you would
use if you were explicitly building a community \textit{qua}
community.'' Spending lots of money on a fancy church
with stained-glass windows and a full-time pastor makes sense if you
actually \textit{want} to spend money on religion \textit{qua}
religion.}

{
 I do confess that when walking past the churches of my city, my
main thought is, ``These buildings look really, really
expensive, and there are too many of them.'' If you
were doing it over from scratch \ldots then you might have a big
building that could be used for the occasional wedding, but it would be
time-shared for different communities meeting at different times on the
weekend, and it would also have a nice large video display that could
be used for speakers giving presentations, lecturers teaching
something, or maybe even showing movies. Stained glass? Not so high a
priority.}

{
 Or to the extent that the church membership lends a helping hand
in times of trouble---could that be improved by an explicit rainy-day
fund or contracting with an insurer, once you realized that this was an
important function? Possibly \textit{not}; dragging explicit finance
into things changes their character oddly. Conversely, maybe keeping
current on some insurance policies should be a \textit{requirement} for
membership, lest you rely \textit{too much} on the community \ldots But
again, to the extent that churches provide community,
they're trying to do it without actually
\textit{admitting} that this is nearly all of what people get out of
it. Same thing with the corporations whose workplaces are friendly
enough to serve as communities; it's still something of
an accidental function.}

{
 Once you start thinking \textit{explicitly} about how to give
people a hunter-gatherer band to belong to, you can see all sorts of
things that sound like good ideas. Should you welcome the newcomer in
your midst? The pastor may give a sermon on that sometime, if you think
church is about religion. But if you're explicitly
setting out to build community---then right after a move is when
someone most lacks community, when they most need your help.
It's also an opportunity for the band to grow. If
anything, tribes ought to be competing at quarterly exhibitions to
capture newcomers.}

{
 But can you really have a community that's
\textit{just} a community---that isn't also an office
or a religion? A community with no purpose beyond itself?}

{
 Maybe you \textit{can.} After all, did hunter-gatherer tribes have
any purposes beyond themselves?---well, there was survival and feeding
yourselves, that was a purpose.}

{
 But anything that people have in common, especially any
\textit{goal} they have in common, tends to \textit{want} to define a
community. Why not take advantage of that?}

{
 Though in this age of the Internet, alas, too many binding factors
have supporters too widely distributed to form a decent band---if
you're the only member of the Church of the Subgenius
in your city, it may not really help much. It really is different
without the physical presence; the Internet does \textit{not} seem to
be an acceptable substitute at the current stage of the technology.}

{
 So to skip right to the point---}

{
 Should the Earth last so long, I would like to see, as the form of
rationalist communities, taskforces focused on all the work that needs
doing to fix up this world. Communities in any geographic area would
form around the most specific cluster that could support a decent-sized
band. If your city doesn't have enough people in it for
you to find 50 fellow Linux programmers, you might have to settle for
15 fellow open-source programmers \ldots or in the days when all of this
is only getting started, 15 fellow rationalists trying to spruce up the
Earth in their assorted ways.}

{
 That's what I think would be a fitting direction
for the energies of communities, and a common purpose that would bind
them together. Tasks like that need communities anyway, and this Earth
has plenty of work that needs doing, so there's no
point in waste. We have so much that needs doing---let the energy that
was once wasted into the void of religious institutions, find an outlet
there. And let purposes admirable without need for delusion fill any
void in the community structure left by deleting religion and its
illusionary higher purposes.}

{
 Strong communities built around worthwhile purposes: That would be
the shape I would like to see for the post-religious age, or whatever
fraction of humanity has then gotten so far in their lives.}

{
 Although \ldots as long as you've got a building
with a nice large high-resolution screen anyway, I
wouldn't mind challenging the idea that all
post-adulthood learning has to take place in distant expensive
university campuses with teachers who would rather be doing something
else. And it's empirically the case that colleges seem
to support communities quite well. So in all fairness, there are other
possibilities for things you could build a post-theistic community
around.}

{
 Is all of this just a dream? Maybe. Probably. It's
not completely devoid of incremental implementability, if
you've got enough rationalists in a sufficiently large
city who have heard of the idea. But on the off chance that rationality
should catch on so widely, or the Earth should last so long, and that
my voice should be heard, then that is the direction I would like to
see things moving in---as the churches fade, we don't
need artificial churches, but we do need new idioms of community.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Rationality: Common Interest of Many Causes}

{
 It is a not-so-hidden agenda of \textit{Less Wrong} that there are
many causes that benefit from the spread of rationality---because it
takes a little more rationality than usual to see their case as a
supporter, or even just as a supportive bystander. Not just the obvious
causes like atheism, but things like marijuana legalization---where you
could wish that people were a bit more self-aware about their motives
and the nature of signaling, and a bit more moved by inconvenient cold
facts. The Machine Intelligence Research Institute was merely an
unusually extreme case of this, wherein it got to the point that after
years of bogging down I threw up my hands and explicitly recursed on
the job of creating rationalists. }

{
 But of course, not \textit{all} the rationalists I create will be
interested in my \textit{own} project---\textit{and
that's fine.} You can't capture
\textit{all} the value you create, and trying can have poor side
effects.}

{
 If the supporters of other causes are enlightened enough to think
similarly \ldots}

{
 Then all the causes that benefit from spreading rationality can,
perhaps, have something in the way of standardized material to which to
point their supporters---a common task, centralized to save
effort---and think of themselves as spreading a little rationality on
the side. They won't capture all the value they create.
And that's fine. They'll capture some
of the value others create. Atheism has very little to do directly with
marijuana legalization, but if both atheists and anti-Prohibitionists
are willing to step back a bit and say a bit about the general,
abstract principle of confronting a discomforting truth that interferes
with a fine righteous tirade, then both atheism and marijuana
legalization pick up some of the benefit from both efforts.}

{
 But this requires---I know I'm repeating myself
here, but it's important---that you be willing not to
capture all the value you create. It requires that, in the course of
talking about rationality, you maintain an ability to temporarily
\textit{shut up} about your own cause even though it is the best cause
ever. It requires that you don't regard those other
causes, and they do not regard you, as competing for a limited supply
of rationalists with a limited capacity for support; but, rather,
creating more rationalists and increasing their capacity for support.
You only reap some of your own efforts, but you reap some of
others' efforts as well.}

{
 If you and they don't agree on
everything---especially priorities---you have to be willing to agree to
\textit{shut up} about the disagreement. (Except possibly in
specialized venues, out of the way of the mainstream discourse, where
such disagreements are explicitly prosecuted.)}

{
 A certain person who was taking over as the president of a certain
organization once pointed out that the organization had not enjoyed
much luck with its message of ``This is \textit{the
best} thing you can do,'' as compared to e.g. the
X-Prize Foundation's tremendous success conveying to
rich individuals ``Here is \textit{a cool} thing you
can do.''}

{
 This is one of those insights where you blink incredulously and
then grasp how much sense it makes. The human brain
can't grasp large stakes, and people are not anything
remotely like expected utility maximizers, and we are generally
altruistic akrasics. Saying, ``This is the
\textit{best} thing'' doesn't add
much motivation beyond ``This is a cool
thing.'' It just establishes a much higher burden of
proof. And invites invidious motivation-sapping comparison to all other
good things you know (perhaps threatening to diminish moral
satisfaction already purchased).}

{
 If we're operating under the assumption that
everyone by default is an altruistic akrasic (someone who wishes they
could choose to do more)---or at least, that most potential supporters
of interest fit this description---then fighting it out over which
cause is the \textit{best} to support may have the effect of decreasing
the overall supply of altruism.}

{
 ``But,'' you say,
``dollars are fungible; a dollar you use for one thing
indeed cannot be used for anything else!'' To which I
reply: But human beings \textit{really aren't} expected
utility maximizers, as cognitive systems. Dollars come out of different
mental accounts, cost different amounts of \textit{willpower} (the true
limiting resource) under different circumstances. People want to spread
their donations around as an act of mental accounting to minimize the
regret if a single cause fails, and telling someone about an additional
cause may increase the total amount they're willing to
help.}

{
 There are, of course, limits to this principle of benign
tolerance. If someone's pet project is to teach salsa
dance, it would be quite a stretch to say they're
working on a worthy sub-task of the great common Neo-Enlightenment
project of human progress.}

{
 But to the extent that something really is a task you would wish
to see done on behalf of humanity \ldots then invidious comparisons of
that project to Your-Favorite-Project may not help your own project as
much as you might think. We may need to learn to say, by habit and in
nearly all forums, ``Here is \textit{a cool}
rationalist project,'' not, ``Mine
alone is \textit{the highest-return in expected utilons per marginal
dollar} project.'' If someone cold-blooded enough to
maximize expected utility of fungible money without regard to emotional
side effects \textit{explicitly asks}, we could perhaps steer them to a
\textit{specialized} subforum where anyone willing to make the claim of
\textit{top} priority fights it out. Though if all goes well, those
projects that have a strong claim to this kind of underserved-ness will
get more investment and their marginal returns will go down, and the
winner of the competing claims will no longer be clear.}

{
 If there are many rationalist projects that benefit from raising
the sanity waterline, then their mutual tolerance and common investment
in spreading rationality could conceivably exhibit a commons problem.
But this doesn't seem too hard to deal with: if
there's a group that's not willing to
share the rationalists they create or mention to them that other
Neo-Enlightenment projects might exist, then any common, centralized
rationalist resources could remove the mention of their project as a
cool thing to do.}

{
 Though all this is an idealistic and future-facing thought, the
benefits---for all of us---could be finding some important things
we're missing right now. So many rationalist projects
have few supporters and far-flung; if we could all identify as elements
of the Common Project of human progress, the Neo-Enlightenment, there
would be a substantially higher probability of finding ten of us in any
given city. Right now, a lot of these projects are just a little lonely
for their supporters. Rationality may not be \textit{the most important
thing in the world}{}---that, of course, is the thing that we
protect---but it is \textit{a cool} thing that more of us have in
common. We might gain much from identifying ourselves also as
rationalists.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Helpless Individuals}

{
 When you consider that our grouping instincts are optimized for
50-person hunter-gatherer bands where everyone knows everyone else, it
begins to seem miraculous that modern-day large institutions survive at
all. }

{
 Well---there are governments with specialized militaries and
police, which can extract taxes. That's a non-ancestral
idiom which dates back to the invention of sedentary agriculture and
extractible surpluses; humanity is still struggling to deal with it.}

{
 There are corporations in which the flow of money is controlled by
centralized management, a non-ancestral idiom dating back to the
invention of large-scale trade and professional specialization.}

{
 And in a world with large populations and close contact, memes
evolve far more virulent than the average case of the ancestral
environment; memes that wield threats of damnation, promises of heaven,
and professional priest classes to transmit them.}

{
 But by and large, the answer to the question
``How do large institutions
survive?'' is ``They
don't!'' The vast majority of large
modern-day institutions---some of them extremely vital to the
functioning of our complex civilization---simply \textit{fail to exist
in the first place.}}

{
 I first realized this as a result of grasping how Science gets
funded: namely, \textit{not} by individual donations.}

{
 Science traditionally gets funded by governments, corporations,
and large foundations. I've had the opportunity to
discover firsthand that it's \textit{amazingly}
difficult to raise money for Science from individuals. Not unless
it's science about a disease with gruesome victims, and
maybe not even then.}

{
 Why? People are, in fact, prosocial; they give money to, say,
puppy pounds. Science is one of the great social interests, and people
are even widely aware of this---why not Science, then?}

{
 Any \textit{particular} science project---say, studying the
genetics of trypanotolerance in cattle---is not a good
\textit{emotional fit} for individual charity. Science has a long time
horizon that requires continual support. The interim or even final
press releases may not sound all that emotionally arousing. You
can't volunteer; it's a job for
specialists. Being shown a picture of the scientist
you're supporting at or somewhat below the market price
for their salary lacks the impact of being shown the wide-eyed puppy
that you helped usher to a new home. You don't get the
immediate feedback and the sense of immediate accomplishment
that's required to keep an individual \textit{spending
their own money.}}

{
 Ironically, I finally realized this, not from my own work, but
from thinking ``Why don't Seth
Roberts's readers come together to support experimental
tests of Roberts's hypothesis about obesity? Why
aren't individual philanthropists paying to test
Bussard's polywell fusor?'' These are
examples of \textit{obviously} ridiculously underfunded science, with
applications (if true) that would be relevant to many, many
individuals. That was when it occurred to me that, in full generality,
Science is not a good emotional fit for people spending their own
money.}

{
 In fact \textit{very few things are}, with the individuals we have
now. It seems to me that this is key to understanding how the world
works the way it does---why so many individual interests are poorly
protected---why 200 million adult Americans have such tremendous
trouble supervising the 535 members of Congress, for example.}

{
 So how does Science actually get funded? By governments that think
they ought to spend some amount of money on Science, with legislatures
or executives deciding to do so---it's not quite their
\textit{own} money they're spending. Sufficiently large
corporations decide to throw some amount of money at blue-sky R\&D.
Large grassroots organizations built around affective death spirals may
look at science that suits their ideals. Large private foundations,
based on money block-allocated by wealthy individuals to their
reputations, spend money on Science that promises to sound very
charitable, sort of like allocating money to orchestras or modern art.
And then the individual scientists (or individual scientific
task-forces) fight it out for control of that pre-allocated money
supply, given into the hands of grant committee members who seem like
the sort of people who ought to be judging scientists.}

{
 You rarely see a scientific project making a \textit{direct} bid
for some portion of society's resource flow; rather, it
first gets allocated to Science, and then scientists fight over who
actually gets it. Even the exceptions to this rule are more likely to
be driven by politicians (moonshot) or military purposes (Manhattan
project) than by the appeal of scientists to the public.}

{
 Now I'm sure that if the general public were in
the habit of funding particular science by individual donations, a
whole lotta money would be wasted on e.g. quantum gibberish---assuming
that the general public somehow acquired the habit of funding science
without changing any other facts about the people or the society.}

{
 But it's still an interesting point that Science
manages to survive not because it is in our collective individual
interest to see Science get done, but rather, because Science has
fastened itself as a parasite onto the few forms of large organization
that can exist in our world. There are plenty of other projects that
simply fail to exist in the first place.}

{
 It seems to me that modern humanity manages to put forth very
little in the way of coordinated effort to serve collective individual
interests. It's just too non-ancestral a problem when
you scale to more than 50 people. There are only big taxers, big
traders, supermemes, occasional individuals of great power; and a few
other organizations, like Science, that can fasten parasitically onto
them.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Money: The Unit of Caring}

{
 Steve Omohundro has suggested a folk theorem to the effect that,
within the interior of any approximately rational self-modifying agent,
the marginal benefit of investing additional resources in anything
ought to be about equal. Or, to put it a bit more exactly, shifting a
unit of resource between any two tasks should produce no increase in
expected utility, relative to the agent's utility
function and its probabilistic expectations about its own algorithms. }

{
 This resource balance principle implies that---over a very wide
range of approximately rational systems, including even the interior of
a self-modifying mind---there will exist some common currency of
expected utilons, by which everything worth doing can be measured.}

{
 In our society, this common currency of expected utilons is called
``money.'' It is the measure of how
much society cares about something.}

{
 This is a brutal yet obvious point, which many are motivated to
deny.}

{
 With this audience, I hope, I can simply state it and move on.
It's not as if you thought
``society'' was intelligent,
benevolent, and sane up until this point, right?}

{
 I say this to make a certain point held in common across many good
causes. Any charitable institution you've ever had a
kind word for, certainly \textit{wishes} you would appreciate this
point, whether or not they've ever said anything out
loud. For I have listened to others in the nonprofit world, and I know
that I am not speaking only for myself here \ldots}

{
 Many people, when they see something that they think is worth
doing, would like to volunteer a few hours of spare time, or maybe mail
in a five-year-old laptop and some canned goods, or walk in a march
somewhere, but at any rate, not spend \textit{money.}}

{
 Believe me, I understand the feeling. Every time I spend money I
feel like I'm losing hit points. That's
the problem with having a unified quantity describing your net worth:
Seeing that number go down is not a pleasant feeling, even though it
has to fluctuate in the ordinary course of your existence. There ought
to be a fun-theoretic principle against it.}

{
 But, well \ldots}

{
 There \textit{is} this very, very old puzzle/observation in
economics about the lawyer who spends an hour volunteering at the soup
kitchen, instead of working an extra hour and donating the money to
hire someone to work for five hours at the soup kitchen.}

{
 There's this thing called
``Ricardo's Law of Comparative
Advantage.'' There's this idea called
``professional specialization.''
There's this notion of ``economies of
scale.'' There's this concept of
``gains from trade.'' The whole
reason why we have money is to realize the \textit{tremendous} gains
possible from each of us doing what we do \textit{best}.}

{
 This is what grownups do. This is what you do when you want
something to actually get \textit{done.} You use \textit{money} to
employ \textit{full-time specialists.}}

{
 Yes, people are sometimes limited in their ability to trade time
for money (underemployed), so that it is better for them if they can
directly donate that which they would usually trade for money.
\textit{If} the soup kitchen \textit{needed} a lawyer, and the lawyer
donated a \textit{large contiguous high-priority} block of lawyering,
then \textit{that} sort of volunteering makes
sense---that's the same \textit{specialized} capability
the lawyer ordinarily trades for money. But
``volunteering'' just one hour of
legal work, constantly delayed, spread across three weeks in casual
minutes between other jobs? This is not the way something gets done
\textit{when anyone actually cares about it}, or to state it
near-equivalently, \textit{when money is involved}.}

{
 To the extent that individuals fail to grasp this principle on a
\textit{gut level}, they may think that the use of money is somehow
\textit{optional} in the pursuit of things that merely seem
\textit{morally} desirable---as opposed to tasks like feeding
ourselves, whose desirability seems to be treated oddly differently.
This factor may be sufficient \textit{by itself} to prevent us from
pursuing our collective common interest in groups larger than 40
people.}

{
 Economies of trade and professional specialization are not just
vaguely good yet unnatural-sounding ideas, \textit{they are the only
way that anything ever gets done in this world.} Money is not pieces of
paper, it is the \textit{common currency of caring}.}

{
 Hence the old saying: ``Money makes the world go
`round, love barely keeps it from blowing
up.''}

{
 Now, we do have the problem of akrasia---of not being able to do
what we've decided to do---which is a part of the art
of rationality that I hope someone else will develop; I specialize more
in the impossible questions business. And yes, spending money is more
painful than volunteering, because you can see the bank account number
go down, whereas the remaining hours of our span are not visibly
numbered. But when it comes time to feed yourself, do you think,
``Hm, maybe I should try raising my own cattle,
that's less painful than spending money on
beef?'' Not everything can get done \textit{without}
invoking Ricardo's Law; and on the other end of that
trade are people who feel just the same pain at the thought of having
less money.}

{
 It does seem to me offhand that there ought to be things doable to
diminish the pain of losing hit points, and to increase the felt
strength of the connection from donating money to ``I
did a good thing!'' Some of that I am trying to
accomplish right now, by emphasizing the true nature and power of
money; and by inveighing against the poisonous meme saying that someone
who gives mere money must not care enough to get personally involved.
This is a mere reflection of a mind that doesn't
understand the post-hunter-gatherer concept of a market economy. The
act of donating money is not the momentary act of writing the check; it
is the act of every hour you spent to earn the money to write that
check---just as though you worked at the charity itself \textit{in your
professional capacity}, at maximum, grownup efficiency.}

{
 If the lawyer needs to work an hour at the soup kitchen to keep
themselves motivated and remind themselves why they're
doing what they're doing,
\textit{that's fine.} But they should \textit{also} be
donating some of the hours they worked at the office, because that is
the power of professional specialization. One might consider the check
as buying the right to volunteer at the soup kitchen, or validating the
time spent at the soup kitchen. More on this later.}

{
 To a first approximation, money is the unit of caring up to a
positive scalar factor---the unit of relative caring. Some people are
frugal and spend less money on \textit{everything}; but if you would,
in fact, spend \$5 on a burrito, then whatever you will not spend \$5
on, you care about \textit{less than} you care about the burrito. If
you don't spend two months' salary on a
diamond ring, it doesn't mean you don't
love your Significant Other. (``De Beers:
It's Just A Rock.'') But conversely,
if you're \textit{always} reluctant to spend
\textit{any} money on your Significant Other, and yet seem to have no
emotional problems with spending \$1,000 on a flat-screen TV, then yes,
this \textit{does} say something about your relative values.}

{
 Yes, frugality is a virtue. Yes, spending money hurts. But in the
end, if you are never willing to spend any units of caring, it means
you don't care.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Purchase Fuzzies and Utilons Separately}

{
 Previously:}

{
 There is this very, very old puzzle/observation in economics about
the lawyer who spends an hour volunteering at the soup kitchen, instead
of working an extra hour and donating the money to hire someone \ldots}

{
 If the lawyer needs to work an hour at the soup kitchen to keep
themselves motivated and remind themselves why they're
doing what they're doing,
\textit{that's fine.} But they should \textit{also} be
donating some of the hours they worked at the office, because that is
the power of professional specialization. One might consider the check
as buying the right to volunteer at the soup kitchen, or validating the
time spent at the soup kitchen. More on this later.}

{
 I hold open doors for little old ladies. I can't
actually remember the last time this happened literally (though
I'm sure it has, sometime in the last year or so). But
within the last month, say, I was out on a walk and discovered a
station wagon parked in a driveway with its trunk completely open,
giving full access to the car's interior. I looked in
to see if there were packages being taken out, but this was not so. I
looked around to see if anyone was doing anything with the car. And
finally I went up to the house and knocked, then rang the bell. And
yes, the trunk had been accidentally left open.}

{
 Under other circumstances, this would be a simple act of altruism,
which might signify true concern for another's welfare,
or fear of guilt for inaction, or a desire to signal trustworthiness to
oneself or others, or finding altruism pleasurable. I think that these
are all perfectly legitimate motives, by the way; I might give bonus
points for the first, but I wouldn't deduct any penalty
points for the others. Just so long as people get helped.}

{
 But in my own case, since I already work in the nonprofit sector,
the further question arises as to whether I could have better employed
the same sixty seconds in a more \textit{specialized} way, to bring
greater benefit to others. That is: can I really defend this as the
\textit{best} use of my time, given the other things I claim to
believe?}

{
 The obvious defense---or, perhaps, obvious rationalization---is
that an act of altruism like this one acts as a willpower restorer,
much more efficiently than, say, listening to music. I also mistrust my
ability to be an altruist \textit{only} in theory; I suspect that if I
walk past problems, my altruism will start to fade.
I've never pushed that far enough to test it; it
doesn't seem worth the risk.}

{
 But if that's the defense, then my act
can't be defended as a good deed, can it? For these are
self-directed benefits that I list.}

{
 Well---who said that I \textit{was} defending the act as a
selfless good deed? It's a \textit{selfish} good deed.
If it restores my willpower, or if it keeps me altruistic, then there
are indirect other-directed benefits from that (or so I believe). You
could, of course, reply that you don't trust selfish
acts that are supposed to be other-benefiting as an
``ulterior motive''; but then I
could just as easily respond that, by the same principle, you should
just look directly at the original good deed rather than \textit{its}
supposed ulterior motive.}

{
 Can I get away with that? That is, can I really get away with
calling it a ``selfish good deed,''
and still derive willpower restoration therefrom, rather than feeling
guilt about its being selfish? Apparently I can. I'm
surprised it works out that way, but it does. So long as I knock to
tell them about the open trunk, and so long as the one says
``Thank you!,'' my brain feels like
it's done its wonderful good deed for the day.}

{
 Your mileage may vary, of course. The problem with trying to work
out an art of willpower restoration is that different things seem to
work for different people. (That is: We're probing
around on the level of surface phenomena without understanding the
deeper rules that would also predict the variations.)}

{
 But if you find that you are like me in this aspect---that selfish
good deeds still work---then I recommend that you \textit{purchase warm
fuzzies and utilons separately.} Not at the same time. Trying to do
both at the same time just means that neither ends up done well. If
status matters to you, purchase status separately too!}

{
 If I had to give advice to some new-minted billionaire entering
the realm of charity, my advice would go something like this:}

{
 To purchase warm fuzzies, find some hard-working but
poverty-stricken woman who's about to drop out of state
college after her husband's hours were cut back, and
personally, but anonymously, give her a cashier's check
for \$10,000. Repeat as desired.}

{
 To purchase status among your friends, donate \$100,000 to the
current sexiest X-Prize, or whatever other charity seems to offer the
most stylishness for the least price. Make a big deal out of it, show
up for their press events, and brag about it for the next five years.}

{
 Then---with absolute cold-blooded calculation---without scope
insensitivity or ambiguity aversion{}---without concern for status or
warm fuzzies---figuring out some common scheme for converting outcomes
to utilons, and trying to express uncertainty in percentage
probabilities---find the charity that offers the greatest expected
utilons per dollar. Donate up to however much money you wanted to give
to charity, until their marginal efficiency drops below that of the
next charity on the list.}

{
 I would furthermore advise the billionaire that what they spend on
utilons should be at least, say, 20 times what they spend on warm
fuzzies---5\% overhead on keeping yourself altruistic seems reasonable,
and I, your dispassionate judge, would have no trouble
\textit{validating} the warm fuzzies against a multiplier that large.
Save that the original fuzzy act really should be helpful rather than
actively harmful.}

{
 (Purchasing \textit{status} seems to me essentially unrelated to
altruism. If giving money to the X-Prize gets you more awe from your
friends than an equivalently priced speedboat, then
there's really no reason to buy the speedboat. Just put
the money under the ``impressing
friends'' column, and be aware that this is not the
``altruism'' column.)}

{
 But the main lesson is that all three of these things---warm
fuzzies, status, and expected utilons---can be bought \textit{far} more
efficiently when you buy \textit{separately}, optimizing for only one
thing at a time. Writing a check for \$10,000,000 to a breast-cancer
charity---while far more laudable than spending the same \$10,000,000
on, I don't know, parties or
something---won't give you the concentrated euphoria of
being present in person when you turn a single human's
life around, probably not anywhere \textit{close}. It
won't give you as much to talk about at parties as
donating to something sexy like an X-Prize---maybe a short nod from the
other rich. And if you threw away all concern for warm fuzzies and
status, there are probably at least a \textit{thousand} underserved
existing charities that could produce \textit{orders of magnitude} more
utilons with ten million dollars. Trying to optimize for all three
criteria in one go only ensures that none of them end up optimized very
well---just vague pushes along all three dimensions.}

{
 Of course, if you're not a millionaire or even a
billionaire---then you can't be quite as
\textit{efficient} about things, can't so easily
purchase in bulk. But I would still say---for warm fuzzies, find a
relatively \textit{cheap} charity with bright, vivid, ideally in-person
and direct beneficiaries. Volunteer at a soup kitchen. Or just get your
warm fuzzies from holding open doors for little old ladies. Let that be
\textit{validated} by your other efforts to purchase utilons, but
don't \textit{confuse} it with purchasing utilons.
Status is probably cheaper to purchase by buying nice clothes.}

{
 And when it comes to purchasing expected utilons---then, of
course, shut up and multiply.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Bystander Apathy}

{
 The bystander effect, also known as bystander apathy, is that
larger groups are less likely to act in emergencies---not just
individually, but collectively. Put an experimental subject alone in a
room and let smoke start coming up from under the door. Seventy-five
percent of the subjects will leave to report it. Now put \textit{three}
subjects in the room---real subjects, none of whom know
what's going on. On only 38\% of the occasions will
\textit{anyone} report the smoke. Put the subject with two confederates
who ignore the smoke, and they'll only report it 10\%
on the time---even staying in the room until it becomes
hazy.\textsuperscript{1} }

{
 On the standard model, the two primary drivers of bystander apathy
are:}

{
 \textit{Diffusion of responsibility}{}---everyone hopes that
someone else will be first to step up and incur any costs of acting.
When no one does act, being part of a crowd provides an excuse and
reduces the chance of being held personally responsible for the
results.}

{
 \textit{Pluralistic ignorance}{}---people try to \textit{appear}
calm while looking for cues, and see \ldots that the others appear
calm.}

{
 Cialdini:\textsuperscript{2}}

{
 Very often an emergency is not obviously an emergency. Is the man
lying in the alley a heart-attack victim or a drunk sleeping one off?
\ldots In times of such uncertainty, the natural tendency is to look
around at the actions of others for clues. We can learn from the way
the other witnesses are reacting whether the event is or is not an
emergency. What is easy to forget, though, is that everybody else
observing the event is likely to be looking for social evidence, too.
Because we all prefer to appear poised and unflustered among others, we
are likely to search for that evidence placidly, with brief,
camouflaged glances at those around us. Therefore everyone is likely to
see everyone else looking unruffled and failing to act.}

{
 Cialdini suggests that if you're ever in emergency
need of help, you point to \textit{one single} bystander and ask them
for help---making it very clear to whom you're
referring. Remember that the \textit{total} group, combined, may have
less chance of helping than one individual.}

{
 I've mused a bit on the evolutionary psychology of
the bystander effect. Suppose that in the ancestral environment, most
people in your band were likely to be at least a little related to
you---enough to be worth saving, if you were the only one who could do
it. But if there are two others present, then the \textit{first} person
to act incurs a cost, while the other two both reap the
\textit{genetic} benefit of a partial relative being saved. Could there
have been an arms race for who waited the longest?}

{
 As far as I've followed this line of speculation,
it doesn't seem to be a good explanation---at the point
where the whole group is failing to act, a gene that helps immediately
ought to be able to invade, I would think. The experimental result is
not a long wait before helping, but simply failure to help: if
it's a genetic benefit to help when
you're the only person who can do it (as \textit{does}
happen in the experiments) then the group equilibrium should not be
\textit{no} one helping (as happens in the experiments).}

{
 So I don't think an arms race of delay is a
plausible evolutionary explanation. More likely, I think, is that
we're looking at a nonancestral problem. If the
experimental subjects actually \textit{know} the apparent victim, the
chances of helping go way up (i.e., we're not looking
at the correlate of helping an actual fellow band member). If I recall
correctly, if the experimental subjects know each \textit{other}, the
chances of action also go up.}

{
 Nervousness about public action may also play a role. If Robin
Hanson is right about the evolutionary role of
``choking,'' then being
\textit{first} to act in an emergency might also be taken as a
dangerous bid for high status. (Come to think, I can't
actually recall seeing \textit{shyness} discussed in analyses of the
bystander effect, but that's probably just my poor
memory.)}

{
 Can the bystander effect be explained primarily by diffusion of
moral responsibility? We could be cynical and suggest that people are
mostly interested in \textit{not being blamed} for not helping, rather
than having any positive desire to help---that they mainly wish to
escape antiheroism and possible retribution. Something like this may
well be a contributor, but two observations that mitigate against it
are (a) the experimental subjects did not report smoke coming in from
under the door, even though it could well have represented a strictly
selfish threat and (b) telling people about the bystander effect
reduces the bystander effect, even though they're no
more likely to be held publicly responsible thereby.}

{
 In fact, the bystander effect is one of the main cases I recall
offhand where telling people about a bias actually seems able to
strongly reduce it---maybe because the appropriate way to compensate is
so obvious, and it's not easy to
\textit{over}compensate (as when you're trying to e.g.
adjust your calibration). So we should be careful not to be too cynical
about the implications of the bystander effect and diffusion of
responsibility, if we interpret individual action in terms of a cold,
calculated attempt to avoid public censure. People seem at least to
sometimes hold \textit{themselves} responsible, once they realize
they're the only ones who know enough about the
bystander effect to be likely to act.}

{
 Though I wonder what happens if you know that
you're part of a crowd where \textit{everyone} has been
told about the bystander effect \ldots}

{\centering
 \ ~
\par}

{\centering
 *
\par}


\bigskip

{
 1. Bibb Latané and John M. Darley, ``Bystander
`Apathy,'''
\textit{American Scientist} 57, no. 2 (1969): 244--268,
http://www.jstor.org/stable/27828530.}

{
 2. Cialdini, \textit{Influence}.}

\mysection{Collective Apathy and the Internet}

{
 In the last essay I covered the bystander effect, a.k.a. bystander
apathy: given a fixed problem situation, a \textit{group} of bystanders
is actually \textit{less} likely to act than a \textit{single}
bystander. The standard explanation for this result is in terms of
pluralistic ignorance (if it's not clear whether the
situation is an emergency, each person tries to \textit{look} calm
while darting their eyes at the other bystanders, and sees other people
\textit{looking} calm) and diffusion of responsibility (everyone hopes
that someone else will be first to act; being part of a crowd
diminishes the individual pressure to the point where no one acts). }

{
 Which may be a symptom of our hunter-gatherer coordination
mechanisms being defeated by modern conditions. You
didn't usually form task-forces with strangers back in
the ancestral environment; it was mostly people you knew. And in fact,
when all the subjects know each other, the bystander effect
diminishes.}

{
 So I know this is an amazing and revolutionary observation, and I
hope that I don't kill any readers outright from shock
by saying this: but people seem to have a hard time reacting
constructively to problems encountered over the Internet.}

{
 Perhaps because our innate coordination instincts are not tuned
for:}

{
 Being part of a group of strangers. (When all subjects know each
other, the bystander effect diminishes.)}

{
 Being part of a group of unknown size, of strangers of unknown
identity.}

{
 Not being in physical contact (or visual contact); not being able
to exchange meaningful glances.}

{
 Not communicating in real time.}

{
 Not being much beholden to each other for other forms of help; not
being codependent on the group you're in.}

{
 Being shielded from reputational damage, or the fear of
reputational damage, by your own apparent anonymity; no one is visibly
looking at you, before whom your reputation might suffer from
inaction.}

{
 Being part of a large collective of other inactives; no one will
single out you to blame.}

{
 Not hearing a voiced plea for help.}

{
 Et cetera. I don't have a brilliant solution to
this problem. But it's the sort of thing that I would
wish for potential dot-com cofounders to ponder explicitly, rather than
wondering how to throw sheep on Facebook. (Yes, I'm
looking at \textit{you}, Hacker News.) There are online activism web
apps, but they tend to be along the lines of \textit{sign this
petition! yay, you signed something!} rather than \textit{how can we
counteract the bystander effect, restore motivation, and work with
native group-coordination instincts, over the Internet?}}

{
 Some of the things that come to mind:}

{
 Put a video of someone asking for help online.}

{
 Put up names and photos or even brief videos if available of the
\textit{first} people who helped (or have some reddit-ish priority
algorithm that depends on a combination of amount-helped and recency).}

{
 Give helpers a video thank-you from the founder of the cause that
they can put up on their ``people I've
helped'' page, which with enough standardization
could be partially or wholly assembled automatically and easily
embedded in their home webpage or Facebook account.}

{
 Find a \textit{non-annoying} idiom for ``Tell a
friend about cause X''; allow referrer link codes;
then show people how many others they've evangelized
(how many people who initially got here using referrer code X actually
contributed or took some other action).}

{
 (All of the above applies not just to donations, but to
open-source projects to which people have contributed code. Or if
people really do want nothing but signatures on a petition, then for
signatures. There are ways to help besides money---even though money is
usually the most effective. The main thing is that the form of help has
to be verifiable online.)}

{
 Make it easier for people to offer monetary bounties on subtasks
whose performance is verifiable.}

{
 But mostly I just hand you an open, unsolved problem: make it
possible/easier for groups of strangers to coalesce into an effective
task force over the Internet, in defiance of the usual failure modes
and the default reasons why this is a non-ancestral problem. Think of
that old statistic about Wikipedia representing 1/2,000 of the time
spent in the US alone on watching television. There's
quite a lot of fuel out there, if there were only such a thing as an
effective engine \ldots}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Incremental Progress and the Valley}

{
 Rationality is systematized winning. }

{
 ``But,'' you protest,
``the reasonable person
\textit{doesn't} always win!''}

{
 What do you mean by this? Do you mean that every week or two,
someone who bought a lottery ticket with negative expected value wins
the lottery and becomes much richer than you? That is not a
\textit{systematic} loss; it is selective reporting by the media. From
a statistical standpoint, lottery winners don't
exist---you would never encounter one in your lifetime, if it
weren't for the selective reporting.}

{
 Even perfectly rational agents can lose. They just
can't \textit{know in advance} that
they'll lose. They can't \textit{expect
to underperform} any other performable strategy, or they would simply
perform it.}

{
 ``No,'' you say,
``I'm talking about how startup
founders strike it rich by believing in themselves and their ideas more
strongly than any reasonable person would. I'm talking
about how religious people are happier---''}

{
 Ah. Well, here's the thing: An
\textit{incremental} step in the direction of rationality, if the
result is still irrational in other ways, does not have to yield
\textit{incrementally} more winning.}

{
 The optimality theorems that we have for probability theory and
decision theory are for \textit{perfect} probability theory and
decision theory. There is no companion theorem which says that,
starting from some flawed initial form, every \textit{incremental}
modification of the algorithm that takes the structure closer to the
ideal must yield an \textit{incremental} improvement in performance.
This has not yet been proven, because it is not, in fact, true.}

{
 ``So,'' you say,
``what point is there then in striving to be more
rational? We won't reach the perfect ideal. So we have
no guarantee that our steps forward are helping.''}

{
 You have no guarantee that a step \textit{backward} will help you
win, either. Guarantees don't exist in the world of
flesh; but, contrary to popular misconceptions, judgment under
\textit{uncertainty} is what rationality is all about.}

{
 ``But we have several cases where, based on
either vaguely plausible-sounding reasoning, or survey data, it looks
like an incremental step forward in rationality is going to make us
worse off. If it's really all about winning---if you
have something to protect more important than any ritual of
cognition---then \textit{why} take that step?''}

{
 Ah, and \textit{now} we come to the meat of it.}

{
 I can't necessarily answer for everyone, but
\ldots}

{
 My first reason is that, on a professional basis, I deal with
deeply confused problems that make huge demands on precision of
thought. One small mistake can lead you astray for years, and there are
worse penalties waiting in the wings. An unimproved level of
performance isn't \textit{enough}; my choice is to try
to do better, or give up and go home.}

{
 ``But that's just \textit{you}.
Not all of us lead that kind of life. What if you're
just trying some ordinary human task like an Internet
startup?''}

{
 My second reason is that I am trying to push some aspects of my
art further than I have seen done. I don't
\textit{know} where these improvements lead. The loss of failing to
take a step forward is not that \textit{one step}. It is all the
\textit{other} steps forward you could have taken, beyond that point.
Robin Hanson has a saying: The problem with slipping on the stairs is
not falling the height of the first step; it is that falling one step
leads to falling another step. In the same way, refusing to climb one
step up forfeits not the height of that step but the height of the
staircase.}

{
 ``But again---that's just you.
Not all of us are trying to push the art into uncharted
territory.''}

{
 My third reason is that once I realize I have been deceived, I
can't just shut my eyes and pretend I
haven't seen it. I have \textit{already taken} that
step forward; what use to deny it to myself? I couldn't
believe in God if I tried, any more than I could believe the sky above
me was green while looking straight at it. If you \textit{know}
everything you need to know in order to know that you are better off
deceiving yourself, it's much too late to deceive
yourself.}

{
 ``But that realization is \textit{unusual}; other
people have an easier time of doublethink because they
don't realize it's impossible.
\textit{You} go around trying to actively sponsor the collapse of
doublethink. \textit{You}, from a higher vantage point, may know enough
to expect that this will make them unhappier. So is this out of a
sadistic desire to hurt your readers, or what?''}

{
 Then I finally reply that my experience so far---even in this
realm of merely human possibility---\textit{does} seem to indicate
that, once you sort yourself out a bit and you aren't
doing \textit{quite} so many other things wrong, striving for more
rationality actually \textit{will} make you better off. The long road
leads out of the valley and higher than before, even in the human
lands.}

{
 The more I know about some particular facet of the Art, the more I
can see this is so. As I've previously remarked, my
essays may be unreflective of what a true martial art of rationality
would be like, because I have only focused on answering confusing
questions---not fighting akrasia, coordinating groups, or being happy.
In the field of answering confusing questions---the area where I have
most intensely practiced the Art---it now seems \textit{massively}
obvious that anyone who thought they were better off
``staying optimistic about solving the
problem'' would get stomped into the \textit{ground}.
By a \textit{casual student.}}

{
 When it comes to keeping motivated, or being happy, I
can't guarantee that someone who loses their illusions
will be better off---because my knowledge of these facets of
rationality is still crude. If these parts of the Art have been
developed systematically, I do not know of it. But even here I have
gone to some considerable pains to dispel half-rational half-mistaken
ideas that could get in a beginner's way, like the idea
that rationality opposes feeling, or the idea that rationality opposes
value, or the idea that sophisticated thinkers should be angsty and
cynical.}

{
 And if, as I hope, someone goes on to develop the art of fighting
akrasia or achieving mental well-being as thoroughly as I have
developed the art of answering impossible questions, I do fully expect
that those who wrap themselves in their illusions will not
\textit{begin} to compete. Meanwhile---others may do better than I, if
happiness is their dearest desire, for I myself have invested little
effort here.}

{
 I find it hard to believe that the \textit{optimally} motivated
individual, the \textit{strongest} entrepreneur a human being can
become, is still wrapped up in a blanket of comforting overconfidence.
I think they've probably thrown that blanket out the
window and organized their mind a little \textit{differently.} I find
it hard to believe that the happiest we can possibly live, even in the
realms of human possibility, involves a tiny awareness lurking in the
corner of your mind that it's all a lie.
I'd rather stake my hopes on neurofeedback or Zen
meditation, though I've tried neither.}

{
 But it cannot be denied that this is a very real issue in very
real life. Consider this pair of comments from \textit{Less Wrong}:}

{
 I'll be honest---my life has taken a sharp
downturn since I deconverted. My theist girlfriend, with whom I was
very much in love, couldn't deal with this change in
me, and after six months of painful vacillation, she left me for a
co-worker. That was another six months ago, and I have been
heartbroken, miserable, unfocused, and \textit{extremely} ineffective
since.}

{
 Perhaps this is an example of the valley of bad rationality of
which PhilGoetz spoke, but I still hold my current situation higher in
my preference ranking than happiness with false beliefs.}

{
 And:}

{
 My empathies: that happened to me about 6 years ago (though
thankfully without as much visible vacillation).}

{
 My sister, who had some Cognitive Behaviour Therapy training,
reminded me that relationships are forming and breaking all the time,
and given I wasn't unattractive and
hadn't retreated into monastic seclusion, it
wasn't rational to think I'd be alone
for the rest of my life (she turned out to be right). That was helpful
at the times when my feelings hadn't completely got the
better of me.}

{
 So---in practice, in real life, in sober fact---those first steps
can, in fact, be painful. And then things can, in fact, get better. And
there is, in fact, no \textit{guarantee} that you'll
end up higher than before. Even if in principle the path must go
further, there is no guarantee that any given person will get that
far.}

{
 If you don't \textit{prefer} truth to happiness
with false beliefs \ldots}

{
 Well \ldots \textit{and} if you are not doing anything especially
precarious or confusing \ldots and if you are not buying lottery tickets
\ldots and if you're already signed up for cryonics, a
sudden ultra-high-stakes confusing acid test of rationality that
illustrates the Black Swan quality of trying to bet on ignorance
\textit{in} ignorance \ldots}

{
 Then it's not \textit{guaranteed} that taking all
the incremental steps toward rationality that you can find will leave
you better off. But the vaguely plausible-sounding arguments against
losing your illusions generally \textit{do} consider just one single
step, without postulating any further steps, without suggesting any
attempt to regain everything that was lost and go it one better. Even
the surveys are comparing the average religious person to the average
atheist, not the most advanced theologians to the most advanced
rationalists.}

{
 But if you don't care about the
truth---\textit{and} you have nothing to protect---\textit{and}
you're not attracted to the thought of pushing your art
as far as it can go---\textit{and} your current life seems to be going
fine---\textit{and} you have a sense that your mental well-being
depends on illusions you'd rather not think about---}

{
 Then you're probably not reading this. But if you
are, then, I guess \ldots well \ldots (a) sign up for cryonics, and then
(b) \textit{stop reading Less Wrong before your illusions collapse! RUN
AWAY!}}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Bayesians vs. Barbarians}

{
 Previously:}

{
 Let's say we have two groups of soldiers. In group
1, the privates are ignorant of tactics and strategy; only the
sergeants know anything about tactics and only the officers know
anything about strategy. In group 2, everyone at all levels knows all
about tactics and strategy.}

{
 Should we expect group 1 to defeat group 2, because group 1 will
follow orders, while everyone in group 2 comes up with \textit{better
idea}s than whatever orders they were given?}

{
 In this case I have to question how much group 2 really
understands about military theory, because it is an \textit{elementary}
proposition that an uncoordinated mob gets slaughtered.}

{
 Suppose that a country of rationalists is attacked by a country of
Evil Barbarians who know nothing of probability theory or decision
theory.}

{
 Now there's a certain viewpoint on
``rationality'' or
``rationalism'' which would say
something like this:}

{
 ``Obviously, the rationalists will lose. The
Barbarians believe in an afterlife where they'll be
rewarded for courage; so they'll throw themselves into
battle without hesitation or remorse. Thanks to their affective death
spirals around their Cause and Great Leader Bob, their warriors will
obey orders, and their citizens at home will produce enthusiastically
and at full capacity for the war; anyone caught skimming or holding
back will be burned at the stake in accordance with Barbarian
tradition. They'll believe in each
other's goodness and hate the enemy more strongly than
any sane person would, binding themselves into a tight group.
Meanwhile, the rationalists will realize that there's
no conceivable reward to be had from dying in battle;
they'll wish that others would fight, but not want to
fight themselves. Even if they can find soldiers, their civilians
won't be as cooperative: So long as any \textit{one}
sausage almost certainly doesn't lead to the collapse
of the war effort, they'll want to keep that sausage
for themselves, and so not contribute as much as they could. No matter
how refined, elegant, civilized, productive, and nonviolent their
culture was to start with, they won't be able to resist
the Barbarian invasion; sane discussion is no match for a frothing
lunatic armed with a gun. In the end, the Barbarians will win because
they \textit{want} to fight, they \textit{want} to hurt the
rationalists, they \textit{want} to conquer and their whole society is
united around conquest; they care about that more than any sane person
would.''}

{
 War is not fun. As many, many people have found since the dawn of
recorded history, as many, many people have found before the dawn of
recorded history, as some community somewhere is finding out right now
in some sad little country whose internal agonies don't
even make the front pages any more.}

{
 War is not fun. \textit{Losing} a war is even less fun. And it was
said since the ancient times: ``If thou would have
peace, prepare for war.'' Your opponents
don't have to believe that you'll
\textit{win}, that you'll conquer; but they have to
believe you'll put up enough of a fight to make it not
worth their while.}

{
 You perceive, then, that if it were genuinely the lot of
``rationalists'' to always lose in
war, that I could not in good conscience advocate the widespread public
adoption of ``rationality.''}

{
 This is probably the dirtiest topic I've discussed
or plan to discuss here. War is not clean. Current high-tech
militaries---by this I mean the US military---are unique in the
overwhelmingly superior force they can bring to bear on opponents,
which allows for a historically extraordinary degree of concern about
enemy casualties and civilian casualties.}

{
 Winning in war has not always meant tossing aside \textit{all}
morality. Wars have been won without using torture. The unfunness of
war does not imply, say, that questioning the President is unpatriotic.
We're used to
``war'' being exploited as an excuse
for bad behavior, because in recent US history that pretty much
\textit{is} exactly what it's been used for \ldots}

{
 But reversed stupidity is not intelligence. And reversed evil is
not intelligence either. It remains true that \textit{real} wars cannot
be won by refined politeness. If
``rationalists''
can't prepare themselves for that mental shock, the
Barbarians really will win; and the
``rationalists'' \ldots I
don't want to say, ``deserve to
lose.'' But they will have failed that test of their
society's existence.}

{
 Let me start by disposing of the idea that, \textit{in principle},
ideal rational agents cannot fight a war, because each of them prefers
being a civilian to being a soldier.}

{
 As has already been discussed at some length, I one-box on
Newcomb's Problem.}

{
 Consistently, I do \textit{not} believe that if an election is
settled by 100,000 to 99,998 votes, that all of the voters were
irrational in expending effort to go to the polling place because
``my staying home would not have affected the
outcome.'' (Nor do I believe that if the election
came out 100,000 to 99,999, then 100,000 people were \textit{all},
individually, \textit{solely responsible} for the outcome.)}

{
 Consistently, I also hold that two rational AIs (that use my kind
of decision theory), even if they had completely different utility
functions and were designed by different creators, will cooperate on
the true Prisoner's Dilemma if they have common
knowledge of each other's source code. (Or even just
common knowledge of each other's \textit{rationality}
in the appropriate sense.)}

{
 Consistently, I believe that rational agents are capable of
coordinating on group projects whenever the (expected probabilistic)
outcome is better than it would be without such coordination. A society
of agents that use my kind of decision theory, and have common
knowledge of this fact, will end up at Pareto optima instead of Nash
equilibria. If all rational agents agree that they are better off
fighting than surrendering, they will fight the Barbarians rather than
surrender.}

{
 Imagine a community of self-modifying AIs who collectively prefer
fighting to surrender, but individually prefer being a civilian to
fighting. One solution is to run a lottery, unpredictable to any agent,
to select warriors. \textit{Before} the lottery is run, all the AIs
change their code, in advance, so that if selected they will fight as a
warrior in the most communally efficient possible way---even if it
means calmly marching into their own death.}

{
 (A reflectively consistent decision theory works the same way,
only without the self-modification.)}

{
 You reply: ``But in the real, human world, agents
are not perfectly rational, nor do they have common knowledge of each
other's source code. Cooperation in the
Prisoner's Dilemma requires certain conditions
according to your decision theory (which these margins are too small to
contain) and these conditions are not met in real
life.''}

{
 I reply: The pure, true Prisoner's Dilemma is
incredibly rare in real life. In real life you usually have knock-on
effects---what you do affects your reputation. In real life most people
care to some degree about what happens to other people. And in real
life you have an opportunity to set up incentive mechanisms.}

{
 And in real life, I \textit{do} think that a community of human
rationalists could manage to produce soldiers willing to die to defend
the community. So long as children aren't told in
school that ideal rationalists are supposed to defect against each
other in the Prisoner's Dilemma. Let it be widely
believed---and I do believe it, for exactly the same reason I one-box
on Newcomb's Problem---that if people decided as
individuals not to be soldiers or if soldiers decided to run away, then
that is the same as deciding for the Barbarians to win. By that same
theory whereby, if an election is won by 100,000 votes to 99,998 votes,
it does not make sense for every voter to say ``my
vote made no difference.'' Let it be said (for it is
true) that utility functions don't need to be
solipsistic, and that a rational agent can fight to the death if they
care enough about what they're protecting. Let them not
be told that rationalists should expect to lose reasonably.}

{
 If this is the culture and the mores of the rationalist society,
then, I think, \textit{ordinary human beings} in that society would
volunteer to be soldiers. That also seems to be built into human
beings, after all. You only need to ensure that the cultural training
\textit{does not get in the way.}}

{
 And if I'm wrong, and that doesn't
get you enough volunteers?}

{
 Then so long as people still prefer, on the whole, fighting to
surrender, they have an opportunity to set up incentive mechanisms, and
avert the True Prisoner's Dilemma.}

{
 You can have lotteries for who gets elected as a warrior. Sort of
like the example above with AIs changing their own code. Except that if
``be reflectively consistent; do that which you would
precommit to do'' is not sufficient motivation for
humans to obey the lottery, then \ldots}

{
 \ldots well, in advance of the lottery actually running, we can
perhaps all agree that it is a good idea to give the selectees drugs
that will induce extra courage, and shoot them if they run away. Even
considering that we ourselves might be selected in the lottery. Because
in \textit{advance} of the lottery, this is the general policy that
gives us the highest \textit{expectation} of survival.}

{
 \ldots like I said: Real wars = not fun, losing wars = less fun.}

{
 Let's be clear, by the way, that
I'm not endorsing the draft as practiced nowadays.
Those drafts are not collective attempts by a populace to move from a
Nash equilibrium to a Pareto optimum. Drafts are a tool of kings
playing games in need of toy soldiers. The Vietnam draftees who fled to
Canada, I hold to have been in the right. But a society that considers
itself too smart for kings does \textit{not} have to be too smart to
survive. Even if the Barbarian hordes are invading, and the Barbarians
do practice the draft.}

{
 Will rational soldiers obey orders? What if the commanding officer
makes a mistake?}

{
 Soldiers march. Everyone's feet hitting the ground
in the same rhythm. Even, perhaps, against their own inclinations,
since people left to themselves would walk all at separate paces.
Lasers made out of people. That's marching.}

{
 If it's possible to invent some method of group
decisionmaking that is \textit{superior} to the captain handing down
orders, then a company of rational soldiers might implement that
procedure. If there is no proven method better than a captain, then a
company of rational soldiers commit to obey the captain, even against
their own separate inclinations. And if human beings
aren't that rational \ldots then in advance of the
lottery, the general policy that gives you the highest personal
expectation of survival is to shoot soldiers who disobey orders. This
is not to say that those who fragged their own officers in Vietnam were
in the wrong; for they could have consistently held that they preferred
\textit{no one} to participate in the draft lottery.}

{
 But an uncoordinated mob gets slaughtered, and so the soldiers
need \textit{some} way of all doing the same thing at the same time in
the pursuit of the same goal, even though, left to their own devices,
they might march off in all directions. The orders may not come from a
captain like a superior tribal chief, but unified orders have to come
from \textit{somewhere.} A society whose soldiers are too clever to
obey orders is a society that is too clever to survive. Just like a
society whose people are too clever to \textit{be} soldiers. That is
why I say ``clever,'' which I often
use as a term of opprobrium, rather than
``rational.''}

{
 (Though I do think it's an important question as
to whether you can come up with a small-group coordination method that
really genuinely in practice works better than having a leader. The
more people can trust the group decision method---the more they can
believe that it really is superior to people going their own way---the
more coherently they can behave even in the absence of enforceable
penalties for disobedience.)}

{
 I say all this, even though I certainly don't
expect rationalists to take over a country any time soon, because I
think that what we believe about a society of ``people
like us'' has some reflection on what we think of
ourselves. If you believe that a society of people like you would be
too reasonable to survive in the long run \ldots that's
one sort of self-image. And it's a different sort of
self-image if you think that a society of people all like you could
fight the vicious Evil Barbarians and \textit{win}{}---not just by dint
of superior technology, but because your people care about each other
and about their collective society---and because they can face the
realities of war without losing themselves---and because they would
calculate the group-rational thing to do and make sure it got
done---and because there's nothing in the rules of
probability theory or decision theory that says you
can't sacrifice yourself for a cause---and because if
you really \textit{are} smarter than the Enemy and not just flattering
yourself about that, then you should be able to exploit the blind spots
that the Enemy does not allow itself to think about---and because no
matter how heavily the Enemy hypes itself up before battle, you think
that just maybe a coherent mind, undivided within itself, and perhaps
practicing something akin to meditation or self-hypnosis, can fight as
hard in practice as someone who theoretically believes
they've got seventy-two virgins waiting for them.}

{
 Then you'll expect more of yourself \textit{and
people like you operating in groups}; and then you can see yourself as
something more than a cultural dead end.}

{
 So look at it this way: Jeffreyssai probably
wouldn't give up against the Evil Barbarians if he were
fighting \textit{alone}. A whole \textit{army} of \textit{beisutsukai}
masters ought to be a force that \textit{no one} would mess with.
That's the motivating vision. The question is how,
exactly, that works.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Beware of Other{}-Optimizing}

{
 I've noticed a serious problem in which aspiring
rationalists vastly overestimate their ability to optimize other
people's lives. And I think I have some idea of how the
problem arises. }

{
 You read nineteen different webpages advising you about personal
improvement---productivity, dieting, saving money. And the writers all
sound bright and enthusiastic about Their Method, they tell tales of
how it worked for them and promise \textit{amazing} results \ldots}

{
 But most of the advice rings so false as to not even seem worth
considering. So you sigh, mournfully pondering the wild, childish
enthusiasm that people can seem to work up for just about anything, no
matter how silly. Pieces of advice \#4 and \#15 sound interesting, and
you try them, but \ldots they don't \ldots quite \ldots
well, it fails miserably. The advice was wrong, or you
couldn't do it, and either way you're
not any better off.}

{
 And then you read the twentieth piece of advice---or even more,
you discover a twentieth method that wasn't in any of
the pages---and STARS ABOVE IT ACTUALLY WORKS THIS TIME.}

{
 At long, long last you have discovered the \textit{real} way, the
\textit{right} way, the way that actually \textit{works.} And when
someone else gets into the sort of trouble you used to have---well,
this time you \textit{know} how to help them. You can save them all the
trouble of reading through nineteen useless pieces of advice and skip
directly to the correct answer. As an aspiring rationalist
you've already learned that most people
don't listen, and you usually don't
bother---but this person is a friend, someone you know, someone you
trust and respect to listen.}

{
 And so you put a comradely hand on their shoulder, look them
straight in the eyes, and tell them how to do it.}

{
 I, personally, get quite a lot of this. Because you see \ldots when
you've discovered the way that \textit{really works}
\ldots well, you know better by now than to run out and tell your
friends and family. But you've got to try telling
Eliezer Yudkowsky. He \textit{needs} it, and there's a
pretty good chance that \textit{he'll} understand.}

{
 It actually did take me a while to understand. One of the critical
events was when someone on the Board of the Machine Intelligence
Research Institute told me that I didn't need a salary
increase to keep up with inflation---because I could be spending
substantially less money on food if I used an online coupon service.
And I believed this, because it was a friend I trusted, and it was
delivered in a tone of such confidence. So my girlfriend started trying
to use the service, and a couple of weeks later she gave up.}

{
 Now here's the thing: if I'd run
across exactly the same advice about using coupons on some blog
somewhere, I probably wouldn't even have paid much
attention, just read it and moved on. Even if it were written by Scott
Aaronson or some similar person known to be intelligent, I still would
have read it and moved on. But because it was delivered to me
personally, by a friend who I knew, my brain processed it
differently---as though I were being told \textit{the} secret; and that
indeed is the tone in which it was told to me. And it was something of
a delayed reaction to realize that I'd simply been
told, as personal advice, what otherwise would have been just a blog
post somewhere; no more and no less likely to work for me, than a
productivity blog post written by any other intelligent person.}

{
 And because I have encountered a great many people trying to
optimize me, I can attest that the advice I get is as wide-ranging as
the productivity blogosphere. But others don't see this
plethora of productivity advice as indicating that people are
\textit{diverse} in which advice works for them. Instead they see a lot
of obviously wrong poor advice. And then they finally discover the
right way---the way that works, unlike all those other blog posts that
don't work---and then, quite often, they decide to use
it to optimize Eliezer Yudkowsky.}

{
 Don't get me wrong. Sometimes the advice is
helpful. Sometimes it works. ``Stuck In The Middle
With Bruce''{}---that resonated, for me. It may prove
to be the most helpful thing I've read on the new
\textit{Less Wrong} so far, though that has yet to be determined.}

{
 It's just that your earnest personal advice, that
amazing thing you've found to actually work by golly,
is no more and no less likely to work for me than a random personal
improvement blog post written by an intelligent author is likely to
work for you.}

{
 ``Different things work for different
people.'' That sentence may give you a squicky
feeling; I know it gives me one. Because this sentence is a tool
wielded by Dark Side Epistemology to shield from criticism, used in a
way closely akin to ``Different things are true for
different people'' (which is simply false).}

{
 But until you grasp the laws that are near-universal
generalizations, sometimes you end up messing around with surface
tricks that work for one person and not another, without your
understanding why, because you don't know the general
laws that would dictate what works for who. And the best you can do is
remember that, and be willing to take
``No'' for an answer.}

{
 You \textit{especially} had better be willing to take
``No'' for an answer, if you have
\textit{power} over the Other. Power is, in general, a very dangerous
thing, which is tremendously easy to abuse, without your being aware
that you're abusing it. There are things you can do to
prevent yourself from abusing power, but you have to actually do them
or they don't work. There was a post on
\textit{Overcoming Bias} on how being in a position of power has been
shown to decrease our ability to empathize with and understand the
other, though I can't seem to locate it now. I have
seen a rationalist who did not think he had power, and so did not think
he needed to be cautious, who was amazed to learn that he might be
feared \ldots}

{
 It's even worse when their discovery that works
for them requires a little \textit{willpower.} Then if you say it
doesn't work for you, the answer is clear and obvious:
you're just being \textit{lazy}, and they need to exert
some \textit{pressure} on you to get you to do the \textit{correct}
thing, the advice they've found that actually works.}

{
 Sometimes---I suppose---people are being lazy. But be very, very,
\textit{very} careful before you assume that's the case
and wield power over others to ``get them
moving.'' Bosses who can tell when something actually
\textit{is} in your capacity if you're a little more
motivated, without it burning you out or making your life incredibly
painful---these are the bosses who are a pleasure to work under.
\textit{That ability is extremely rare}, and the bosses who have it are
worth their weight in silver. It's a high-level
interpersonal technique that most people do not have. I surely
don't have it. Do not assume you have it because your
intentions are good. Do not assume you have it because
you'd never do anything to \textit{others} that you
didn't want done to \textit{yourself}. Do not assume
you have it because no one has ever complained to you. Maybe
they're just scared. That rationalist of whom I
spoke---who did not think he held power and threat, though it was
certainly obvious enough to me---he did not realize that anyone could
be scared of him.}

{
 Be careful even when you hold \textit{leverage}, when you hold an
important decision in your hand, or a threat, or something that the
other person needs, and all of a sudden the temptation to optimize them
seems overwhelming.}

{
 Consider, if you would, that Ayn Rand's whole
reign of terror over Objectivists can be seen in just this light---that
she found herself with power and leverage, and could not resist the
temptation to optimize.}

{
 We underestimate the distance between ourselves and others. Not
just inferential distance, but distances of temperament and ability,
distances of situation and resource, distances of unspoken knowledge
and unnoticed skills and luck, distances of interior landscape.}

{
 Even I am often surprised to find that X, which worked so well for
me, doesn't work for someone else. But with so many
others having tried to optimize me, I can at least recognize distance
when I'm hit over the head with it.}

{
 Maybe being pushed on does work \ldots for you. Maybe \textit{you}
don't get sick to the stomach when someone with power
over you starts helpfully trying to reorganize your life the correct
way. I don't know what makes you tick. In the realm of
willpower and akrasia and productivity, as in other realms, I
don't know the generalizations deep enough to hold
almost always. I don't possess the deep keys that would
tell me \textit{when} and \textit{why} and for \textit{who} a technique
works or doesn't work. All I can do is be willing to
accept it when someone tells me it doesn't work \ldots
and go on looking for the deeper generalizations that will hold
everywhere, the deeper laws governing both the rule and the exception,
waiting to be found, someday.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Practical Advice Backed by Deep Theories}

{
 Once upon a time, Seth Roberts took a European vacation and found
that he started losing weight while drinking unfamiliar-tasting caloric
fruit juices. }

{
 Now suppose Roberts had not known, and never did know, anything
about metabolic set points or flavor-calorie associations---all this
high-falutin' scientific experimental research that had
been done on rats and occasionally humans.}

{
 He would have posted to his blog, ``Gosh,
everyone! You should try these amazing fruit juices that are making me
lose weight!'' And that would have been the end of
it. Some people would have tried it, it would have worked
\textit{temporarily} for some of them (until the flavor-calorie
association kicked in) and there never would have been a Shangri-La
Diet \textit{per se}.}

{
 The existing Shangri-La Diet is visibly incomplete---for some
people, like me, it doesn't seem to work, and there is
no apparent reason for this or any logic permitting it. But the reason
why as many people have benefited as they have---the reason why there
was more than just one more blog post describing a trick that seemed to
work for one person and didn't work for anyone
else---is that Roberts \textit{knew the experimental science that let
him interpret what he was seeing, in terms of deep factors that
actually did exist.}}

{
 One of the pieces of advice on \textit{Overcoming Bias} /
\textit{Less Wrong} that was frequently cited as the most important
thing learned, was the idea of ``the bottom
line''---that once a conclusion is written in your
mind, it is already true or already false, already wise or already
stupid, and no amount of later argument can change that except by
changing the conclusion. And this ties directly into another oft-cited
most important thing, which is the idea of ``engines
of cognition,'' minds as mapping engines that require
evidence as fuel.}

{
 Suppose I had merely written one more blog post that said,
``You know, you really should be more open to changing
your mind---it's pretty important---and oh yes, you
should pay attention to the evidence too.'' This
would not have been as useful. Not just because it was \textit{less
persuasive}, but because the \textit{actual operations} would have been
much less clear without the explicit theory backing it up. What
constitutes \textit{evidence}, for example? Is it anything that seems
like a forceful argument? Having an explicit probability theory and an
explicit causal account of what makes reasoning effective makes a
\textit{large} difference in the forcefulness and implementational
details of the old advice to ``Keep an open mind and
pay attention to the evidence.''}

{
 It is also important to realize that \textit{causal theories} are
much more likely to be true when they are picked up from a science
textbook than when invented on the fly---it is very easy to invent
cognitive structures that look like causal theories but are not even
anticipation-controlling, let alone true.}

{
 This is the signature style I want to convey from all those essays
that entangled cognitive science experiments and probability theory and
epistemology with the practical advice---that practical advice actually
becomes practically more powerful if you go out and read up on
cognitive science experiments, or probability theory, or even
materialist epistemology, and \textit{realize what
you're seeing.} This is the brand that can distinguish
\textit{Less Wrong} from ten thousand other blogs purporting to offer
advice.}

{
 I could tell you, ``You know, how much
you're satisfied with your food probably depends more
on the quality of the food than on how much of it you
eat.'' And you would read it and forget about it, and
the impulse to finish off a whole plate would still feel just as
strong. But if I tell you about scope insensitivity, and duration
neglect and the Peak/End rule, you are suddenly aware in a very
concrete way, looking at your plate, that you will form almost exactly
the same retrospective memory whether your portion size is large or
small; you now possess a deep theory about the \textit{rules} governing
your memory, and you know that this is what the rules say. (You also
know to save the dessert for last.)}

{
 I want to hear how I can overcome akrasia---how I can have more
willpower, or get more done with less mental pain. But there are ten
thousand people purporting to give advice on this, and for the most
part, it is on the level of that alternate Seth Roberts who just tells
people about the amazing effects of drinking fruit juice. Or actually,
somewhat worse than that---it's people trying to
describe internal mental levers that they pulled, for which there are
no standard words, and which they do not actually know how to point to.
See also the illusion of transparency, inferential distance, and double
illusion of transparency. (Notice how ``You
overestimate how much you're explaining and your
listeners overestimate how much they're
hearing'' becomes \textit{much more forceful} as
advice, after I back it up with a cognitive science experiment and some
evolutionary psychology?)}

{
 I think that the advice I \textit{need} is from someone who reads
up on a whole lot of experimental psychology dealing with willpower,
mental conflicts, ego depletion, preference reversals, hyperbolic
discounting, the breakdown of the self, picoeconomics, et cetera, and
who, in the process of overcoming their own akrasia, manages to
understand what they did in \textit{truly general terms}{}---thanks to
experiments that give them a vocabulary of cognitive phenomena that
\textit{actually exist}, as opposed to phenomena they just made up. And
moreover, someone who can \textit{explain} what they did to someone
else, thanks again to the experimental and theoretical vocabulary that
lets them point to replicable experiments that ground the ideas in very
concrete results, or mathematically clear ideas.}

{
 Note the grade of increasing difficulty in citing:}

{
 \textit{Concrete} \textit{experimental results} (for which one
need merely consult a paper, hopefully one that reported p {\textless}
0.01 because p {\textless} 0.05 may fail to replicate);}

{
 \textit{Causal accounts that are actually true} (which may be most
reliably obtained by looking for the theories that are used by a
majority within a given science);}

{
 \textit{Math validly interpreted} (on which I have trouble
offering useful advice because so much of my own math talent is
intuition that kicks in before I get a chance to deliberate).}

{
 If you don't know who to trust, or you
don't trust yourself, you should concentrate on
experimental results to start with, move on to thinking in terms of
causal theories that are widely used within a science, and dip your
toes into math and epistemology with extreme caution.}

{
 But practical advice really, really \textit{does} become a lot
more powerful when it's backed up by \textit{concrete
experimental results}, \textit{causal accounts that are actually true},
and \textit{math validly interpreted.}}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{The Sin of Underconfidence}

{
 There are three great besetting sins of rationalists in
particular, and the third of these is underconfidence. Michael Vassar
regularly accuses me of this sin, which makes him unique among the
entire population of the Earth. }

{
 But he's actually quite right to worry, and I
worry too, and any adept rationalist will probably spend a fair amount
of time worying about it. When subjects know about a bias or are warned
about a bias, \textit{overcorrection} is not unheard of as an
experimental result. That's what makes a lot of
cognitive subtasks so troublesome---you know you're
biased but you're not sure \textit{how much}, and you
don't know if you're correcting
\textit{enough}{}---and so perhaps you ought to correct a little more,
and then a little more, but is \textit{that} enough? Or have you,
perhaps, far overshot? Are you now perhaps worse off than if you
hadn't tried any correction?}

{
 You contemplate the matter, feeling more and more lost, and the
very task of estimation begins to feel increasingly futile \ldots}

{
 And when it comes to the particular questions of
\textit{confidence}, \textit{overconfidence}, and
\textit{underconfidence}{}---being interpreted now in the broader
sense, not just calibrated confidence intervals---then there is a
natural tendency to cast overconfidence as the \textit{sin} of pride,
out of that \textit{other} list which never warned against the improper
use of humility or the abuse of doubt. To place yourself too high---to
overreach your proper place---to think too much of yourself---to put
yourself forward---to put down your fellows by implicit
comparison---and the consequences of humiliation and being cast down,
perhaps publicly---are these not loathesome and fearsome things?}

{
 To be too \textit{modest}{}---seems lighter by comparison; it
wouldn't be so humiliating to be called on it publicly.
Indeed, finding out that you're better than you
imagined might come as a warm surprise; and to put yourself down, and
others implicitly above, has a positive tinge of \textit{niceness}
about it. It's the sort of thing that Gandalf would
do.}

{
 So if you have learned a thousand ways that humans fall into error
and read a hundred experimental results in which anonymous subjects are
humiliated of their overconfidence---heck, even if
you've just read a couple of dozen---and you
don't \textit{know} exactly how overconfident you
are---then yes, you might genuinely be in danger of nudging yourself a
step too far down.}

{
 I have no perfect formula to give you that will counteract this.
But I have an item or two of advice.}

{
 What is the \textit{danger} of underconfidence?}

{
 Passing up opportunities. Not doing things you could have done,
but didn't try (hard enough).}

{
 So here's a first item of advice: If
there's a way to \textit{find out} how good you are,
the thing to do is \textit{test} it. \textit{A hypothesis affords
testing;} hypotheses about your own abilities likewise. Once upon a
time it seemed to me that I ought to be able to win at the AI-Box
Experiment; and it seemed like a very doubtful and hubristic thought;
so I tested it. Then later it seemed to me that I might be able to win
even with large sums of money at stake, and I tested that, but I only
won one time out of three. So that was the limit of my ability at that
time, and it was not necessary to argue myself upward or downward,
because I could just \textit{test} it.}

{
 One of the chief ways that smart people end up stupid is by
getting \textit{so used to winning} that they stick to places where
they \textit{know they can win}{}---meaning that they never stretch
their abilities, they never try anything difficult.}

{
 It is said that this is linked to defining yourself in terms of
your ``intelligence'' rather than
``effort,'' because then winning
\textit{easily} is a sign of your
``intelligence,'' where failing on a
hard problem could have been interpreted in terms of a good effort.}

{
 Now, I am not quite sure this is how an adept rationalist should
think about these things: rationality \textit{is} systematized winning
and trying to try seems like a path to failure. I would put it this
way: A hypothesis affords testing! If you \textit{don't
know} whether you'll win on a hard problem---then
\textit{challenge your rationality} to \textit{discover} your current
level. I don't usually hold with congratulating
yourself on having tried---it seems like a bad mental habit to me---but
surely \textit{not} trying is even \textit{worse.} If you have
cultivated a general habit of confronting challenges, and won on at
least \textit{some} of them, then you may, perhaps, think to yourself,
``I did keep up my habit of confronting challenges,
and will do so next time as well.'' You may also
think to yourself ``I have gained valuable information
about my current level and where I need
improvement,'' so long as you properly complete the
thought, ``I shall try not to gain this same valuable
information again next time.''}

{
 If you win \textit{every} time, it means you
aren't stretching yourself enough. But you
\textit{should} seriously try to win every time. And if you console
yourself too much for failure, you lose your winning spirit and become
a scrub.}

{
 When I try to imagine what a fictional master of the Competitive
Conspiracy would say about this, it comes out something like:
``It's \textit{not} okay to lose. But
the \textit{hurt} of losing is not something so scary that you should
flee the challenge for fear of it. It's not so scary
that you have to carefully avoid feeling it, or refuse to admit that
you lost and lost hard. Losing is \textit{supposed} to hurt. If it
didn't hurt you wouldn't be a
Competitor. And there's \textit{no} Competitor who
\textit{never} knows the pain of losing. Now get out there and
\textit{win}.''}

{
 Cultivate a habit of confronting challenges---not the ones that
can kill you outright, perhaps, but perhaps ones that can potentially
\textit{humiliate} you. I recently read of a certain theist that he had
defeated Christopher Hitchens in a debate (severely so; this was said
by atheists). And so I wrote at once to the Bloggingheads folks and
asked if they could arrange a debate. This seemed like someone I wanted
to test myself against. Also, it was said by them that Christopher
Hitchens should have watched the theist's earlier
debates and been prepared, so I decided \textit{not} to do that,
because I think I should be able to handle damn near anything on the
fly, and I desire to learn whether this thought is correct; and I am
willing to risk public humiliation to find out. Note that this is
\textit{not} self-handicapping in the classic sense---if the debate is
indeed arranged (I haven't yet heard back), and I do
not prepare, and I fail, then I do lose those stakes of myself that I
have put up; I gain information about my limits; I have \textit{not}
given myself anything I consider an excuse for losing.}

{
 Of course this is only a way to think when you really \textit{are}
confronting a challenge just to test yourself, and not because you have
to win at any cost. In \textit{that} case you make everything as easy
for yourself as possible. To do otherwise would be \textit{spectacular}
overconfidence, even if you're playing tic-tac-toe
against a three-year-old.}

{
 A subtler form of underconfidence is \textit{losing your forward
momentum}{}---amid all the things you realize that humans are doing
wrong, that you used to be doing wrong, of which you are probably still
doing some wrong. You become timid; you question yourself \textit{but
don't answer the self-questions and move on}; when you
hypothesize your own inability \textit{you do not put that hypothesis
to the test.}}

{
 Perhaps without there ever being a watershed moment when you
deliberately, self-visibly \textit{decide not to try} at some
particular test \ldots you just \ldots~. slow \ldots~.~. down
\ldots~\ldots~.}

{
 It doesn't seem worthwhile any more, to go on
trying to fix one thing when there are a dozen other things that will
still be wrong \ldots}

{
 There's not enough hope of triumph to
\textit{inspire} you to try \textit{hard} \ldots}

{
 When you consider doing any new thing, a dozen questions about
your ability at once leap into your mind, and it does not occur to you
that you could \textit{answer} the questions by \textit{testing}
yourself \ldots}

{
 And having read so much wisdom of human flaws, it seems that the
course of wisdom is ever doubting (never resolving doubts), ever the
humility of refusal (never the humility of preparation), and just
generally, that it is wise to say worse and worse things about human
abilities, to pass into feel-good feel-bad cynicism.}

{
 And so my last piece of advice is another perspective from which
to view the problem---by which to judge any potential habit of thought
you might adopt---and that is to ask:}

{
 \textit{Does this way of thinking make me stronger, or weaker?
Really truly?}}

{
 I have previously spoken of the danger of
\textit{reasonableness}{}---the reasonable-sounding argument that we
should two-box on Newcomb's problem, the
reasonable-sounding argument that we can't know
anything due to the problem of induction, the reasonable-sounding
argument that we will be better off on average if we always adopt the
majority belief, and other such impediments to the Way.
``Does it win?'' is one question you
could ask to get an alternate perspective. Another, slightly different
perspective is to ask, ``Does this way of thinking
make me stronger, or weaker?'' Does constantly
reminding yourself to doubt everything make you stronger, or weaker?
Does never resolving or decreasing those doubts make you stronger, or
weaker? Does undergoing a deliberate crisis of faith in the face of
uncertainty make you stronger, or weaker? Does answering every
objection with a humble confession of you fallibility make you
stronger, or weaker?}

{
 Are your current attempts to compensate for possible
overconfidence making you stronger, or weaker? Hint: If you are taking
more precautions, more scrupulously trying to test yourself, asking
friends for advice, working your way up to big things incrementally, or
still failing sometimes but less often then you used to, you are
probably getting stronger. If you are \textit{never} failing, avoiding
challenges, and feeling generally hopeless and dispirited, you are
probably getting weaker.}

{
 I learned the first form of this rule at a very early age, when I
was practicing for a certain math test, and found that my score was
going down with each practice test I took, and noticed going over the
answer sheet that I had been pencilling in the correct answers and
erasing them. So I said to myself, ``All right,
\textit{this} time I'm going to use the Force and act
on instinct,'' and my score shot up to above what it
had been in the beginning, and on the real test it was higher still. So
that was how I learned that doubting yourself does not always make you
stronger---especially if it interferes with your ability to be moved by
good information, such as your math intuitions. (But I \textit{did}
need the test to tell me this!)}

{
 Underconfidence is not a unique sin of rationalists alone. But it
is a particular danger into which \textit{the attempt to be rational}
can lead you. And it is a \textit{stopping} mistake---an error that
prevents you from gaining that further experience that would correct
the error.}

{
 Because underconfidence actually \textit{does} seem quite common
among aspiring rationalists who I meet---though rather less common
among rationalists who have become famous role models---I would indeed
name it third among the three besetting sins of rationalists.}

{\centering
 \ ~
\par}

{\centering
 *
\par}

\mysection{Go Forth and Create the Art!}

{
 I have said a thing or two about rationality, these past months. I
have said a thing or two about how to untangle questions that have
become confused, and how to tell the difference between real reasoning
and fake reasoning, and the will to become stronger that leads you to
try before you flee; I have said something about doing the impossible.
}

{
 And these are all techniques that I developed in the course of my
own projects---which is why there is so much about cognitive
reductionism, say---and it is possible that your mileage may vary in
trying to apply it yourself. The one's mileage may
vary. Still, those wandering about asking ``But what
good is it?'' might consider rereading some of the
earlier essays; knowing about e.g. the conjunction fallacy, and how to
spot it in an argument, hardly seems esoteric. Understanding why
motivated skepticism is bad for you can constitute the whole
difference, I suspect, between a smart person who ends up smart and a
smart person who ends up stupid. Affective death spirals consume
\textit{many} among the unwary \ldots}

{
 Yet there is, I think, more \textit{absent} than \textit{present}
in this ``art of
rationality''---defeating akrasia and coordinating
groups are two of the deficits I feel most keenly. I've
concentrated more heavily on epistemic rationality than instrumental
rationality, in general. And then there's training,
teaching, verification, and becoming a proper experimental science
based on that. And if you generalize a bit further, then
\textit{building the Art} could also be taken to include issues like
developing better introductory literature, developing better slogans
for public relations, establishing common cause with other
Enlightenment subtasks, analyzing and addressing the gender imbalance
problem \ldots}

{
 But those small pieces of rationality that I've
set out \ldots I \textit{hope} \ldots just maybe \ldots}

{
 I suspect---you could even call it a guess---that there is a
\textit{barrier to getting started}, in this matter of rationality.
Where by default, in the beginning, you don't have
enough to build on. Indeed so little that you don't
have a clue that more exists, that there is an Art to be found. And if
you do begin to sense that more is possible---then you may just
\textit{instantaneously} go wrong. As David Stove observes, most
``great thinkers'' in philosophy,
e.g., Hegel, are properly objects of pity.\textsuperscript{1}
That's what happens by default to anyone who sets out
to develop the art of thinking; they develop fake answers.}

{
 When you try to develop part of the human art of thinking \ldots
then you are doing something \textit{not too dissimilar} to what I was
doing over in Artificial Intelligence. You will be tempted by fake
explanations of the mind, fake accounts of causality, mysterious holy
words, and the amazing idea that solves everything.}

{
 It's not that the particular, epistemic,
fake-detecting methods that I use are so good for every
\textit{particular} problem; but they seem like they might be helpful
for discriminating good and bad \textit{systems of thinking}.}

{
 I hope that someone who learns the part of the Art that
I've set down here will not \textit{instantaneously}
and \textit{automatically} go wrong if they start asking themselves,
``How should people think, in order to solve new
problem X that I'm working on?'' They
will not immediately run away; they will not just make stuff up at
random; they may be moved to consult the literature in experimental
psychology; they will not automatically go into an affective death
spiral around their Brilliant Idea; they will have some idea of what
distinguishes a fake explanation from a real one. They will get a
saving throw.}

{
 It's this sort of barrier, \textit{perhaps}, that
prevents people from \textit{beginning} to develop an art of
rationality, if they are not already rational.}

{
 And so instead they \ldots go off and invent Freudian
psychoanalysis. Or a new religion. Or something. That's
what happens by \textit{default}, when people start thinking about
thinking.}

{
 I hope that the part of the Art I have set down, as incomplete as
it may be, can surpass that preliminary barrier---give people a base to
build on; give them an idea that an Art exists, and somewhat of how it
ought to be developed; and give them at least a \textit{saving throw}
before they \textit{instantaneously} go astray.}

{
 That's my dream---that this
highly-specialized-seeming art of answering confused questions may be
some of what is needed, in the very beginning, \textit{to go and
complete the rest}.}

{
 A task which I am leaving to \textit{you}. Probably, anyway. I
make no promises as to where my attention may turn in the future. But
y'know, there \textit{are} certain other things I need
to do. Even if I develop yet more Art by accident, it may be that I
will not have the time to write any of it up.}

{
 Beyond all that I have said of fake answers and traps, there are
two things I would like you to keep in mind.}

{
 The first---that I drew on multiple sources to create my Art. I
read many different authors, many different experiments, used analogies
from many different fields. \textit{You} will need to draw on multiple
sources to create \textit{your} portion of the Art. You should not be
getting all your rationality from one author---though there might be,
perhaps, a certain centralized website, where you went to post the
links and papers that struck you as really important. And a maturing
Art will need to draw from multiple sources. To the best of my
knowledge there is \textit{no} true science that draws its strength
from only one person. To the best of my knowledge that is
\textit{strictly} an idiom of cults. A true science may have its
heroes, it may even have its lonely defiant heroes, but \textit{it will
have more than one.}}

{
 The second---that I created my Art in the course of \textit{trying
to do some particular thing} that animated all my efforts. Maybe
I'm being too idealistic---maybe thinking too much of
the way the world \textit{should} work---but even so, I somewhat
suspect that you couldn't develop the Art \textit{just}
by sitting around thinking to yourself, ``Now how can
I fight that akrasia thingy?'' You'd
develop the rest of the Art in the course of trying to \textit{do
something}. Maybe even---if I'm not overgeneralizing
from my own history---some task difficult enough to strain and break
your old understanding and force you to reinvent a few things. But
maybe I'm wrong, and the next leg of the work will be
done by direct, specific investigation of
``rationality,'' without any need of
a specific application considered more important.}

{
 A past attempt of mine to describe this principle, in terms of
maintaining a secret identity or day job in which one
doesn't teach rationality, was roundly rejected by my
audience. Maybe ``leave the house''
would be more appropriate? It \textit{sounds} to me like a really good,
healthy idea. Still---perhaps I am deceived. We shall see where the
next pieces of the Art do, in fact, come from.}

{
 I have striven for a long time now to convey, pass on, share a
piece of the strange thing I touched, which seems to me so precious.
And I'm not sure that I ever said the central rhythm
into words. Maybe you can find it by listening to the notes. I can say
these words but not the rule that generates them, or the rule behind
the rule; one can only hope that by \textit{using} the ideas, perhaps,
similar machinery might be born inside you. Remember that \textit{all}
human efforts at learning arcana slide by default into passwords,
hymns, and floating assertions.}

{
 I have striven for a long time now to convey my Art. Mostly
without success, before this present effort. Earlier I made efforts
only in passing, and got, perhaps, as much success as I deserved. Like
throwing pebbles in a pond, that generate a few ripples, and then fade
away \ldots This time I put some back into it, and heaved a large rock.
Time will tell if it was large enough---if I really \textit{disturbed}
anyone deeply enough that the waves of the impact will continue under
their own motion. Time will tell if I have created anything that moves
under its own power.}

{
 I want people to go forth, but also to return. Or maybe even to go
forth and stay simultaneously, because this is the Internet and we can
get away with that sort of thing; I've learned some
interesting things on \textit{Less Wrong}, lately, and if continuing
motivation over years is any sort of problem, talking to others (or
even \textit{seeing} that others are also trying) does often help.}

{
 But at any rate, if I have affected you at all, then I hope you
will go forth and confront challenges, and achieve somewhere beyond
your armchair, and create new Art; and then, remembering whence you
came, radio back to tell others what you learned.}

{\centering
 \ ~
\par}

{\centering
 *
\par}


\bigskip

{
 1. David Charles Stove, \textit{The Plato Cult and Other
Philosophical Follies} (Cambridge University Press, 1991).}

